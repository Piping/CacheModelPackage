package org . apache . cassandra . db ; import java . io . File ; import java . util . * ; import java . util . concurrent . * ; import java . util . concurrent . atomic . AtomicBoolean ; import java . util . concurrent . atomic . AtomicLong ; import java . util . concurrent . atomic . AtomicReference ; import com . google . common . annotations . VisibleForTesting ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . config . CFMetaData ; import org . apache . cassandra . config . ColumnDefinition ; import org . apache . cassandra . config . DatabaseDescriptor ; import org . apache . cassandra . db . commitlog . CommitLog ; import org . apache . cassandra . db . commitlog . ReplayPosition ; import org . apache . cassandra . db . compaction . OperationType ; import org . apache . cassandra . db . filter . ClusteringIndexFilter ; import org . apache . cassandra . db . filter . ColumnFilter ; import org . apache . cassandra . db . lifecycle . LifecycleTransaction ; import org . apache . cassandra . db . partitions . * ; import org . apache . cassandra . db . rows . EncodingStats ; import org . apache . cassandra . db . rows . UnfilteredRowIterator ; import org . apache . cassandra . dht . * ; import org . apache . cassandra . dht . Murmur3Partitioner . LongToken ; import org . apache . cassandra . index . transactions . UpdateTransaction ; import org . apache . cassandra . io . sstable . Descriptor ; import org . apache . cassandra . io . sstable . SSTableTxnWriter ; import org . apache . cassandra . io . sstable . format . SSTableReader ; import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; import org . apache . cassandra . io . util . DiskAwareRunnable ; import org . apache . cassandra . service . ActiveRepairService ; import org . apache . cassandra . utils . ByteBufferUtil ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . ObjectSizes ; import org . apache . cassandra . utils . concurrent . OpOrder ; import org . apache . cassandra . utils . memory . MemtableAllocator ; import org . apache . cassandra . utils . memory . MemtablePool ; public class Memtable implements Comparable < Memtable > { private static final Logger logger = LoggerFactory . getLogger ( Memtable . class ) ; static final MemtablePool MEMORY_POOL = DatabaseDescriptor . getMemtableAllocatorPool ( ) ; private static final int ROW_OVERHEAD_HEAP_SIZE = estimateRowOverhead ( Integer . parseInt ( System . getProperty ( <str> , <str> ) ) ) ; private final MemtableAllocator allocator ; private final AtomicLong liveDataSize = new AtomicLong ( <int> ) ; private final AtomicLong currentOperations = new AtomicLong ( <int> ) ; private volatile OpOrder . Barrier writeBarrier ; private volatile AtomicReference < ReplayPosition > lastReplayPosition ; private final ReplayPosition minReplayPosition = CommitLog . instance . getContext ( ) ; public int compareTo ( Memtable that ) { return this . minReplayPosition . compareTo ( that . minReplayPosition ) ; } public static final class LastReplayPosition extends ReplayPosition { public LastReplayPosition ( ReplayPosition copy ) { super ( copy . segment , copy . position ) ; } } private final ConcurrentNavigableMap < PartitionPosition , AtomicBTreePartition > partitions = new ConcurrentSkipListMap < > ( ) ; public final ColumnFamilyStore cfs ; private final long creationTime = System . currentTimeMillis ( ) ; private final long creationNano = System . nanoTime ( ) ; public final ClusteringComparator initialComparator ; private final ColumnsCollector columnsCollector ; private final StatsCollector statsCollector = new StatsCollector ( ) ; public Memtable ( ColumnFamilyStore cfs ) { this . cfs = cfs ; this . allocator = MEMORY_POOL . newAllocator ( ) ; this . initialComparator = cfs . metadata . comparator ; this . cfs . scheduleFlush ( ) ; this . columnsCollector = new ColumnsCollector ( cfs . metadata . partitionColumns ( ) ) ; } @VisibleForTesting public Memtable ( CFMetaData metadata ) { this . initialComparator = metadata . comparator ; this . cfs = null ; this . allocator = null ; this . columnsCollector = new ColumnsCollector ( metadata . partitionColumns ( ) ) ; } public MemtableAllocator getAllocator ( ) { return allocator ; } public long getLiveDataSize ( ) { return liveDataSize . get ( ) ; } public long getOperations ( ) { return currentOperations . get ( ) ; } @VisibleForTesting public void setDiscarding ( OpOrder . Barrier writeBarrier , AtomicReference < ReplayPosition > lastReplayPosition ) { assert this . writeBarrier = = null ; this . lastReplayPosition = lastReplayPosition ; this . writeBarrier = writeBarrier ; allocator . setDiscarding ( ) ; } void setDiscarded ( ) { allocator . setDiscarded ( ) ; } public boolean accepts ( OpOrder . Group opGroup , ReplayPosition replayPosition ) { OpOrder . Barrier barrier = this . writeBarrier ; if ( barrier = = null ) return true ; if ( ! barrier . isAfter ( opGroup ) ) return false ; if ( replayPosition = = null ) return true ; while ( true ) { ReplayPosition currentLast = lastReplayPosition . get ( ) ; if ( currentLast instanceof LastReplayPosition ) return currentLast . compareTo ( replayPosition ) > = <int> ; if ( currentLast ! = null & & currentLast . compareTo ( replayPosition ) > = <int> ) return true ; if ( lastReplayPosition . compareAndSet ( currentLast , replayPosition ) ) return true ; } } public boolean isLive ( ) { return allocator . isLive ( ) ; } public boolean isClean ( ) { return partitions . isEmpty ( ) ; } public boolean isCleanAfter ( ReplayPosition position ) { return isClean ( ) | | ( position ! = null & & minReplayPosition . compareTo ( position ) > = <int> ) ; } public boolean isExpired ( ) { int period = cfs . metadata . params . memtableFlushPeriodInMs ; return period > <int> & & ( System . nanoTime ( ) - creationNano > = TimeUnit . MILLISECONDS . toNanos ( period ) ) ; } long put ( PartitionUpdate update , UpdateTransaction indexer , OpOrder . Group opGroup ) { AtomicBTreePartition previous = partitions . get ( update . partitionKey ( ) ) ; long initialSize = <int> ; if ( previous = = null ) { final DecoratedKey cloneKey = allocator . clone ( update . partitionKey ( ) , opGroup ) ; AtomicBTreePartition empty = new AtomicBTreePartition ( cfs . metadata , cloneKey , allocator ) ; previous = partitions . putIfAbsent ( cloneKey , empty ) ; if ( previous = = null ) { previous = empty ; int overhead = ( int ) ( cloneKey . getToken ( ) . getHeapSize ( ) + ROW_OVERHEAD_HEAP_SIZE ) ; allocator . onHeap ( ) . allocate ( overhead , opGroup ) ; initialSize = <int> ; } else { allocator . reclaimer ( ) . reclaimImmediately ( cloneKey ) ; } } long [ ] pair = previous . addAllWithSizeDelta ( update , opGroup , indexer ) ; liveDataSize . addAndGet ( initialSize + pair [ <int> ] ) ; columnsCollector . update ( update . columns ( ) ) ; statsCollector . update ( update . stats ( ) ) ; currentOperations . addAndGet ( update . operationCount ( ) ) ; return pair [ <int> ] ; } public int partitionCount ( ) { return partitions . size ( ) ; } public FlushRunnable flushRunnable ( ) { return new FlushRunnable ( lastReplayPosition . get ( ) ) ; } public String toString ( ) { return String . format ( <str> , cfs . name , hashCode ( ) , FBUtilities . prettyPrintMemory ( liveDataSize . get ( ) ) , currentOperations , <int> * allocator . onHeap ( ) . ownershipRatio ( ) , <int> * allocator . offHeap ( ) . ownershipRatio ( ) ) ; } public MemtableUnfilteredPartitionIterator makePartitionIterator ( final ColumnFilter columnFilter , final DataRange dataRange , final boolean isForThrift ) { AbstractBounds < PartitionPosition > keyRange = dataRange . keyRange ( ) ; boolean startIsMin = keyRange . left . isMinimum ( ) ; boolean stopIsMin = keyRange . right . isMinimum ( ) ; boolean isBound = keyRange instanceof Bounds ; boolean includeStart = isBound | | keyRange instanceof IncludingExcludingBounds ; boolean includeStop = isBound | | keyRange instanceof Range ; Map < PartitionPosition , AtomicBTreePartition > subMap ; if ( startIsMin ) subMap = stopIsMin ? partitions : partitions . headMap ( keyRange . right , includeStop ) ; else subMap = stopIsMin ? partitions . tailMap ( keyRange . left , includeStart ) : partitions . subMap ( keyRange . left , includeStart , keyRange . right , includeStop ) ; int minLocalDeletionTime = Integer . MAX_VALUE ; if ( cfs . getCompactionStrategyManager ( ) . onlyPurgeRepairedTombstones ( ) ) minLocalDeletionTime = findMinLocalDeletionTime ( subMap . entrySet ( ) . iterator ( ) ) ; final Iterator < Map . Entry < PartitionPosition , AtomicBTreePartition > > iter = subMap . entrySet ( ) . iterator ( ) ; return new MemtableUnfilteredPartitionIterator ( cfs , iter , isForThrift , minLocalDeletionTime , columnFilter , dataRange ) ; } private int findMinLocalDeletionTime ( Iterator < Map . Entry < PartitionPosition , AtomicBTreePartition > > iterator ) { int minLocalDeletionTime = Integer . MAX_VALUE ; while ( iterator . hasNext ( ) ) { Map . Entry < PartitionPosition , AtomicBTreePartition > entry = iterator . next ( ) ; minLocalDeletionTime = Math . min ( minLocalDeletionTime , entry . getValue ( ) . stats ( ) . minLocalDeletionTime ) ; } return minLocalDeletionTime ; } public Partition getPartition ( DecoratedKey key ) { return partitions . get ( key ) ; } public long creationTime ( ) { return creationTime ; } class FlushRunnable extends DiskAwareRunnable { private final ReplayPosition context ; private final long estimatedSize ; private final boolean isBatchLogTable ; FlushRunnable ( ReplayPosition context ) { this . context = context ; long keySize = <int> ; for ( PartitionPosition key : partitions . keySet ( ) ) { assert key instanceof DecoratedKey ; keySize + = ( ( DecoratedKey ) key ) . getKey ( ) . remaining ( ) ; } estimatedSize = ( long ) ( ( keySize + keySize + liveDataSize . get ( ) ) * <float> ) ; this . isBatchLogTable = cfs . name . equals ( SystemKeyspace . BATCHES ) & & cfs . keyspace . getName ( ) . equals ( SystemKeyspace . NAME ) ; } public long getExpectedWriteSize ( ) { return estimatedSize ; } protected void runMayThrow ( ) throws Exception { long writeSize = getExpectedWriteSize ( ) ; Directories . DataDirectory dataDirectory = getWriteDirectory ( writeSize ) ; File sstableDirectory = cfs . getDirectories ( ) . getLocationForDisk ( dataDirectory ) ; assert sstableDirectory ! = null : <str> ; Collection < SSTableReader > sstables = writeSortedContents ( context , sstableDirectory ) ; cfs . replaceFlushed ( Memtable . this , sstables ) ; } protected Directories getDirectories ( ) { return cfs . getDirectories ( ) ; } private Collection < SSTableReader > writeSortedContents ( ReplayPosition context , File sstableDirectory ) { logger . debug ( <str> , Memtable . this . toString ( ) ) ; Collection < SSTableReader > ssTables ; try ( SSTableTxnWriter writer = createFlushWriter ( cfs . getSSTablePath ( sstableDirectory ) , columnsCollector . get ( ) , statsCollector . get ( ) ) ) { boolean trackContention = logger . isTraceEnabled ( ) ; int heavilyContendedRowCount = <int> ; for ( AtomicBTreePartition partition : partitions . values ( ) ) { if ( isBatchLogTable & & ! partition . partitionLevelDeletion ( ) . isLive ( ) & & partition . hasRows ( ) ) continue ; if ( trackContention & & partition . usePessimisticLocking ( ) ) heavilyContendedRowCount + + ; if ( ! partition . isEmpty ( ) ) { try ( UnfilteredRowIterator iter = partition . unfilteredIterator ( ) ) { writer . append ( iter ) ; } } } if ( writer . getFilePointer ( ) > <int> ) { logger . debug ( String . format ( <str> , writer . getFilename ( ) , FBUtilities . prettyPrintMemory ( writer . getFilePointer ( ) ) , context ) ) ; ssTables = writer . finish ( true ) ; } else { logger . debug ( <str> , writer . getFilename ( ) , context ) ; writer . abort ( ) ; ssTables = null ; } if ( heavilyContendedRowCount > <int> ) logger . trace ( String . format ( <str> , heavilyContendedRowCount , partitions . size ( ) , Memtable . this . toString ( ) ) ) ; return ssTables ; } } @SuppressWarnings ( <str> ) public SSTableTxnWriter createFlushWriter ( String filename , PartitionColumns columns , EncodingStats stats ) { LifecycleTransaction txn = null ; try { txn = LifecycleTransaction . offline ( OperationType . FLUSH ) ; MetadataCollector sstableMetadataCollector = new MetadataCollector ( cfs . metadata . comparator ) . replayPosition ( context ) ; return new SSTableTxnWriter ( txn , cfs . createSSTableMultiWriter ( Descriptor . fromFilename ( filename ) , ( long ) partitions . size ( ) , ActiveRepairService . UNREPAIRED_SSTABLE , sstableMetadataCollector , new SerializationHeader ( true , cfs . metadata , columns , stats ) , txn ) ) ; } catch ( Throwable t ) { if ( txn ! = null ) txn . close ( ) ; throw t ; } } } private static int estimateRowOverhead ( final int count ) { try ( final OpOrder . Group group = new OpOrder ( ) . start ( ) ) { int rowOverhead ; MemtableAllocator allocator = MEMORY_POOL . newAllocator ( ) ; ConcurrentNavigableMap < PartitionPosition , Object > partitions = new ConcurrentSkipListMap < > ( ) ; final Object val = new Object ( ) ; for ( int i = <int> ; i < count ; i + + ) partitions . put ( allocator . clone ( new BufferDecoratedKey ( new LongToken ( i ) , ByteBufferUtil . EMPTY_BYTE_BUFFER ) , group ) , val ) ; double avgSize = ObjectSizes . measureDeep ( partitions ) / ( double ) count ; rowOverhead = ( int ) ( ( avgSize - Math . floor ( avgSize ) ) < <float> ? Math . floor ( avgSize ) : Math . ceil ( avgSize ) ) ; rowOverhead - = ObjectSizes . measureDeep ( new LongToken ( <int> ) ) ; rowOverhead + = AtomicBTreePartition . EMPTY_SIZE ; allocator . setDiscarding ( ) ; allocator . setDiscarded ( ) ; return rowOverhead ; } } public static class MemtableUnfilteredPartitionIterator extends AbstractUnfilteredPartitionIterator { private final ColumnFamilyStore cfs ; private final Iterator < Map . Entry < PartitionPosition , AtomicBTreePartition > > iter ; private final boolean isForThrift ; private final int minLocalDeletionTime ; private final ColumnFilter columnFilter ; private final DataRange dataRange ; public MemtableUnfilteredPartitionIterator ( ColumnFamilyStore cfs , Iterator < Map . Entry < PartitionPosition , AtomicBTreePartition > > iter , boolean isForThrift , int minLocalDeletionTime , ColumnFilter columnFilter , DataRange dataRange ) { this . cfs = cfs ; this . iter = iter ; this . isForThrift = isForThrift ; this . minLocalDeletionTime = minLocalDeletionTime ; this . columnFilter = columnFilter ; this . dataRange = dataRange ; } public boolean isForThrift ( ) { return isForThrift ; } public int getMinLocalDeletionTime ( ) { return minLocalDeletionTime ; } public CFMetaData metadata ( ) { return cfs . metadata ; } public boolean hasNext ( ) { return iter . hasNext ( ) ; } public UnfilteredRowIterator next ( ) { Map . Entry < PartitionPosition , AtomicBTreePartition > entry = iter . next ( ) ; assert entry . getKey ( ) instanceof DecoratedKey ; DecoratedKey key = ( DecoratedKey ) entry . getKey ( ) ; ClusteringIndexFilter filter = dataRange . clusteringIndexFilter ( key ) ; return filter . getUnfilteredRowIterator ( columnFilter , entry . getValue ( ) ) ; } } private static class ColumnsCollector { private final HashMap < ColumnDefinition , AtomicBoolean > predefined = new HashMap < > ( ) ; private final ConcurrentSkipListSet < ColumnDefinition > extra = new ConcurrentSkipListSet < > ( ) ; ColumnsCollector ( PartitionColumns columns ) { for ( ColumnDefinition def : columns . statics ) predefined . put ( def , new AtomicBoolean ( ) ) ; for ( ColumnDefinition def : columns . regulars ) predefined . put ( def , new AtomicBoolean ( ) ) ; } public void update ( PartitionColumns columns ) { for ( ColumnDefinition s : columns . statics ) update ( s ) ; for ( ColumnDefinition r : columns . regulars ) update ( r ) ; } private void update ( ColumnDefinition definition ) { AtomicBoolean present = predefined . get ( definition ) ; if ( present ! = null ) { if ( ! present . get ( ) ) present . set ( true ) ; } else { extra . add ( definition ) ; } } public PartitionColumns get ( ) { PartitionColumns . Builder builder = PartitionColumns . builder ( ) ; for ( Map . Entry < ColumnDefinition , AtomicBoolean > e : predefined . entrySet ( ) ) if ( e . getValue ( ) . get ( ) ) builder . add ( e . getKey ( ) ) ; return builder . addAll ( extra ) . build ( ) ; } } private static class StatsCollector { private final AtomicReference < EncodingStats > stats = new AtomicReference < > ( EncodingStats . NO_STATS ) ; public void update ( EncodingStats newStats ) { while ( true ) { EncodingStats current = stats . get ( ) ; EncodingStats updated = current . mergeWith ( newStats ) ; if ( stats . compareAndSet ( current , updated ) ) return ; } } public EncodingStats get ( ) { return stats . get ( ) ; } } } 
