package org . elasticsearch . common . lucene . search ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . analysis . tokenattributes . CharTermAttribute ; import org . apache . lucene . document . Document ; import org . apache . lucene . index . * ; import org . apache . lucene . search . * ; import org . apache . lucene . search . similarities . DefaultSimilarity ; import org . apache . lucene . search . similarities . TFIDFSimilarity ; import org . apache . lucene . util . BytesRef ; import org . apache . lucene . util . CharsRefBuilder ; import org . apache . lucene . util . PriorityQueue ; import org . elasticsearch . common . Nullable ; import org . elasticsearch . common . io . FastStringReader ; import java . io . IOException ; import java . io . Reader ; import java . util . * ; public final class XMoreLikeThis { public static final int DEFAULT_MAX_NUM_TOKENS_PARSED = <int> ; public static final int DEFAULT_MIN_TERM_FREQ = <int> ; public static final int DEFAULT_MIN_DOC_FREQ = <int> ; public static final int DEFAULT_MAX_DOC_FREQ = Integer . MAX_VALUE ; public static final boolean DEFAULT_BOOST = false ; public static final String [ ] DEFAULT_FIELD_NAMES = new String [ ] { <str> } ; public static final int DEFAULT_MIN_WORD_LENGTH = <int> ; public static final int DEFAULT_MAX_WORD_LENGTH = <int> ; public static final Set < ? > DEFAULT_STOP_WORDS = null ; private Set < ? > stopWords = DEFAULT_STOP_WORDS ; public static final int DEFAULT_MAX_QUERY_TERMS = <int> ; private Analyzer analyzer = null ; private int minTermFreq = DEFAULT_MIN_TERM_FREQ ; private int minDocFreq = DEFAULT_MIN_DOC_FREQ ; private int maxDocFreq = DEFAULT_MAX_DOC_FREQ ; private boolean boost = DEFAULT_BOOST ; private Set < Term > skipTerms = null ; private String [ ] fieldNames = DEFAULT_FIELD_NAMES ; private int maxNumTokensParsed = DEFAULT_MAX_NUM_TOKENS_PARSED ; private int minWordLen = DEFAULT_MIN_WORD_LENGTH ; private int maxWordLen = DEFAULT_MAX_WORD_LENGTH ; private int maxQueryTerms = DEFAULT_MAX_QUERY_TERMS ; private TFIDFSimilarity similarity ; private final IndexReader ir ; private float boostFactor = <int> ; public float getBoostFactor ( ) { return boostFactor ; } public void setBoostFactor ( float boostFactor ) { this . boostFactor = boostFactor ; } public void setSkipTerms ( Set < Term > skipTerms ) { this . skipTerms = skipTerms ; } public XMoreLikeThis ( IndexReader ir ) { this ( ir , new DefaultSimilarity ( ) ) ; } public XMoreLikeThis ( IndexReader ir , TFIDFSimilarity sim ) { this . ir = ir ; this . similarity = sim ; } public TFIDFSimilarity getSimilarity ( ) { return similarity ; } public void setSimilarity ( TFIDFSimilarity similarity ) { this . similarity = similarity ; } public Analyzer getAnalyzer ( ) { return analyzer ; } public void setAnalyzer ( Analyzer analyzer ) { this . analyzer = analyzer ; } public int getMinTermFreq ( ) { return minTermFreq ; } public void setMinTermFreq ( int minTermFreq ) { this . minTermFreq = minTermFreq ; } public int getMinDocFreq ( ) { return minDocFreq ; } public void setMinDocFreq ( int minDocFreq ) { this . minDocFreq = minDocFreq ; } public int getMaxDocFreq ( ) { return maxDocFreq ; } public void setMaxDocFreq ( int maxFreq ) { this . maxDocFreq = maxFreq ; } public void setMaxDocFreqPct ( int maxPercentage ) { this . maxDocFreq = maxPercentage * ir . numDocs ( ) / <int> ; } public boolean isBoost ( ) { return boost ; } public void setBoost ( boolean boost ) { this . boost = boost ; } public String [ ] getFieldNames ( ) { return fieldNames ; } public void setFieldNames ( String [ ] fieldNames ) { this . fieldNames = fieldNames ; } public int getMinWordLen ( ) { return minWordLen ; } public void setMinWordLen ( int minWordLen ) { this . minWordLen = minWordLen ; } public int getMaxWordLen ( ) { return maxWordLen ; } public void setMaxWordLen ( int maxWordLen ) { this . maxWordLen = maxWordLen ; } public void setStopWords ( Set < ? > stopWords ) { this . stopWords = stopWords ; } public Set < ? > getStopWords ( ) { return stopWords ; } public int getMaxQueryTerms ( ) { return maxQueryTerms ; } public void setMaxQueryTerms ( int maxQueryTerms ) { this . maxQueryTerms = maxQueryTerms ; } public int getMaxNumTokensParsed ( ) { return maxNumTokensParsed ; } public void setMaxNumTokensParsed ( int i ) { maxNumTokensParsed = i ; } public Query like ( int docNum ) throws IOException { if ( fieldNames = = null ) { Collection < String > fields = MultiFields . getIndexedFields ( ir ) ; fieldNames = fields . toArray ( new String [ fields . size ( ) ] ) ; } return createQuery ( retrieveTerms ( docNum ) ) ; } public Query like ( String fieldName , Reader . . . readers ) throws IOException { Map < String , Int > words = new HashMap < > ( ) ; for ( Reader r : readers ) { addTermFrequencies ( r , words , fieldName ) ; } return createQuery ( createQueue ( words ) ) ; } public Query like ( Terms . . . likeTerms ) throws IOException { Map < String , Int > termFreqMap = new HashMap < > ( ) ; for ( Terms vector : likeTerms ) { addTermFrequencies ( termFreqMap , vector ) ; } return createQuery ( createQueue ( termFreqMap ) ) ; } public Query like ( Fields . . . likeFields ) throws IOException { Set < String > fieldNames = new HashSet < > ( ) ; for ( Fields fields : likeFields ) { for ( String fieldName : fields ) { fieldNames . add ( fieldName ) ; } } BooleanQuery bq = new BooleanQuery ( ) ; for ( String fieldName : fieldNames ) { Map < String , Int > termFreqMap = new HashMap < > ( ) ; for ( Fields fields : likeFields ) { Terms vector = fields . terms ( fieldName ) ; if ( vector ! = null ) { addTermFrequencies ( termFreqMap , vector , fieldName ) ; } } addToQuery ( createQueue ( termFreqMap , fieldName ) , bq ) ; } return bq ; } private Query createQuery ( PriorityQueue < ScoreTerm > q ) { BooleanQuery query = new BooleanQuery ( ) ; addToQuery ( q , query ) ; return query ; } private void addToQuery ( PriorityQueue < ScoreTerm > q , BooleanQuery query ) { ScoreTerm scoreTerm ; float bestScore = - <int> ; while ( ( scoreTerm = q . pop ( ) ) ! = null ) { Query tq = new TermQuery ( new Term ( scoreTerm . topField , scoreTerm . word ) ) ; if ( boost ) { if ( bestScore = = - <int> ) { bestScore = ( scoreTerm . score ) ; } float myScore = ( scoreTerm . score ) ; tq = new BoostQuery ( tq , boostFactor * myScore / bestScore ) ; } try { query . add ( tq , BooleanClause . Occur . SHOULD ) ; } catch ( BooleanQuery . TooManyClauses ignore ) { break ; } } } private PriorityQueue < ScoreTerm > createQueue ( Map < String , Int > words ) throws IOException { return createQueue ( words , this . fieldNames ) ; } private PriorityQueue < ScoreTerm > createQueue ( Map < String , Int > words , String . . . fieldNames ) throws IOException { int numDocs = ir . numDocs ( ) ; final int limit = Math . min ( maxQueryTerms , words . size ( ) ) ; FreqQ queue = new FreqQ ( limit ) ; for ( String word : words . keySet ( ) ) { int tf = words . get ( word ) . x ; if ( minTermFreq > <int> & & tf < minTermFreq ) { continue ; } String topField = fieldNames [ <int> ] ; int docFreq = <int> ; for ( String fieldName : fieldNames ) { int freq = ir . docFreq ( new Term ( fieldName , word ) ) ; topField = ( freq > docFreq ) ? fieldName : topField ; docFreq = ( freq > docFreq ) ? freq : docFreq ; } if ( minDocFreq > <int> & & docFreq < minDocFreq ) { continue ; } if ( docFreq > maxDocFreq ) { continue ; } if ( docFreq = = <int> ) { continue ; } float idf = similarity . idf ( docFreq , numDocs ) ; float score = tf * idf ; if ( queue . size ( ) < limit ) { queue . add ( new ScoreTerm ( word , topField , score , idf , docFreq , tf ) ) ; } else { ScoreTerm term = queue . top ( ) ; if ( term . score < score ) { term . update ( word , topField , score , idf , docFreq , tf ) ; queue . updateTop ( ) ; } } } return queue ; } public String describeParams ( ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( <str> ) . append ( <str> ) . append ( maxQueryTerms ) . append ( <str> ) ; sb . append ( <str> ) . append ( <str> ) . append ( minWordLen ) . append ( <str> ) ; sb . append ( <str> ) . append ( <str> ) . append ( maxWordLen ) . append ( <str> ) ; sb . append ( <str> ) . append ( <str> ) ; String delim = <str> ; for ( String fieldName : fieldNames ) { sb . append ( delim ) . append ( fieldName ) ; delim = <str> ; } sb . append ( <str> ) ; sb . append ( <str> ) . append ( <str> ) . append ( boost ) . append ( <str> ) ; sb . append ( <str> ) . append ( <str> ) . append ( minTermFreq ) . append ( <str> ) ; sb . append ( <str> ) . append ( <str> ) . append ( minDocFreq ) . append ( <str> ) ; return sb . toString ( ) ; } private PriorityQueue < ScoreTerm > retrieveTerms ( int docNum ) throws IOException { Map < String , Int > termFreqMap = new HashMap < > ( ) ; for ( String fieldName : fieldNames ) { final Fields vectors = ir . getTermVectors ( docNum ) ; final Terms vector ; if ( vectors ! = null ) { vector = vectors . terms ( fieldName ) ; } else { vector = null ; } if ( vector = = null ) { Document d = ir . document ( docNum ) ; IndexableField fields [ ] = d . getFields ( fieldName ) ; for ( IndexableField field : fields ) { final String stringValue = field . stringValue ( ) ; if ( stringValue ! = null ) { addTermFrequencies ( new FastStringReader ( stringValue ) , termFreqMap , fieldName ) ; } } } else { addTermFrequencies ( termFreqMap , vector , fieldName ) ; } } return createQueue ( termFreqMap ) ; } private void addTermFrequencies ( Map < String , Int > termFreqMap , Terms vector ) throws IOException { addTermFrequencies ( termFreqMap , vector , null ) ; } private void addTermFrequencies ( Map < String , Int > termFreqMap , Terms vector , @Nullable String fieldName ) throws IOException { final TermsEnum termsEnum = vector . iterator ( ) ; final CharsRefBuilder spare = new CharsRefBuilder ( ) ; BytesRef text ; while ( ( text = termsEnum . next ( ) ) ! = null ) { spare . copyUTF8Bytes ( text ) ; final String term = spare . toString ( ) ; if ( isNoiseWord ( term ) ) { continue ; } if ( isSkipTerm ( fieldName , term ) ) { continue ; } final PostingsEnum docs = termsEnum . postings ( null ) ; int freq = <int> ; while ( docs ! = null & & docs . nextDoc ( ) ! = DocIdSetIterator . NO_MORE_DOCS ) { freq + = docs . freq ( ) ; } Int cnt = termFreqMap . get ( term ) ; if ( cnt = = null ) { cnt = new Int ( ) ; termFreqMap . put ( term , cnt ) ; cnt . x = freq ; } else { cnt . x + = freq ; } } } private void addTermFrequencies ( Reader r , Map < String , Int > termFreqMap , String fieldName ) throws IOException { if ( analyzer = = null ) { throw new UnsupportedOperationException ( <str> + <str> ) ; } try ( TokenStream ts = analyzer . tokenStream ( fieldName , r ) ) { int tokenCount = <int> ; CharTermAttribute termAtt = ts . addAttribute ( CharTermAttribute . class ) ; ts . reset ( ) ; while ( ts . incrementToken ( ) ) { String word = termAtt . toString ( ) ; tokenCount + + ; if ( tokenCount > maxNumTokensParsed ) { break ; } if ( isNoiseWord ( word ) ) { continue ; } if ( isSkipTerm ( fieldName , word ) ) { continue ; } Int cnt = termFreqMap . get ( word ) ; if ( cnt = = null ) { termFreqMap . put ( word , new Int ( ) ) ; } else { cnt . x + + ; } } ts . end ( ) ; } } private boolean isNoiseWord ( String term ) { int len = term . length ( ) ; if ( minWordLen > <int> & & len < minWordLen ) { return true ; } if ( maxWordLen > <int> & & len > maxWordLen ) { return true ; } return stopWords ! = null & & stopWords . contains ( term ) ; } private boolean isSkipTerm ( @Nullable String field , String value ) { return field ! = null & & skipTerms ! = null & & skipTerms . contains ( new Term ( field , value ) ) ; } private PriorityQueue < ScoreTerm > retrieveTerms ( Reader r , String fieldName ) throws IOException { Map < String , Int > words = new HashMap < > ( ) ; addTermFrequencies ( r , words , fieldName ) ; return createQueue ( words ) ; } public String [ ] retrieveInterestingTerms ( int docNum ) throws IOException { ArrayList < Object > al = new ArrayList < > ( maxQueryTerms ) ; PriorityQueue < ScoreTerm > pq = retrieveTerms ( docNum ) ; ScoreTerm scoreTerm ; int lim = maxQueryTerms ; while ( ( ( scoreTerm = pq . pop ( ) ) ! = null ) & & lim - - > <int> ) { al . add ( scoreTerm . word ) ; } String [ ] res = new String [ al . size ( ) ] ; return al . toArray ( res ) ; } public String [ ] retrieveInterestingTerms ( Reader r , String fieldName ) throws IOException { ArrayList < Object > al = new ArrayList < > ( maxQueryTerms ) ; PriorityQueue < ScoreTerm > pq = retrieveTerms ( r , fieldName ) ; ScoreTerm scoreTerm ; int lim = maxQueryTerms ; while ( ( ( scoreTerm = pq . pop ( ) ) ! = null ) & & lim - - > <int> ) { al . add ( scoreTerm . word ) ; } String [ ] res = new String [ al . size ( ) ] ; return al . toArray ( res ) ; } private static class FreqQ extends PriorityQueue < ScoreTerm > { FreqQ ( int maxSize ) { super ( maxSize ) ; } @Override protected boolean lessThan ( ScoreTerm a , ScoreTerm b ) { return a . score < b . score ; } } private static class ScoreTerm { String word ; String topField ; float score ; float idf ; int docFreq ; int tf ; ScoreTerm ( String word , String topField , float score , float idf , int docFreq , int tf ) { this . word = word ; this . topField = topField ; this . score = score ; this . idf = idf ; this . docFreq = docFreq ; this . tf = tf ; } void update ( String word , String topField , float score , float idf , int docFreq , int tf ) { this . word = word ; this . topField = topField ; this . score = score ; this . idf = idf ; this . docFreq = docFreq ; this . tf = tf ; } } private static class Int { int x ; Int ( ) { x = <int> ; } } } 
