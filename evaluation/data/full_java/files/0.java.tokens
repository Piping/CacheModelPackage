import java . io . IOException ; import java . nio . ByteBuffer ; import java . util . * ; import java . util . Map . Entry ; import org . apache . cassandra . hadoop . cql3 . CqlConfigHelper ; import org . apache . cassandra . hadoop . cql3 . CqlOutputFormat ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . hadoop . cql3 . CqlInputFormat ; import org . apache . cassandra . hadoop . ConfigHelper ; import org . apache . cassandra . utils . ByteBufferUtil ; import org . apache . hadoop . conf . Configuration ; import org . apache . hadoop . conf . Configured ; import org . apache . hadoop . fs . Path ; import org . apache . hadoop . io . IntWritable ; import org . apache . hadoop . io . Text ; import org . apache . hadoop . mapreduce . Job ; import org . apache . hadoop . mapreduce . Mapper ; import org . apache . hadoop . mapreduce . Reducer ; import org . apache . hadoop . mapreduce . lib . output . FileOutputFormat ; import org . apache . hadoop . util . Tool ; import org . apache . hadoop . util . ToolRunner ; import com . datastax . driver . core . Row ; public class WordCount extends Configured implements Tool { private static final Logger logger = LoggerFactory . getLogger ( WordCount . class ) ; static final String INPUT_MAPPER_VAR = <str> ; static final String KEYSPACE = <str> ; static final String COLUMN_FAMILY = <str> ; static final String OUTPUT_REDUCER_VAR = <str> ; static final String OUTPUT_COLUMN_FAMILY = <str> ; private static final String OUTPUT_PATH_PREFIX = <str> ; private static final String PRIMARY_KEY = <str> ; public static void main ( String [ ] args ) throws Exception { ToolRunner . run ( new Configuration ( ) , new WordCount ( ) , args ) ; System . exit ( <int> ) ; } public static class TokenizerMapper extends Mapper < Map < String , ByteBuffer > , Map < String , ByteBuffer > , Text , IntWritable > { private final static IntWritable one = new IntWritable ( <int> ) ; private Text word = new Text ( ) ; private ByteBuffer sourceColumn ; protected void setup ( org . apache . hadoop . mapreduce . Mapper . Context context ) throws IOException , InterruptedException { } public void map ( Map < String , ByteBuffer > keys , Map < String , ByteBuffer > columns , Context context ) throws IOException , InterruptedException { for ( Entry < String , ByteBuffer > column : columns . entrySet ( ) ) { if ( ! <str> . equalsIgnoreCase ( column . getKey ( ) ) ) continue ; String value = ByteBufferUtil . string ( column . getValue ( ) ) ; StringTokenizer itr = new StringTokenizer ( value ) ; while ( itr . hasMoreTokens ( ) ) { word . set ( itr . nextToken ( ) ) ; context . write ( word , one ) ; } } } } public static class NativeTokenizerMapper extends Mapper < Long , Row , Text , IntWritable > { private final static IntWritable one = new IntWritable ( <int> ) ; private Text word = new Text ( ) ; private ByteBuffer sourceColumn ; protected void setup ( org . apache . hadoop . mapreduce . Mapper . Context context ) throws IOException , InterruptedException { } public void map ( Long key , Row row , Context context ) throws IOException , InterruptedException { String value = row . getString ( <str> ) ; logger . debug ( <str> , key , <str> , value , context . getInputSplit ( ) ) ; StringTokenizer itr = new StringTokenizer ( value ) ; while ( itr . hasMoreTokens ( ) ) { word . set ( itr . nextToken ( ) ) ; context . write ( word , one ) ; } } } public static class ReducerToFilesystem extends Reducer < Text , IntWritable , Text , IntWritable > { public void reduce ( Text key , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = <int> ; for ( IntWritable val : values ) sum + = val . get ( ) ; context . write ( key , new IntWritable ( sum ) ) ; } } public static class ReducerToCassandra extends Reducer < Text , IntWritable , Map < String , ByteBuffer > , List < ByteBuffer > > { private Map < String , ByteBuffer > keys ; private ByteBuffer key ; protected void setup ( org . apache . hadoop . mapreduce . Reducer . Context context ) throws IOException , InterruptedException { keys = new LinkedHashMap < String , ByteBuffer > ( ) ; } public void reduce ( Text word , Iterable < IntWritable > values , Context context ) throws IOException , InterruptedException { int sum = <int> ; for ( IntWritable val : values ) sum + = val . get ( ) ; keys . put ( <str> , ByteBufferUtil . bytes ( word . toString ( ) ) ) ; context . write ( keys , getBindVariables ( word , sum ) ) ; } private List < ByteBuffer > getBindVariables ( Text word , int sum ) { List < ByteBuffer > variables = new ArrayList < ByteBuffer > ( ) ; variables . add ( ByteBufferUtil . bytes ( String . valueOf ( sum ) ) ) ; return variables ; } } public int run ( String [ ] args ) throws Exception { String outputReducerType = <str> ; String inputMapperType = <str> ; String outputReducer = null ; String inputMapper = null ; if ( args ! = null ) { if ( args [ <int> ] . startsWith ( OUTPUT_REDUCER_VAR ) ) outputReducer = args [ <int> ] ; if ( args [ <int> ] . startsWith ( INPUT_MAPPER_VAR ) ) inputMapper = args [ <int> ] ; if ( args . length = = <int> ) { if ( args [ <int> ] . startsWith ( OUTPUT_REDUCER_VAR ) ) outputReducer = args [ <int> ] ; if ( args [ <int> ] . startsWith ( INPUT_MAPPER_VAR ) ) inputMapper = args [ <int> ] ; } } if ( outputReducer ! = null ) { String [ ] s = outputReducer . split ( <str> ) ; if ( s ! = null & & s . length = = <int> ) outputReducerType = s [ <int> ] ; } logger . info ( <str> + outputReducerType ) ; if ( inputMapper ! = null ) { String [ ] s = inputMapper . split ( <str> ) ; if ( s ! = null & & s . length = = <int> ) inputMapperType = s [ <int> ] ; } Job job = new Job ( getConf ( ) , <str> ) ; job . setJarByClass ( WordCount . class ) ; if ( outputReducerType . equalsIgnoreCase ( <str> ) ) { job . setCombinerClass ( ReducerToFilesystem . class ) ; job . setReducerClass ( ReducerToFilesystem . class ) ; job . setOutputKeyClass ( Text . class ) ; job . setOutputValueClass ( IntWritable . class ) ; FileOutputFormat . setOutputPath ( job , new Path ( OUTPUT_PATH_PREFIX ) ) ; } else { job . setReducerClass ( ReducerToCassandra . class ) ; job . setMapOutputKeyClass ( Text . class ) ; job . setMapOutputValueClass ( IntWritable . class ) ; job . setOutputKeyClass ( Map . class ) ; job . setOutputValueClass ( List . class ) ; job . setOutputFormatClass ( CqlOutputFormat . class ) ; ConfigHelper . setOutputColumnFamily ( job . getConfiguration ( ) , KEYSPACE , OUTPUT_COLUMN_FAMILY ) ; job . getConfiguration ( ) . set ( PRIMARY_KEY , <str> ) ; String query = <str> + KEYSPACE + <str> + OUTPUT_COLUMN_FAMILY + <str> ; CqlConfigHelper . setOutputCql ( job . getConfiguration ( ) , query ) ; ConfigHelper . setOutputInitialAddress ( job . getConfiguration ( ) , <str> ) ; ConfigHelper . setOutputPartitioner ( job . getConfiguration ( ) , <str> ) ; } if ( inputMapperType . equalsIgnoreCase ( <str> ) ) { job . setMapperClass ( NativeTokenizerMapper . class ) ; job . setInputFormatClass ( CqlInputFormat . class ) ; CqlConfigHelper . setInputCql ( job . getConfiguration ( ) , <str> + COLUMN_FAMILY + <str> ) ; } else { job . setMapperClass ( TokenizerMapper . class ) ; job . setInputFormatClass ( CqlInputFormat . class ) ; ConfigHelper . setInputRpcPort ( job . getConfiguration ( ) , <str> ) ; } ConfigHelper . setInputInitialAddress ( job . getConfiguration ( ) , <str> ) ; ConfigHelper . setInputColumnFamily ( job . getConfiguration ( ) , KEYSPACE , COLUMN_FAMILY ) ; ConfigHelper . setInputPartitioner ( job . getConfiguration ( ) , <str> ) ; CqlConfigHelper . setInputCQLPageRowSize ( job . getConfiguration ( ) , <str> ) ; job . waitForCompletion ( true ) ; return <int> ; } } 
