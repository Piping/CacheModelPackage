package org . apache . cassandra . hadoop . cql3 ; import java . io . IOException ; import java . util . * ; import java . util . concurrent . * ; import com . datastax . driver . core . Host ; import com . datastax . driver . core . Metadata ; import com . datastax . driver . core . ResultSet ; import com . datastax . driver . core . Row ; import com . datastax . driver . core . Session ; import com . datastax . driver . core . TokenRange ; import org . apache . hadoop . conf . Configuration ; import org . apache . hadoop . mapred . InputSplit ; import org . apache . hadoop . mapred . JobConf ; import org . apache . hadoop . mapred . RecordReader ; import org . apache . hadoop . mapred . Reporter ; import org . apache . hadoop . mapreduce . JobContext ; import org . apache . hadoop . mapreduce . TaskAttemptContext ; import org . apache . hadoop . mapreduce . TaskAttemptID ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . db . SystemKeyspace ; import org . apache . cassandra . dht . * ; import org . apache . cassandra . thrift . KeyRange ; import org . apache . cassandra . hadoop . * ; public class CqlInputFormat extends org . apache . hadoop . mapreduce . InputFormat < Long , Row > implements org . apache . hadoop . mapred . InputFormat < Long , Row > { public static final String MAPRED_TASK_ID = <str> ; private static final Logger logger = LoggerFactory . getLogger ( CqlInputFormat . class ) ; private String keyspace ; private String cfName ; private IPartitioner partitioner ; private Session session ; public RecordReader < Long , Row > getRecordReader ( InputSplit split , JobConf jobConf , final Reporter reporter ) throws IOException { TaskAttemptContext tac = HadoopCompat . newMapContext ( jobConf , TaskAttemptID . forName ( jobConf . get ( MAPRED_TASK_ID ) ) , null , null , null , new ReporterWrapper ( reporter ) , null ) ; CqlRecordReader recordReader = new CqlRecordReader ( ) ; recordReader . initialize ( ( org . apache . hadoop . mapreduce . InputSplit ) split , tac ) ; return recordReader ; } @Override public org . apache . hadoop . mapreduce . RecordReader < Long , Row > createRecordReader ( org . apache . hadoop . mapreduce . InputSplit arg0 , TaskAttemptContext arg1 ) throws IOException , InterruptedException { return new CqlRecordReader ( ) ; } protected void validateConfiguration ( Configuration conf ) { if ( ConfigHelper . getInputKeyspace ( conf ) = = null | | ConfigHelper . getInputColumnFamily ( conf ) = = null ) { throw new UnsupportedOperationException ( <str> ) ; } if ( ConfigHelper . getInputInitialAddress ( conf ) = = null ) throw new UnsupportedOperationException ( <str> ) ; if ( ConfigHelper . getInputPartitioner ( conf ) = = null ) throw new UnsupportedOperationException ( <str> ) ; } public List < org . apache . hadoop . mapreduce . InputSplit > getSplits ( JobContext context ) throws IOException { Configuration conf = HadoopCompat . getConfiguration ( context ) ; validateConfiguration ( conf ) ; keyspace = ConfigHelper . getInputKeyspace ( conf ) ; cfName = ConfigHelper . getInputColumnFamily ( conf ) ; partitioner = ConfigHelper . getInputPartitioner ( conf ) ; logger . trace ( <str> , partitioner ) ; Map < TokenRange , Set < Host > > masterRangeNodes = getRangeMap ( conf , keyspace ) ; ExecutorService executor = new ThreadPoolExecutor ( <int> , <int> , <int> , TimeUnit . SECONDS , new LinkedBlockingQueue < Runnable > ( ) ) ; List < org . apache . hadoop . mapreduce . InputSplit > splits = new ArrayList < > ( ) ; try { List < Future < List < org . apache . hadoop . mapreduce . InputSplit > > > splitfutures = new ArrayList < > ( ) ; KeyRange jobKeyRange = ConfigHelper . getInputKeyRange ( conf ) ; Range < Token > jobRange = null ; if ( jobKeyRange ! = null ) { if ( jobKeyRange . start_key ! = null ) { if ( ! partitioner . preservesOrder ( ) ) throw new UnsupportedOperationException ( <str> ) ; if ( jobKeyRange . start_token ! = null ) throw new IllegalArgumentException ( <str> ) ; if ( jobKeyRange . end_token ! = null ) throw new IllegalArgumentException ( <str> ) ; jobRange = new Range < > ( partitioner . getToken ( jobKeyRange . start_key ) , partitioner . getToken ( jobKeyRange . end_key ) ) ; } else if ( jobKeyRange . start_token ! = null ) { jobRange = new Range < > ( partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . start_token ) , partitioner . getTokenFactory ( ) . fromString ( jobKeyRange . end_token ) ) ; } else { logger . warn ( <str> ) ; } } session = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( <str> ) , conf ) . connect ( ) ; Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; for ( TokenRange range : masterRangeNodes . keySet ( ) ) { if ( jobRange = = null ) { splitfutures . add ( executor . submit ( new SplitCallable ( range , masterRangeNodes . get ( range ) , conf ) ) ) ; } else { TokenRange jobTokenRange = rangeToTokenRange ( metadata , jobRange ) ; if ( range . intersects ( jobTokenRange ) ) { for ( TokenRange intersection : range . intersectWith ( jobTokenRange ) ) { splitfutures . add ( executor . submit ( new SplitCallable ( intersection , masterRangeNodes . get ( range ) , conf ) ) ) ; } } } } for ( Future < List < org . apache . hadoop . mapreduce . InputSplit > > futureInputSplits : splitfutures ) { try { splits . addAll ( futureInputSplits . get ( ) ) ; } catch ( Exception e ) { throw new IOException ( <str> , e ) ; } } } finally { executor . shutdownNow ( ) ; } assert splits . size ( ) > <int> ; Collections . shuffle ( splits , new Random ( System . nanoTime ( ) ) ) ; return splits ; } private TokenRange rangeToTokenRange ( Metadata metadata , Range < Token > range ) { return metadata . newTokenRange ( metadata . newToken ( partitioner . getTokenFactory ( ) . toString ( range . left ) ) , metadata . newToken ( partitioner . getTokenFactory ( ) . toString ( range . right ) ) ) ; } private Map < TokenRange , Long > getSubSplits ( String keyspace , String cfName , TokenRange range , Configuration conf ) throws IOException { int splitSize = ConfigHelper . getInputSplitSize ( conf ) ; int splitSizeMb = ConfigHelper . getInputSplitSizeInMb ( conf ) ; try { return describeSplits ( keyspace , cfName , range , splitSize , splitSizeMb ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } } private Map < TokenRange , Set < Host > > getRangeMap ( Configuration conf , String keyspace ) { try ( Session session = CqlConfigHelper . getInputCluster ( ConfigHelper . getInputInitialAddress ( conf ) . split ( <str> ) , conf ) . connect ( ) ) { Map < TokenRange , Set < Host > > map = new HashMap < > ( ) ; Metadata metadata = session . getCluster ( ) . getMetadata ( ) ; for ( TokenRange tokenRange : metadata . getTokenRanges ( ) ) map . put ( tokenRange , metadata . getReplicas ( <str> + keyspace + <str> , tokenRange ) ) ; return map ; } } private Map < TokenRange , Long > describeSplits ( String keyspace , String table , TokenRange tokenRange , int splitSize , int splitSizeMb ) { String query = String . format ( <str> + <str> + <str> , SystemKeyspace . NAME , SystemKeyspace . SIZE_ESTIMATES ) ; ResultSet resultSet = session . execute ( query , keyspace , table , tokenRange . getStart ( ) . toString ( ) , tokenRange . getEnd ( ) . toString ( ) ) ; Row row = resultSet . one ( ) ; long meanPartitionSize = <int> ; long partitionCount = <int> ; int splitCount = <int> ; if ( row ! = null ) { meanPartitionSize = row . getLong ( <str> ) ; partitionCount = row . getLong ( <str> ) ; splitCount = splitSizeMb > <int> ? ( int ) ( meanPartitionSize * partitionCount / splitSizeMb / <int> / <int> ) : ( int ) ( partitionCount / splitSize ) ; } if ( splitCount = = <int> ) { Map < TokenRange , Long > wrappedTokenRange = new HashMap < > ( ) ; wrappedTokenRange . put ( tokenRange , ( long ) <int> ) ; return wrappedTokenRange ; } List < TokenRange > splitRanges = tokenRange . splitEvenly ( splitCount ) ; Map < TokenRange , Long > rangesWithLength = new HashMap < > ( ) ; for ( TokenRange range : splitRanges ) rangesWithLength . put ( range , partitionCount / splitCount ) ; return rangesWithLength ; } public InputSplit [ ] getSplits ( JobConf jobConf , int numSplits ) throws IOException { TaskAttemptContext tac = HadoopCompat . newTaskAttemptContext ( jobConf , new TaskAttemptID ( ) ) ; List < org . apache . hadoop . mapreduce . InputSplit > newInputSplits = this . getSplits ( tac ) ; InputSplit [ ] oldInputSplits = new InputSplit [ newInputSplits . size ( ) ] ; for ( int i = <int> ; i < newInputSplits . size ( ) ; i + + ) oldInputSplits [ i ] = ( ColumnFamilySplit ) newInputSplits . get ( i ) ; return oldInputSplits ; } class SplitCallable implements Callable < List < org . apache . hadoop . mapreduce . InputSplit > > { private final TokenRange tokenRange ; private final Set < Host > hosts ; private final Configuration conf ; public SplitCallable ( TokenRange tr , Set < Host > hosts , Configuration conf ) { this . tokenRange = tr ; this . hosts = hosts ; this . conf = conf ; } public List < org . apache . hadoop . mapreduce . InputSplit > call ( ) throws Exception { ArrayList < org . apache . hadoop . mapreduce . InputSplit > splits = new ArrayList < > ( ) ; Map < TokenRange , Long > subSplits ; subSplits = getSubSplits ( keyspace , cfName , tokenRange , conf ) ; String [ ] endpoints = new String [ hosts . size ( ) ] ; int endpointIndex = <int> ; for ( Host endpoint : hosts ) endpoints [ endpointIndex + + ] = endpoint . getAddress ( ) . getHostName ( ) ; boolean partitionerIsOpp = partitioner instanceof OrderPreservingPartitioner | | partitioner instanceof ByteOrderedPartitioner ; for ( TokenRange subSplit : subSplits . keySet ( ) ) { List < TokenRange > ranges = subSplit . unwrap ( ) ; for ( TokenRange subrange : ranges ) { ColumnFamilySplit split = new ColumnFamilySplit ( partitionerIsOpp ? subrange . getStart ( ) . toString ( ) . substring ( <int> ) : subrange . getStart ( ) . toString ( ) , partitionerIsOpp ? subrange . getEnd ( ) . toString ( ) . substring ( <int> ) : subrange . getEnd ( ) . toString ( ) , subSplits . get ( subSplit ) , endpoints ) ; logger . trace ( <str> , split ) ; splits . add ( split ) ; } } return splits ; } } } 
