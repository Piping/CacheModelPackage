package org . elasticsearch . indices . flush ; import org . elasticsearch . action . ActionListener ; import org . elasticsearch . action . admin . indices . flush . FlushResponse ; import org . elasticsearch . action . admin . indices . stats . IndexStats ; import org . elasticsearch . action . admin . indices . stats . ShardStats ; import org . elasticsearch . cluster . ClusterState ; import org . elasticsearch . cluster . metadata . IndexMetaData ; import org . elasticsearch . cluster . routing . ShardRouting ; import org . elasticsearch . cluster . routing . allocation . command . MoveAllocationCommand ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . index . engine . Engine ; import org . elasticsearch . index . shard . ShardId ; import org . elasticsearch . test . ESIntegTestCase ; import org . elasticsearch . test . junit . annotations . TestLogging ; import java . io . IOException ; import java . util . Arrays ; import java . util . List ; import java . util . Map ; import java . util . concurrent . CopyOnWriteArrayList ; import java . util . concurrent . CountDownLatch ; import java . util . concurrent . ExecutionException ; import java . util . concurrent . atomic . AtomicBoolean ; import java . util . concurrent . atomic . AtomicInteger ; import static org . hamcrest . Matchers . emptyIterable ; import static org . hamcrest . Matchers . equalTo ; public class FlushIT extends ESIntegTestCase { public void testWaitIfOngoing ( ) throws InterruptedException { createIndex ( <str> ) ; ensureGreen ( <str> ) ; final int numIters = scaledRandomIntBetween ( <int> , <int> ) ; for ( int i = <int> ; i < numIters ; i + + ) { for ( int j = <int> ; j < <int> ; j + + ) { client ( ) . prepareIndex ( <str> , <str> ) . setSource ( <str> ) . get ( ) ; } final CountDownLatch latch = new CountDownLatch ( <int> ) ; final CopyOnWriteArrayList < Throwable > errors = new CopyOnWriteArrayList < > ( ) ; for ( int j = <int> ; j < <int> ; j + + ) { client ( ) . admin ( ) . indices ( ) . prepareFlush ( <str> ) . setWaitIfOngoing ( true ) . execute ( new ActionListener < FlushResponse > ( ) { @Override public void onResponse ( FlushResponse flushResponse ) { try { assertThat ( <str> + Arrays . toString ( flushResponse . getShardFailures ( ) ) , flushResponse . getFailedShards ( ) , equalTo ( <int> ) ) ; latch . countDown ( ) ; } catch ( Throwable ex ) { onFailure ( ex ) ; } } @Override public void onFailure ( Throwable e ) { errors . add ( e ) ; latch . countDown ( ) ; } } ) ; } latch . await ( ) ; assertThat ( errors , emptyIterable ( ) ) ; } } @TestLogging ( <str> ) public void testSyncedFlush ( ) throws ExecutionException , InterruptedException , IOException { internalCluster ( ) . ensureAtLeastNumDataNodes ( <int> ) ; prepareCreate ( <str> ) . setSettings ( IndexMetaData . SETTING_NUMBER_OF_SHARDS , <int> ) . get ( ) ; ensureGreen ( ) ; IndexStats indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; for ( ShardStats shardStats : indexStats . getShards ( ) ) { assertNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } ShardsSyncedFlushResult result ; if ( randomBoolean ( ) ) { logger . info ( <str> ) ; result = SyncedFlushUtil . attemptSyncedFlush ( internalCluster ( ) , new ShardId ( <str> , <int> ) ) ; } else { logger . info ( <str> ) ; IndicesSyncedFlushResult indicesResult = SyncedFlushUtil . attemptSyncedFlush ( internalCluster ( ) , <str> ) ; result = indicesResult . getShardsResultPerIndex ( ) . get ( <str> ) . get ( <int> ) ; } assertFalse ( result . failed ( ) ) ; assertThat ( result . totalShards ( ) , equalTo ( indexStats . getShards ( ) . length ) ) ; assertThat ( result . successfulShards ( ) , equalTo ( indexStats . getShards ( ) . length ) ) ; indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; String syncId = result . syncId ( ) ; for ( ShardStats shardStats : indexStats . getShards ( ) ) { final String shardSyncId = shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ; assertThat ( shardSyncId , equalTo ( syncId ) ) ; } String newNodeName = internalCluster ( ) . startNode ( ) ; ClusterState clusterState = client ( ) . admin ( ) . cluster ( ) . prepareState ( ) . get ( ) . getState ( ) ; ShardRouting shardRouting = clusterState . getRoutingTable ( ) . index ( <str> ) . shard ( <int> ) . iterator ( ) . next ( ) ; String currentNodeName = clusterState . nodes ( ) . resolveNode ( shardRouting . currentNodeId ( ) ) . name ( ) ; assertFalse ( currentNodeName . equals ( newNodeName ) ) ; internalCluster ( ) . client ( ) . admin ( ) . cluster ( ) . prepareReroute ( ) . add ( new MoveAllocationCommand ( new ShardId ( <str> , <int> ) , currentNodeName , newNodeName ) ) . get ( ) ; client ( ) . admin ( ) . cluster ( ) . prepareHealth ( ) . setWaitForRelocatingShards ( <int> ) . get ( ) ; indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; for ( ShardStats shardStats : indexStats . getShards ( ) ) { assertNotNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } client ( ) . admin ( ) . indices ( ) . prepareUpdateSettings ( <str> ) . setSettings ( Settings . builder ( ) . put ( IndexMetaData . SETTING_NUMBER_OF_REPLICAS , <int> ) . build ( ) ) . get ( ) ; ensureGreen ( <str> ) ; indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; for ( ShardStats shardStats : indexStats . getShards ( ) ) { assertNotNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } client ( ) . admin ( ) . indices ( ) . prepareUpdateSettings ( <str> ) . setSettings ( Settings . builder ( ) . put ( IndexMetaData . SETTING_NUMBER_OF_REPLICAS , internalCluster ( ) . numDataNodes ( ) - <int> ) . build ( ) ) . get ( ) ; ensureGreen ( <str> ) ; indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; for ( ShardStats shardStats : indexStats . getShards ( ) ) { assertNotNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } } @TestLogging ( <str> ) public void testSyncedFlushWithConcurrentIndexing ( ) throws Exception { internalCluster ( ) . ensureAtLeastNumDataNodes ( <int> ) ; createIndex ( <str> ) ; client ( ) . admin ( ) . indices ( ) . prepareUpdateSettings ( <str> ) . setSettings ( Settings . builder ( ) . put ( <str> , true ) . put ( <str> , - <int> ) . put ( <str> , internalCluster ( ) . numDataNodes ( ) - <int> ) ) . get ( ) ; ensureGreen ( ) ; final AtomicBoolean stop = new AtomicBoolean ( false ) ; final AtomicInteger numDocs = new AtomicInteger ( <int> ) ; Thread indexingThread = new Thread ( ) { @Override public void run ( ) { while ( stop . get ( ) = = false ) { client ( ) . prepareIndex ( ) . setIndex ( <str> ) . setType ( <str> ) . setSource ( <str> ) . get ( ) ; numDocs . incrementAndGet ( ) ; } } } ; indexingThread . start ( ) ; IndexStats indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; for ( ShardStats shardStats : indexStats . getShards ( ) ) { assertNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } logger . info ( <str> ) ; IndicesSyncedFlushResult syncedFlushResult = SyncedFlushUtil . attemptSyncedFlush ( internalCluster ( ) , <str> ) ; logger . info ( <str> ) ; stop . set ( true ) ; indexingThread . join ( ) ; indexStats = client ( ) . admin ( ) . indices ( ) . prepareStats ( <str> ) . get ( ) . getIndex ( <str> ) ; assertFlushResponseEqualsShardStats ( indexStats . getShards ( ) , syncedFlushResult . getShardsResultPerIndex ( ) . get ( <str> ) ) ; refresh ( ) ; assertThat ( client ( ) . prepareSearch ( ) . setSize ( <int> ) . get ( ) . getHits ( ) . totalHits ( ) , equalTo ( ( long ) numDocs . get ( ) ) ) ; logger . info ( <str> , client ( ) . prepareSearch ( ) . setSize ( <int> ) . get ( ) . getHits ( ) . totalHits ( ) ) ; logClusterState ( ) ; internalCluster ( ) . fullRestart ( ) ; ensureGreen ( ) ; assertThat ( client ( ) . prepareSearch ( ) . setSize ( <int> ) . get ( ) . getHits ( ) . totalHits ( ) , equalTo ( ( long ) numDocs . get ( ) ) ) ; } private void assertFlushResponseEqualsShardStats ( ShardStats [ ] shardsStats , List < ShardsSyncedFlushResult > syncedFlushResults ) { for ( final ShardStats shardStats : shardsStats ) { for ( final ShardsSyncedFlushResult shardResult : syncedFlushResults ) { if ( shardStats . getShardRouting ( ) . getId ( ) = = shardResult . shardId ( ) . getId ( ) ) { for ( Map . Entry < ShardRouting , SyncedFlushService . SyncedFlushResponse > singleResponse : shardResult . shardResponses ( ) . entrySet ( ) ) { if ( singleResponse . getKey ( ) . currentNodeId ( ) . equals ( shardStats . getShardRouting ( ) . currentNodeId ( ) ) ) { if ( singleResponse . getValue ( ) . success ( ) ) { logger . info ( <str> , singleResponse . getKey ( ) . shardId ( ) , singleResponse . getKey ( ) . currentNodeId ( ) ) ; assertNotNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } else { logger . info ( <str> , singleResponse . getKey ( ) . shardId ( ) , singleResponse . getKey ( ) . currentNodeId ( ) ) ; assertNull ( shardStats . getCommitStats ( ) . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } } } } } } } public void testUnallocatedShardsDoesNotHang ( ) throws InterruptedException { prepareCreate ( <str> ) . setSettings ( Settings . builder ( ) . put ( <str> , <str> ) ) . get ( ) ; List < ShardsSyncedFlushResult > shardsResult = SyncedFlushUtil . attemptSyncedFlush ( internalCluster ( ) , <str> ) . getShardsResultPerIndex ( ) . get ( <str> ) ; int numShards = client ( ) . admin ( ) . indices ( ) . prepareGetSettings ( <str> ) . get ( ) . getIndexToSettings ( ) . get ( <str> ) . getAsInt ( IndexMetaData . SETTING_NUMBER_OF_SHARDS , - <int> ) ; assertThat ( shardsResult . size ( ) , equalTo ( numShards ) ) ; assertThat ( shardsResult . get ( <int> ) . failureReason ( ) , equalTo ( <str> ) ) ; } } 
