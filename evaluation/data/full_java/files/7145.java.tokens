package org . elasticsearch . search . aggregations . bucket ; import org . apache . lucene . index . LeafReaderContext ; import org . apache . lucene . search . LeafCollector ; import org . apache . lucene . search . ScoreDoc ; import org . apache . lucene . search . Scorer ; import org . apache . lucene . search . TopDocs ; import org . apache . lucene . search . TopDocsCollector ; import org . apache . lucene . search . TopScoreDocCollector ; import org . elasticsearch . ElasticsearchException ; import org . elasticsearch . common . lease . Releasable ; import org . elasticsearch . common . lease . Releasables ; import org . elasticsearch . common . util . BigArrays ; import org . elasticsearch . common . util . ObjectArray ; import org . elasticsearch . search . aggregations . BucketCollector ; import org . elasticsearch . search . aggregations . LeafBucketCollector ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Arrays ; import java . util . Comparator ; import java . util . List ; public class BestDocsDeferringCollector extends DeferringBucketCollector implements Releasable { final List < PerSegmentCollects > entries = new ArrayList < > ( ) ; BucketCollector deferred ; ObjectArray < PerParentBucketSamples > perBucketSamples ; private int shardSize ; private PerSegmentCollects perSegCollector ; private final BigArrays bigArrays ; public BestDocsDeferringCollector ( int shardSize , BigArrays bigArrays ) { this . shardSize = shardSize ; this . bigArrays = bigArrays ; perBucketSamples = bigArrays . newObjectArray ( <int> ) ; } @Override public boolean needsScores ( ) { return true ; } public void setDeferredCollector ( Iterable < BucketCollector > deferredCollectors ) { this . deferred = BucketCollector . wrap ( deferredCollectors ) ; } @Override public LeafBucketCollector getLeafCollector ( LeafReaderContext ctx ) throws IOException { perSegCollector = new PerSegmentCollects ( ctx ) ; entries . add ( perSegCollector ) ; return new LeafBucketCollector ( ) { @Override public void setScorer ( Scorer scorer ) throws IOException { perSegCollector . setScorer ( scorer ) ; } @Override public void collect ( int doc , long bucket ) throws IOException { perSegCollector . collect ( doc , bucket ) ; } } ; } protected TopDocsCollector < ? extends ScoreDoc > createTopDocsCollector ( int size ) throws IOException { return TopScoreDocCollector . create ( size ) ; } @Override public void preCollection ( ) throws IOException { } @Override public void postCollection ( ) throws IOException { runDeferredAggs ( ) ; } @Override public void prepareSelectedBuckets ( long . . . selectedBuckets ) throws IOException { } private void runDeferredAggs ( ) throws IOException { deferred . preCollection ( ) ; List < ScoreDoc > allDocs = new ArrayList < > ( shardSize ) ; for ( int i = <int> ; i < perBucketSamples . size ( ) ; i + + ) { PerParentBucketSamples perBucketSample = perBucketSamples . get ( i ) ; if ( perBucketSample = = null ) { continue ; } perBucketSample . getMatches ( allDocs ) ; } ScoreDoc [ ] docsArr = allDocs . toArray ( new ScoreDoc [ allDocs . size ( ) ] ) ; Arrays . sort ( docsArr , new Comparator < ScoreDoc > ( ) { @Override public int compare ( ScoreDoc o1 , ScoreDoc o2 ) { if ( o1 . doc = = o2 . doc ) { return o1 . shardIndex - o2 . shardIndex ; } return o1 . doc - o2 . doc ; } } ) ; try { for ( PerSegmentCollects perSegDocs : entries ) { perSegDocs . replayRelatedMatches ( docsArr ) ; } } catch ( IOException e ) { throw new ElasticsearchException ( <str> , e ) ; } deferred . postCollection ( ) ; } class PerParentBucketSamples { private LeafCollector currentLeafCollector ; private TopDocsCollector < ? extends ScoreDoc > tdc ; private long parentBucket ; private int matchedDocs ; public PerParentBucketSamples ( long parentBucket , Scorer scorer , LeafReaderContext readerContext ) { try { this . parentBucket = parentBucket ; tdc = createTopDocsCollector ( shardSize ) ; currentLeafCollector = tdc . getLeafCollector ( readerContext ) ; setScorer ( scorer ) ; } catch ( IOException e ) { throw new ElasticsearchException ( <str> , e ) ; } } public void getMatches ( List < ScoreDoc > allDocs ) { TopDocs topDocs = tdc . topDocs ( ) ; ScoreDoc [ ] sd = topDocs . scoreDocs ; matchedDocs = sd . length ; for ( ScoreDoc scoreDoc : sd ) { scoreDoc . shardIndex = ( int ) parentBucket ; } allDocs . addAll ( Arrays . asList ( sd ) ) ; } public void collect ( int doc ) throws IOException { currentLeafCollector . collect ( doc ) ; } public void setScorer ( Scorer scorer ) throws IOException { currentLeafCollector . setScorer ( scorer ) ; } public void changeSegment ( LeafReaderContext readerContext ) throws IOException { currentLeafCollector = tdc . getLeafCollector ( readerContext ) ; } public int getDocCount ( ) { return matchedDocs ; } } class PerSegmentCollects extends Scorer { private LeafReaderContext readerContext ; int maxDocId = Integer . MIN_VALUE ; private float currentScore ; private int currentDocId = - <int> ; private Scorer currentScorer ; PerSegmentCollects ( LeafReaderContext readerContext ) throws IOException { super ( null ) ; this . readerContext = readerContext ; for ( int i = <int> ; i < perBucketSamples . size ( ) ; i + + ) { PerParentBucketSamples perBucketSample = perBucketSamples . get ( i ) ; if ( perBucketSample = = null ) { continue ; } perBucketSample . changeSegment ( readerContext ) ; } } public void setScorer ( Scorer scorer ) throws IOException { this . currentScorer = scorer ; for ( int i = <int> ; i < perBucketSamples . size ( ) ; i + + ) { PerParentBucketSamples perBucketSample = perBucketSamples . get ( i ) ; if ( perBucketSample = = null ) { continue ; } perBucketSample . setScorer ( scorer ) ; } } public void replayRelatedMatches ( ScoreDoc [ ] sd ) throws IOException { final LeafBucketCollector leafCollector = deferred . getLeafCollector ( readerContext ) ; leafCollector . setScorer ( this ) ; currentScore = <int> ; currentDocId = - <int> ; if ( maxDocId < <int> ) { return ; } for ( ScoreDoc scoreDoc : sd ) { int rebased = scoreDoc . doc - readerContext . docBase ; if ( ( rebased > = <int> ) & & ( rebased < = maxDocId ) ) { currentScore = scoreDoc . score ; currentDocId = rebased ; leafCollector . collect ( rebased , scoreDoc . shardIndex ) ; } } } @Override public float score ( ) throws IOException { return currentScore ; } @Override public int freq ( ) throws IOException { throw new ElasticsearchException ( <str> ) ; } @Override public int docID ( ) { return currentDocId ; } @Override public int nextDoc ( ) throws IOException { throw new ElasticsearchException ( <str> ) ; } @Override public int advance ( int target ) throws IOException { throw new ElasticsearchException ( <str> ) ; } @Override public long cost ( ) { throw new ElasticsearchException ( <str> ) ; } public void collect ( int docId , long parentBucket ) throws IOException { perBucketSamples = bigArrays . grow ( perBucketSamples , parentBucket + <int> ) ; PerParentBucketSamples sampler = perBucketSamples . get ( ( int ) parentBucket ) ; if ( sampler = = null ) { sampler = new PerParentBucketSamples ( parentBucket , currentScorer , readerContext ) ; perBucketSamples . set ( ( int ) parentBucket , sampler ) ; } sampler . collect ( docId ) ; maxDocId = Math . max ( maxDocId , docId ) ; } } public int getDocCount ( long parentBucket ) { PerParentBucketSamples sampler = perBucketSamples . get ( ( int ) parentBucket ) ; if ( sampler = = null ) { return <int> ; } return sampler . getDocCount ( ) ; } @Override public void close ( ) throws ElasticsearchException { Releasables . close ( perBucketSamples ) ; } } 
