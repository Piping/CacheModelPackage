package org . nd4j . linalg . lossfunctions ; import org . nd4j . linalg . api . ndarray . INDArray ; import org . nd4j . linalg . factory . Nd4j ; import java . util . Arrays ; import static org . nd4j . linalg . ops . transforms . Transforms . * ; public class LossFunctions { public static double score ( INDArray labels , LossFunction lossFunction , INDArray z , double l2 , double l1 , boolean useRegularization ) { return LossCalculation . builder ( ) . l1 ( l1 ) . lossFunction ( lossFunction ) . l2 ( l2 ) . labels ( labels ) . z ( z ) . useRegularization ( useRegularization ) . build ( ) . score ( ) ; } public static double score ( INDArray labels , LossFunction lossFunction , INDArray z , double l2 , boolean useRegularization ) { double ret = <float> ; double reg = <float> * l2 ; if ( ! Arrays . equals ( labels . shape ( ) , z . shape ( ) ) ) throw new IllegalArgumentException ( <str> ) ; boolean oldEnforce = Nd4j . ENFORCE_NUMERICAL_STABILITY ; Nd4j . ENFORCE_NUMERICAL_STABILITY = true ; switch ( lossFunction ) { case CUSTOM : throw new IllegalStateException ( <str> ) ; case RECONSTRUCTION_CROSSENTROPY : INDArray xEntLogZ2 = log ( z ) ; INDArray xEntOneMinusLabelsOut2 = labels . rsub ( <int> ) ; INDArray xEntOneMinusLogOneMinusZ2 = log ( z ) . rsubi ( <int> ) ; ret = labels . mul ( xEntLogZ2 ) . add ( xEntOneMinusLabelsOut2 ) . muli ( xEntOneMinusLogOneMinusZ2 ) . sum ( <int> ) . meanNumber ( ) . doubleValue ( ) ; break ; case MCXENT : INDArray sums = log ( z ) ; INDArray columnSums = labels . mul ( sums ) ; ret = - columnSums . sumNumber ( ) . doubleValue ( ) ; break ; case XENT : INDArray xEntLogZ = log ( z ) ; INDArray xEntOneMinusLabelsOut = labels . rsub ( <int> ) ; INDArray xEntOneMinusLogOneMinusZ = log ( z ) . rsubi ( <int> ) ; ret = labels . mul ( xEntLogZ ) . add ( xEntOneMinusLabelsOut ) . muli ( xEntOneMinusLogOneMinusZ ) . sum ( <int> ) . sumNumber ( ) . doubleValue ( ) ; break ; case RMSE_XENT : INDArray rmseXentDiff = labels . sub ( z ) ; INDArray squaredrmseXentDiff = pow ( rmseXentDiff , <float> ) ; INDArray sqrt = sqrt ( squaredrmseXentDiff ) ; ret = sqrt . sum ( <int> ) . sumNumber ( ) . doubleValue ( ) ; break ; case MSE : INDArray mseDelta = labels . sub ( z ) ; ret = <float> * pow ( mseDelta , <int> ) . sum ( <int> ) . sumNumber ( ) . doubleValue ( ) ; break ; case EXPLL : INDArray expLLLogZ = log ( z ) ; ret = z . sub ( labels . mul ( expLLLogZ ) ) . sum ( <int> ) . sumNumber ( ) . doubleValue ( ) ; break ; case SQUARED_LOSS : ret = pow ( labels . sub ( z ) , <int> ) . sum ( <int> ) . sumNumber ( ) . doubleValue ( ) ; break ; case NEGATIVELOGLIKELIHOOD : INDArray sums2 = log ( z ) ; INDArray columnSums2 = labels . mul ( sums2 ) ; ret = - columnSums2 . sumNumber ( ) . doubleValue ( ) ; break ; } if ( useRegularization ) ret + = reg ; ret / = ( double ) labels . size ( <int> ) ; Nd4j . ENFORCE_NUMERICAL_STABILITY = oldEnforce ; return ret ; } public enum LossFunction { MSE , EXPLL , XENT , MCXENT , RMSE_XENT , SQUARED_LOSS , RECONSTRUCTION_CROSSENTROPY , NEGATIVELOGLIKELIHOOD , CUSTOM } } 
