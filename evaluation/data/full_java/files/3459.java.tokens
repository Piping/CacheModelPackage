package org . nd4j . linalg . learning ; import org . junit . Test ; import org . nd4j . linalg . BaseNd4jTest ; import org . nd4j . linalg . api . ndarray . INDArray ; import org . nd4j . linalg . api . rng . distribution . Distribution ; import org . nd4j . linalg . factory . Nd4j ; import org . nd4j . linalg . factory . Nd4jBackend ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import java . util . Random ; import static org . junit . Assert . assertEquals ; public class UpdaterTest extends BaseNd4jTest { private static final Logger log = LoggerFactory . getLogger ( UpdaterTest . class ) ; public UpdaterTest ( String name , Nd4jBackend backend ) { super ( name , backend ) ; } public UpdaterTest ( ) { } public UpdaterTest ( Nd4jBackend backend ) { super ( backend ) ; } public UpdaterTest ( String name ) { super ( name ) ; } @Test public void testAdaGrad1 ( ) { int rows = <int> ; int cols = <int> ; AdaGrad grad = new AdaGrad ( rows , cols , <float> ) ; INDArray W = Nd4j . ones ( rows , cols ) ; assertEquals ( <float> , grad . getGradient ( W , <int> ) . getDouble ( <int> ) , <float> ) ; } @Test public void testAdaGradCombining ( ) { int n = <int> ; Nd4j . getRandom ( ) . setSeed ( <int> ) ; Random r = new Random ( <int> ) ; double [ ] lrs = new double [ n ] ; AdaGrad [ ] adaGrads = new AdaGrad [ n ] ; INDArray [ ] arr = new INDArray [ n ] ; double avgLr = <float> ; INDArray avgState = Nd4j . zeros ( <int> , <int> ) ; for ( int i = <int> ; i < arr . length ; i + + ) { lrs [ i ] = r . nextDouble ( ) ; avgLr + = lrs [ i ] ; adaGrads [ i ] = new AdaGrad ( lrs [ i ] ) ; arr [ i ] = Nd4j . rand ( <int> , <int> ) ; avgState . addi ( arr [ i ] ) ; adaGrads [ i ] . setHistoricalGradient ( arr [ i ] . dup ( ) ) ; } avgLr / = n ; avgState . divi ( n ) ; GradientUpdaterAggregator ag = adaGrads [ <int> ] . getAggregator ( true ) ; for ( int i = <int> ; i < n ; i + + ) { ag . aggregate ( adaGrads [ i ] ) ; } AdaGrad combined = ( AdaGrad ) ag . getUpdater ( ) ; double lrCombined = combined . getLearningRate ( ) ; INDArray histCombined = combined . getHistoricalGradient ( ) ; assertEquals ( avgLr , lrCombined , <float> ) ; assertEquals ( avgState , histCombined ) ; GradientUpdaterAggregator first = adaGrads [ <int> ] . getAggregator ( false ) ; GradientUpdaterAggregator second = adaGrads [ <int> ] . getAggregator ( false ) ; for ( int i = <int> ; i < n ; i + + ) { if ( i < <int> ) { first . aggregate ( adaGrads [ i ] ) ; } else { second . aggregate ( adaGrads [ i ] ) ; } } GradientUpdaterAggregator agMerged = first . combine ( second ) ; AdaGrad combined2 = ( AdaGrad ) agMerged . getUpdater ( ) ; assertEquals ( avgLr , combined2 . getLearningRate ( ) , <float> ) ; assertEquals ( avgState , combined2 . getHistoricalGradient ( ) ) ; } @Test public void testNesterovs ( ) { int rows = <int> ; int cols = <int> ; Nesterovs grad = new Nesterovs ( <float> ) ; INDArray W = Nd4j . zeros ( rows , cols ) ; Distribution dist = Nd4j . getDistributions ( ) . createNormal ( <int> , <int> ) ; for ( int i = <int> ; i < W . rows ( ) ; i + + ) W . putRow ( i , Nd4j . create ( dist . sample ( W . columns ( ) ) ) ) ; for ( int i = <int> ; i < <int> ; i + + ) { String learningRates = String . valueOf ( <str> + grad . getGradient ( W , i ) ) . replaceAll ( <str> , <str> ) ; System . out . println ( learningRates ) ; W . addi ( Nd4j . randn ( rows , cols ) ) ; } } @Test public void testNesterovCombining ( ) { int n = <int> ; Nd4j . getRandom ( ) . setSeed ( <int> ) ; Random r = new Random ( <int> ) ; double [ ] lrs = new double [ n ] ; double [ ] momentums = new double [ n ] ; Nesterovs [ ] nesterovs = new Nesterovs [ n ] ; INDArray [ ] vs = new INDArray [ n ] ; double avgLr = <float> ; double avgMomentums = <float> ; INDArray avgState = Nd4j . zeros ( <int> , <int> ) ; for ( int i = <int> ; i < vs . length ; i + + ) { lrs [ i ] = r . nextDouble ( ) ; momentums [ i ] = r . nextDouble ( ) ; avgLr + = lrs [ i ] ; avgMomentums + = momentums [ i ] ; nesterovs [ i ] = new Nesterovs ( momentums [ i ] , lrs [ i ] ) ; vs [ i ] = Nd4j . rand ( <int> , <int> ) ; avgState . addi ( vs [ i ] ) ; nesterovs [ i ] . setV ( vs [ i ] . dup ( ) ) ; } avgLr / = n ; avgMomentums / = n ; avgState . divi ( n ) ; GradientUpdaterAggregator ag = nesterovs [ <int> ] . getAggregator ( true ) ; for ( int i = <int> ; i < n ; i + + ) ag . aggregate ( nesterovs [ i ] ) ; Nesterovs combined = ( Nesterovs ) ag . getUpdater ( ) ; assertEquals ( avgLr , combined . getLearningRate ( ) , <float> ) ; assertEquals ( avgMomentums , combined . getMomentum ( ) , <float> ) ; assertEquals ( avgState , combined . getV ( ) ) ; GradientUpdaterAggregator first = nesterovs [ <int> ] . getAggregator ( false ) ; GradientUpdaterAggregator second = nesterovs [ <int> ] . getAggregator ( false ) ; for ( int i = <int> ; i < n ; i + + ) { if ( i < <int> ) { first . aggregate ( nesterovs [ i ] ) ; } else { second . aggregate ( nesterovs [ i ] ) ; } } GradientUpdaterAggregator agMerged = first . combine ( second ) ; Nesterovs combined2 = ( Nesterovs ) agMerged . getUpdater ( ) ; assertEquals ( avgLr , combined2 . getLearningRate ( ) , <float> ) ; assertEquals ( avgMomentums , combined2 . getMomentum ( ) , <float> ) ; assertEquals ( avgState , combined2 . getV ( ) ) ; } @Test public void testAdaGrad ( ) { int rows = <int> ; int cols = <int> ; AdaGrad grad = new AdaGrad ( rows , cols , <float> ) ; INDArray W = Nd4j . zeros ( rows , cols ) ; Distribution dist = Nd4j . getDistributions ( ) . createNormal ( <int> , <int> ) ; for ( int i = <int> ; i < W . rows ( ) ; i + + ) W . putRow ( i , Nd4j . create ( dist . sample ( W . columns ( ) ) ) ) ; for ( int i = <int> ; i < <int> ; i + + ) { String learningRates = String . valueOf ( <str> + grad . getGradient ( W , i ) ) . replaceAll ( <str> , <str> ) ; System . out . println ( learningRates ) ; W . addi ( Nd4j . randn ( rows , cols ) ) ; } } @Test public void testAdaDelta ( ) { int rows = <int> ; int cols = <int> ; AdaDelta grad = new AdaDelta ( ) ; INDArray W = Nd4j . zeros ( rows , cols ) ; Distribution dist = Nd4j . getDistributions ( ) . createNormal ( <float> , <float> ) ; for ( int i = <int> ; i < W . rows ( ) ; i + + ) W . putRow ( i , Nd4j . create ( dist . sample ( W . columns ( ) ) ) ) ; for ( int i = <int> ; i < <int> ; i + + ) { String learningRates = String . valueOf ( <str> + grad . getGradient ( W , i ) ) . replaceAll ( <str> , <str> ) ; System . out . println ( learningRates ) ; W . addi ( Nd4j . randn ( rows , cols ) ) ; } } @Test public void testAdaDeltaCombining ( ) { int n = <int> ; Nd4j . getRandom ( ) . setSeed ( <int> ) ; Random r = new Random ( <int> ) ; double [ ] rhos = new double [ n ] ; AdaDelta [ ] adaDeltas = new AdaDelta [ n ] ; INDArray [ ] msgs = new INDArray [ n ] ; INDArray [ ] msdxs = new INDArray [ n ] ; double avgRho = <float> ; INDArray avgStateMsg = Nd4j . zeros ( <int> , <int> ) ; INDArray avgStateMsdxs = Nd4j . zeros ( <int> , <int> ) ; for ( int i = <int> ; i < msgs . length ; i + + ) { rhos [ i ] = r . nextDouble ( ) ; avgRho + = rhos [ i ] ; adaDeltas [ i ] = new AdaDelta ( rhos [ i ] ) ; msgs [ i ] = Nd4j . rand ( <int> , <int> ) ; msdxs [ i ] = Nd4j . rand ( <int> , <int> ) ; avgStateMsg . addi ( msgs [ i ] ) ; avgStateMsdxs . addi ( msdxs [ i ] ) ; adaDeltas [ i ] . setMsg ( msgs [ i ] . dup ( ) ) ; adaDeltas [ i ] . setMsdx ( msdxs [ i ] . dup ( ) ) ; } avgRho / = n ; avgStateMsg . divi ( n ) ; avgStateMsdxs . divi ( n ) ; GradientUpdaterAggregator ag = adaDeltas [ <int> ] . getAggregator ( true ) ; for ( int i = <int> ; i < n ; i + + ) ag . aggregate ( adaDeltas [ i ] ) ; AdaDelta combined = ( AdaDelta ) ag . getUpdater ( ) ; assertEquals ( avgRho , combined . getRho ( ) , <float> ) ; assertEquals ( avgStateMsg , combined . getMsg ( ) ) ; assertEquals ( avgStateMsdxs , combined . getMsdx ( ) ) ; GradientUpdaterAggregator first = adaDeltas [ <int> ] . getAggregator ( false ) ; GradientUpdaterAggregator second = adaDeltas [ <int> ] . getAggregator ( false ) ; for ( int i = <int> ; i < n ; i + + ) { if ( i < <int> ) { first . aggregate ( adaDeltas [ i ] ) ; } else { second . aggregate ( adaDeltas [ i ] ) ; } } GradientUpdaterAggregator agMerged = first . combine ( second ) ; AdaDelta combined2 = ( AdaDelta ) agMerged . getUpdater ( ) ; assertEquals ( avgRho , combined2 . getRho ( ) , <float> ) ; assertEquals ( avgStateMsg , combined2 . getMsg ( ) ) ; assertEquals ( avgStateMsdxs , combined2 . getMsdx ( ) ) ; } @Test public void testAdam ( ) { int rows = <int> ; int cols = <int> ; Adam grad = new Adam ( ) ; INDArray W = Nd4j . zeros ( rows , cols ) ; Distribution dist = Nd4j . getDistributions ( ) . createNormal ( <float> , <float> ) ; for ( int i = <int> ; i < W . rows ( ) ; i + + ) W . putRow ( i , Nd4j . create ( dist . sample ( W . columns ( ) ) ) ) ; for ( int i = <int> ; i < <int> ; i + + ) { String learningRates = String . valueOf ( <str> + grad . getGradient ( W , i ) ) . replaceAll ( <str> , <str> ) ; System . out . println ( learningRates ) ; W . addi ( Nd4j . randn ( rows , cols ) ) ; } } @Test public void testAdamCombining ( ) { int n = <int> ; Nd4j . getRandom ( ) . setSeed ( <int> ) ; Random r = new Random ( <int> ) ; double [ ] lrs = new double [ n ] ; double [ ] beta1s = new double [ n ] ; double [ ] beta2s = new double [ n ] ; double [ ] eps = new double [ n ] ; Adam [ ] adams = new Adam [ n ] ; INDArray [ ] ms = new INDArray [ n ] ; INDArray [ ] vs = new INDArray [ n ] ; double avgLr = <float> ; double avgBeta1 = <float> ; double avgBeta2 = <float> ; double avgEps = <float> ; INDArray avgStateM = Nd4j . zeros ( <int> , <int> ) ; INDArray avgStateV = Nd4j . zeros ( <int> , <int> ) ; for ( int i = <int> ; i < n ; i + + ) { lrs [ i ] = r . nextDouble ( ) ; beta1s [ i ] = r . nextDouble ( ) ; beta2s [ i ] = r . nextDouble ( ) ; eps [ i ] = r . nextDouble ( ) ; avgLr + = lrs [ i ] ; avgBeta1 + = beta1s [ i ] ; avgBeta2 + = beta2s [ i ] ; avgEps + = eps [ i ] ; adams [ i ] = new Adam ( lrs [ i ] ) ; adams [ i ] . setBeta1 ( beta1s [ i ] ) ; adams [ i ] . setBeta2 ( beta2s [ i ] ) ; adams [ i ] . setEpsilon ( eps [ i ] ) ; ms [ i ] = Nd4j . rand ( <int> , <int> ) ; vs [ i ] = Nd4j . rand ( <int> , <int> ) ; avgStateM . addi ( ms [ i ] ) ; avgStateV . addi ( vs [ i ] ) ; adams [ i ] . setM ( ms [ i ] . dup ( ) ) ; adams [ i ] . setV ( vs [ i ] . dup ( ) ) ; } avgLr / = n ; avgBeta1 / = n ; avgBeta2 / = n ; avgEps / = n ; avgStateM . divi ( n ) ; avgStateV . divi ( n ) ; GradientUpdaterAggregator ag = adams [ <int> ] . getAggregator ( true ) ; for ( int i = <int> ; i < n ; i + + ) ag . aggregate ( adams [ i ] ) ; Adam combined = ( Adam ) ag . getUpdater ( ) ; assertEquals ( avgLr , combined . getLearningRate ( ) , <float> ) ; assertEquals ( avgBeta1 , combined . getBeta1 ( ) , <float> ) ; assertEquals ( avgBeta2 , combined . getBeta2 ( ) , <float> ) ; assertEquals ( avgEps , combined . getEpsilon ( ) , <float> ) ; assertEquals ( avgStateM , combined . getM ( ) ) ; assertEquals ( avgStateV , combined . getV ( ) ) ; GradientUpdaterAggregator first = adams [ <int> ] . getAggregator ( false ) ; GradientUpdaterAggregator second = adams [ <int> ] . getAggregator ( false ) ; for ( int i = <int> ; i < n ; i + + ) { if ( i < <int> ) { first . aggregate ( adams [ i ] ) ; } else { second . aggregate ( adams [ i ] ) ; } } GradientUpdaterAggregator agMerged = first . combine ( second ) ; Adam combined2 = ( Adam ) agMerged . getUpdater ( ) ; assertEquals ( avgLr , combined2 . getLearningRate ( ) , <float> ) ; assertEquals ( avgBeta1 , combined2 . getBeta1 ( ) , <float> ) ; assertEquals ( avgBeta2 , combined2 . getBeta2 ( ) , <float> ) ; assertEquals ( avgEps , combined2 . getEpsilon ( ) , <float> ) ; assertEquals ( avgStateM , combined2 . getM ( ) ) ; assertEquals ( avgStateV , combined2 . getV ( ) ) ; } @Test public void testRmsPropCombining ( ) { int n = <int> ; Nd4j . getRandom ( ) . setSeed ( <int> ) ; Random r = new Random ( <int> ) ; double [ ] lrs = new double [ n ] ; double [ ] rmsDecays = new double [ n ] ; RmsProp [ ] rmsProps = new RmsProp [ n ] ; INDArray [ ] lastGradients = new INDArray [ n ] ; double avgLr = <float> ; double avgRmsDecay = <float> ; INDArray avgLastGradient = Nd4j . zeros ( <int> , <int> ) ; for ( int i = <int> ; i < lastGradients . length ; i + + ) { lrs [ i ] = r . nextDouble ( ) ; rmsDecays [ i ] = r . nextDouble ( ) ; avgLr + = lrs [ i ] ; avgRmsDecay + = rmsDecays [ i ] ; rmsProps [ i ] = new RmsProp ( lrs [ i ] , rmsDecays [ i ] ) ; lastGradients [ i ] = Nd4j . rand ( <int> , <int> ) ; avgLastGradient . addi ( lastGradients [ i ] ) ; rmsProps [ i ] . setLastGradient ( lastGradients [ i ] . dup ( ) ) ; } avgLr / = n ; avgRmsDecay / = n ; avgLastGradient . divi ( n ) ; GradientUpdaterAggregator ag = rmsProps [ <int> ] . getAggregator ( true ) ; for ( int i = <int> ; i < n ; i + + ) ag . aggregate ( rmsProps [ i ] ) ; RmsProp combined = ( RmsProp ) ag . getUpdater ( ) ; assertEquals ( avgLr , combined . getLearningRate ( ) , <float> ) ; assertEquals ( avgRmsDecay , combined . getRmsDecay ( ) , <float> ) ; assertEquals ( avgLastGradient , combined . getLastGradient ( ) ) ; GradientUpdaterAggregator first = rmsProps [ <int> ] . getAggregator ( false ) ; GradientUpdaterAggregator second = rmsProps [ <int> ] . getAggregator ( false ) ; for ( int i = <int> ; i < n ; i + + ) { if ( i < <int> ) { first . aggregate ( rmsProps [ i ] ) ; } else { second . aggregate ( rmsProps [ i ] ) ; } } GradientUpdaterAggregator agMerged = first . combine ( second ) ; RmsProp combined2 = ( RmsProp ) agMerged . getUpdater ( ) ; assertEquals ( avgLr , combined2 . getLearningRate ( ) , <float> ) ; assertEquals ( avgRmsDecay , combined2 . getRmsDecay ( ) , <float> ) ; assertEquals ( avgLastGradient , combined2 . getLastGradient ( ) ) ; } @Test public void testSgdCombining ( ) { int n = <int> ; Nd4j . getRandom ( ) . setSeed ( <int> ) ; Random r = new Random ( <int> ) ; double [ ] lrs = new double [ n ] ; Sgd [ ] sgds = new Sgd [ n ] ; double avgLr = <float> ; for ( int i = <int> ; i < n ; i + + ) { lrs [ i ] = r . nextDouble ( ) ; avgLr + = lrs [ i ] ; sgds [ i ] = new Sgd ( lrs [ i ] ) ; } avgLr / = n ; GradientUpdaterAggregator ag = sgds [ <int> ] . getAggregator ( true ) ; for ( int i = <int> ; i < n ; i + + ) ag . aggregate ( sgds [ i ] ) ; Sgd combined = ( Sgd ) ag . getUpdater ( ) ; assertEquals ( avgLr , combined . getLearningRate ( ) , <float> ) ; GradientUpdaterAggregator first = sgds [ <int> ] . getAggregator ( false ) ; GradientUpdaterAggregator second = sgds [ <int> ] . getAggregator ( false ) ; for ( int i = <int> ; i < n ; i + + ) { if ( i < <int> ) { first . aggregate ( sgds [ i ] ) ; } else { second . aggregate ( sgds [ i ] ) ; } } GradientUpdaterAggregator agMerged = first . combine ( second ) ; Sgd combined2 = ( Sgd ) agMerged . getUpdater ( ) ; assertEquals ( avgLr , combined2 . getLearningRate ( ) , <float> ) ; } @Override public char ordering ( ) { return <str> ; } } 
