package org . elasticsearch . index . store ; import org . apache . lucene . codecs . CodecUtil ; import org . apache . lucene . index . * ; import org . apache . lucene . store . * ; import org . apache . lucene . util . * ; import org . elasticsearch . ElasticsearchException ; import org . elasticsearch . ExceptionsHelper ; import org . elasticsearch . common . Strings ; import org . elasticsearch . common . bytes . BytesReference ; import org . elasticsearch . common . collect . Tuple ; import org . elasticsearch . common . inject . Inject ; import org . elasticsearch . common . io . Streams ; import org . elasticsearch . common . io . stream . BytesStreamOutput ; import org . elasticsearch . common . io . stream . StreamInput ; import org . elasticsearch . common . io . stream . StreamOutput ; import org . elasticsearch . common . io . stream . Writeable ; import org . elasticsearch . common . logging . ESLogger ; import org . elasticsearch . common . logging . Loggers ; import org . elasticsearch . common . lucene . Lucene ; import org . elasticsearch . common . lucene . store . ByteArrayIndexInput ; import org . elasticsearch . common . lucene . store . InputStreamIndexInput ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . common . unit . TimeValue ; import org . elasticsearch . common . util . Callback ; import org . elasticsearch . common . util . SingleObjectCache ; import org . elasticsearch . common . util . concurrent . AbstractRefCounted ; import org . elasticsearch . common . util . concurrent . RefCounted ; import org . elasticsearch . common . util . iterable . Iterables ; import org . elasticsearch . env . ShardLock ; import org . elasticsearch . index . IndexSettings ; import org . elasticsearch . index . engine . Engine ; import org . elasticsearch . index . shard . AbstractIndexShardComponent ; import org . elasticsearch . index . shard . ShardId ; import java . io . * ; import java . nio . file . NoSuchFileException ; import java . nio . file . Path ; import java . util . * ; import java . util . concurrent . atomic . AtomicBoolean ; import java . util . concurrent . locks . ReentrantReadWriteLock ; import java . util . zip . Adler32 ; import java . util . zip . CRC32 ; import java . util . zip . Checksum ; import static java . util . Collections . emptyMap ; import static java . util . Collections . unmodifiableMap ; public class Store extends AbstractIndexShardComponent implements Closeable , RefCounted { static final String CODEC = <str> ; static final int VERSION_WRITE_THROWABLE = <int> ; static final int VERSION_STACK_TRACE = <int> ; static final int VERSION_START = <int> ; static final int VERSION = VERSION_WRITE_THROWABLE ; static final String CORRUPTED = <str> ; public static final String INDEX_STORE_STATS_REFRESH_INTERVAL = <str> ; private final AtomicBoolean isClosed = new AtomicBoolean ( false ) ; private final StoreDirectory directory ; private final ReentrantReadWriteLock metadataLock = new ReentrantReadWriteLock ( ) ; private final ShardLock shardLock ; private final OnClose onClose ; private final SingleObjectCache < StoreStats > statsCache ; private final AbstractRefCounted refCounter = new AbstractRefCounted ( <str> ) { @Override protected void closeInternal ( ) { Store . this . closeInternal ( ) ; } } ; public Store ( ShardId shardId , IndexSettings indexSettings , DirectoryService directoryService , ShardLock shardLock ) throws IOException { this ( shardId , indexSettings , directoryService , shardLock , OnClose . EMPTY ) ; } @Inject public Store ( ShardId shardId , IndexSettings indexSettings , DirectoryService directoryService , ShardLock shardLock , OnClose onClose ) throws IOException { super ( shardId , indexSettings ) ; final Settings settings = indexSettings . getSettings ( ) ; this . directory = new StoreDirectory ( directoryService . newDirectory ( ) , Loggers . getLogger ( <str> , settings , shardId ) ) ; this . shardLock = shardLock ; this . onClose = onClose ; final TimeValue refreshInterval = settings . getAsTime ( INDEX_STORE_STATS_REFRESH_INTERVAL , TimeValue . timeValueSeconds ( <int> ) ) ; this . statsCache = new StoreStatsCache ( refreshInterval , directory , directoryService ) ; logger . debug ( <str> , refreshInterval ) ; assert onClose ! = null ; assert shardLock ! = null ; assert shardLock . getShardId ( ) . equals ( shardId ) ; } public Directory directory ( ) { ensureOpen ( ) ; return directory ; } public SegmentInfos readLastCommittedSegmentsInfo ( ) throws IOException { failIfCorrupted ( ) ; try { return readSegmentsInfo ( null , directory ( ) ) ; } catch ( CorruptIndexException ex ) { markStoreCorrupted ( ex ) ; throw ex ; } } private static SegmentInfos readSegmentsInfo ( IndexCommit commit , Directory directory ) throws IOException { assert commit = = null | | commit . getDirectory ( ) = = directory ; try { return commit = = null ? Lucene . readSegmentInfos ( directory ) : Lucene . readSegmentInfos ( commit ) ; } catch ( EOFException eof ) { throw new CorruptIndexException ( <str> , <str> + commit + <str> , eof ) ; } catch ( IOException exception ) { throw exception ; } catch ( Exception ex ) { throw new CorruptIndexException ( <str> , <str> + commit + <str> , ex ) ; } } final void ensureOpen ( ) { if ( this . refCounter . refCount ( ) < = <int> ) { throw new AlreadyClosedException ( <str> ) ; } } public MetadataSnapshot getMetadataOrEmpty ( ) throws IOException { try { return getMetadata ( null ) ; } catch ( IndexNotFoundException ex ) { } catch ( FileNotFoundException | NoSuchFileException ex ) { logger . info ( <str> ) ; } return MetadataSnapshot . EMPTY ; } public MetadataSnapshot getMetadata ( ) throws IOException { return getMetadata ( null ) ; } public MetadataSnapshot getMetadata ( IndexCommit commit ) throws IOException { ensureOpen ( ) ; failIfCorrupted ( ) ; metadataLock . readLock ( ) . lock ( ) ; try { return new MetadataSnapshot ( commit , directory , logger ) ; } catch ( CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex ) { markStoreCorrupted ( ex ) ; throw ex ; } finally { metadataLock . readLock ( ) . unlock ( ) ; } } public void renameTempFilesSafe ( Map < String , String > tempFileMap ) throws IOException { final Map . Entry < String , String > [ ] entries = tempFileMap . entrySet ( ) . toArray ( new Map . Entry [ tempFileMap . size ( ) ] ) ; ArrayUtil . timSort ( entries , new Comparator < Map . Entry < String , String > > ( ) { @Override public int compare ( Map . Entry < String , String > o1 , Map . Entry < String , String > o2 ) { String left = o1 . getValue ( ) ; String right = o2 . getValue ( ) ; if ( left . startsWith ( IndexFileNames . SEGMENTS ) | | right . startsWith ( IndexFileNames . SEGMENTS ) ) { if ( left . startsWith ( IndexFileNames . SEGMENTS ) = = false ) { return - <int> ; } else if ( right . startsWith ( IndexFileNames . SEGMENTS ) = = false ) { return <int> ; } } return left . compareTo ( right ) ; } } ) ; metadataLock . writeLock ( ) . lock ( ) ; try ( Lock writeLock = directory ( ) . obtainLock ( IndexWriter . WRITE_LOCK_NAME ) ) { for ( Map . Entry < String , String > entry : entries ) { String tempFile = entry . getKey ( ) ; String origFile = entry . getValue ( ) ; try { directory . deleteFile ( origFile ) ; } catch ( FileNotFoundException | NoSuchFileException e ) { } catch ( Throwable ex ) { logger . debug ( <str> , ex , origFile ) ; } this . renameFile ( tempFile , origFile ) ; final String remove = tempFileMap . remove ( tempFile ) ; assert remove ! = null ; } } finally { metadataLock . writeLock ( ) . unlock ( ) ; } } public StoreStats stats ( ) throws IOException { ensureOpen ( ) ; return statsCache . getOrRefresh ( ) ; } public void renameFile ( String from , String to ) throws IOException { ensureOpen ( ) ; directory . renameFile ( from , to ) ; } @Override public final void incRef ( ) { refCounter . incRef ( ) ; } @Override public final boolean tryIncRef ( ) { return refCounter . tryIncRef ( ) ; } @Override public final void decRef ( ) { refCounter . decRef ( ) ; } @Override public void close ( ) { if ( isClosed . compareAndSet ( false , true ) ) { decRef ( ) ; logger . debug ( <str> + refCounter . refCount ( ) ) ; } } private void closeInternal ( ) { try { try { directory . innerClose ( ) ; } finally { onClose . handle ( shardLock ) ; } } catch ( IOException e ) { logger . debug ( <str> , e ) ; } finally { IOUtils . closeWhileHandlingException ( shardLock ) ; } } public static MetadataSnapshot readMetadataSnapshot ( Path indexLocation , ESLogger logger ) throws IOException { try ( Directory dir = new SimpleFSDirectory ( indexLocation ) ) { failIfCorrupted ( dir , new ShardId ( <str> , <int> ) ) ; return new MetadataSnapshot ( null , dir , logger ) ; } catch ( IndexNotFoundException ex ) { } catch ( FileNotFoundException | NoSuchFileException ex ) { logger . info ( <str> ) ; } return MetadataSnapshot . EMPTY ; } public static boolean canOpenIndex ( ESLogger logger , Path indexLocation ) throws IOException { try { tryOpenIndex ( indexLocation ) ; } catch ( Exception ex ) { logger . trace ( <str> , ex , indexLocation ) ; return false ; } return true ; } public static void tryOpenIndex ( Path indexLocation ) throws IOException { try ( Directory dir = new SimpleFSDirectory ( indexLocation ) ) { failIfCorrupted ( dir , new ShardId ( <str> , <int> ) ) ; Lucene . readSegmentInfos ( dir ) ; } } public IndexOutput createVerifyingOutput ( String fileName , final StoreFileMetaData metadata , final IOContext context ) throws IOException { IndexOutput output = directory ( ) . createOutput ( fileName , context ) ; boolean success = false ; try { if ( metadata . hasLegacyChecksum ( ) ) { logger . debug ( <str> , fileName ) ; output = new LegacyVerification . Adler32VerifyingIndexOutput ( output , metadata . checksum ( ) , metadata . length ( ) ) ; } else if ( metadata . checksum ( ) = = null ) { logger . debug ( <str> , fileName ) ; output = new LegacyVerification . LengthVerifyingIndexOutput ( output , metadata . length ( ) ) ; } else { assert metadata . writtenBy ( ) ! = null ; assert metadata . writtenBy ( ) . onOrAfter ( Version . LUCENE_4_8 ) ; output = new LuceneVerifyingIndexOutput ( metadata , output ) ; } success = true ; } finally { if ( success = = false ) { IOUtils . closeWhileHandlingException ( output ) ; } } return output ; } public static void verify ( IndexOutput output ) throws IOException { if ( output instanceof VerifyingIndexOutput ) { ( ( VerifyingIndexOutput ) output ) . verify ( ) ; } } public IndexInput openVerifyingInput ( String filename , IOContext context , StoreFileMetaData metadata ) throws IOException { if ( metadata . hasLegacyChecksum ( ) | | metadata . checksum ( ) = = null ) { logger . debug ( <str> , filename ) ; return directory ( ) . openInput ( filename , context ) ; } assert metadata . writtenBy ( ) ! = null ; assert metadata . writtenBy ( ) . onOrAfter ( Version . LUCENE_4_8_0 ) ; return new VerifyingIndexInput ( directory ( ) . openInput ( filename , context ) ) ; } public static void verify ( IndexInput input ) throws IOException { if ( input instanceof VerifyingIndexInput ) { ( ( VerifyingIndexInput ) input ) . verify ( ) ; } } public boolean checkIntegrityNoException ( StoreFileMetaData md ) { return checkIntegrityNoException ( md , directory ( ) ) ; } public static boolean checkIntegrityNoException ( StoreFileMetaData md , Directory directory ) { try { checkIntegrity ( md , directory ) ; return true ; } catch ( IOException e ) { return false ; } } public static void checkIntegrity ( final StoreFileMetaData md , final Directory directory ) throws IOException { try ( IndexInput input = directory . openInput ( md . name ( ) , IOContext . READONCE ) ) { if ( input . length ( ) ! = md . length ( ) ) { throw new CorruptIndexException ( <str> + md . length ( ) + <str> + input . length ( ) + <str> , input ) ; } if ( md . writtenBy ( ) ! = null & & md . writtenBy ( ) . onOrAfter ( Version . LUCENE_4_8_0 ) ) { String checksum = Store . digestToString ( CodecUtil . checksumEntireFile ( input ) ) ; if ( ! checksum . equals ( md . checksum ( ) ) ) { throw new CorruptIndexException ( <str> + checksum + <str> + md . checksum ( ) , input ) ; } } else if ( md . hasLegacyChecksum ( ) ) { final Checksum checksum = new Adler32 ( ) ; final byte [ ] buffer = new byte [ md . length ( ) > <int> ? <int> : ( int ) md . length ( ) ] ; final long len = input . length ( ) ; long read = <int> ; while ( len > read ) { final long bytesLeft = len - read ; final int bytesToRead = bytesLeft < buffer . length ? ( int ) bytesLeft : buffer . length ; input . readBytes ( buffer , <int> , bytesToRead , false ) ; checksum . update ( buffer , <int> , bytesToRead ) ; read + = bytesToRead ; } String adler32 = Store . digestToString ( checksum . getValue ( ) ) ; if ( ! adler32 . equals ( md . checksum ( ) ) ) { throw new CorruptIndexException ( <str> + md . checksum ( ) + <str> + adler32 , input ) ; } } } } public boolean isMarkedCorrupted ( ) throws IOException { ensureOpen ( ) ; final String [ ] files = directory ( ) . listAll ( ) ; for ( String file : files ) { if ( file . startsWith ( CORRUPTED ) ) { return true ; } } return false ; } public void removeCorruptionMarker ( ) throws IOException { ensureOpen ( ) ; final Directory directory = directory ( ) ; IOException firstException = null ; final String [ ] files = directory . listAll ( ) ; for ( String file : files ) { if ( file . startsWith ( CORRUPTED ) ) { try { directory . deleteFile ( file ) ; } catch ( IOException ex ) { if ( firstException = = null ) { firstException = ex ; } else { firstException . addSuppressed ( ex ) ; } } } } if ( firstException ! = null ) { throw firstException ; } } public void failIfCorrupted ( ) throws IOException { ensureOpen ( ) ; failIfCorrupted ( directory , shardId ) ; } private static final void failIfCorrupted ( Directory directory , ShardId shardId ) throws IOException { final String [ ] files = directory . listAll ( ) ; List < CorruptIndexException > ex = new ArrayList < > ( ) ; for ( String file : files ) { if ( file . startsWith ( CORRUPTED ) ) { try ( ChecksumIndexInput input = directory . openChecksumInput ( file , IOContext . READONCE ) ) { int version = CodecUtil . checkHeader ( input , CODEC , VERSION_START , VERSION ) ; if ( version = = VERSION_WRITE_THROWABLE ) { final int size = input . readVInt ( ) ; final byte [ ] buffer = new byte [ size ] ; input . readBytes ( buffer , <int> , buffer . length ) ; StreamInput in = StreamInput . wrap ( buffer ) ; Throwable t = in . readThrowable ( ) ; if ( t instanceof CorruptIndexException ) { ex . add ( ( CorruptIndexException ) t ) ; } else { ex . add ( new CorruptIndexException ( t . getMessage ( ) , <str> , t ) ) ; } } else { assert version = = VERSION_START | | version = = VERSION_STACK_TRACE ; String msg = input . readString ( ) ; StringBuilder builder = new StringBuilder ( shardId . toString ( ) ) ; builder . append ( <str> ) ; builder . append ( file ) . append ( <str> ) ; builder . append ( msg ) ; if ( version = = VERSION_STACK_TRACE ) { builder . append ( System . lineSeparator ( ) ) ; builder . append ( input . readString ( ) ) ; } ex . add ( new CorruptIndexException ( builder . toString ( ) , <str> ) ) ; } CodecUtil . checkFooter ( input ) ; } } } if ( ex . isEmpty ( ) = = false ) { ExceptionsHelper . rethrowAndSuppress ( ex ) ; } } public void cleanupAndVerify ( String reason , MetadataSnapshot sourceMetaData ) throws IOException { metadataLock . writeLock ( ) . lock ( ) ; try ( Lock writeLock = directory . obtainLock ( IndexWriter . WRITE_LOCK_NAME ) ) { final StoreDirectory dir = directory ; for ( String existingFile : dir . listAll ( ) ) { if ( Store . isAutogenerated ( existingFile ) | | sourceMetaData . contains ( existingFile ) ) { continue ; } try { dir . deleteFile ( reason , existingFile ) ; } catch ( IOException ex ) { if ( existingFile . startsWith ( IndexFileNames . SEGMENTS ) | | existingFile . equals ( IndexFileNames . OLD_SEGMENTS_GEN ) ) { throw new IllegalStateException ( <str> + existingFile + <str> , ex ) ; } logger . debug ( <str> , ex , existingFile ) ; } } final Store . MetadataSnapshot metadataOrEmpty = getMetadata ( ) ; verifyAfterCleanup ( sourceMetaData , metadataOrEmpty ) ; } finally { metadataLock . writeLock ( ) . unlock ( ) ; } } final void verifyAfterCleanup ( MetadataSnapshot sourceMetaData , MetadataSnapshot targetMetaData ) { final RecoveryDiff recoveryDiff = targetMetaData . recoveryDiff ( sourceMetaData ) ; if ( recoveryDiff . identical . size ( ) ! = recoveryDiff . size ( ) ) { if ( recoveryDiff . missing . isEmpty ( ) ) { for ( StoreFileMetaData meta : recoveryDiff . different ) { StoreFileMetaData local = targetMetaData . get ( meta . name ( ) ) ; StoreFileMetaData remote = sourceMetaData . get ( meta . name ( ) ) ; final boolean same = local . isSame ( remote ) ; final boolean hashAndLengthEqual = ( local . checksum ( ) = = null & & remote . checksum ( ) = = null & & local . hash ( ) . equals ( remote . hash ( ) ) & & local . length ( ) = = remote . length ( ) ) ; final boolean consistent = hashAndLengthEqual | | same ; if ( consistent = = false ) { logger . debug ( <str> , recoveryDiff ) ; throw new IllegalStateException ( <str> + local + <str> + remote , null ) ; } } } else { logger . debug ( <str> , recoveryDiff ) ; throw new IllegalStateException ( <str> + recoveryDiff . different + <str> + recoveryDiff . missing + <str> , null ) ; } } } public int refCount ( ) { return refCounter . refCount ( ) ; } private static final class StoreDirectory extends FilterDirectory { private final ESLogger deletesLogger ; StoreDirectory ( Directory delegateDirectory , ESLogger deletesLogger ) throws IOException { super ( delegateDirectory ) ; this . deletesLogger = deletesLogger ; } @Override public void close ( ) throws IOException { assert false : <str> ; } public void deleteFile ( String msg , String name ) throws IOException { deletesLogger . trace ( <str> , msg , name ) ; super . deleteFile ( name ) ; } @Override public void deleteFile ( String name ) throws IOException { deleteFile ( <str> , name ) ; } private void innerClose ( ) throws IOException { super . close ( ) ; } @Override public String toString ( ) { return <str> + in . toString ( ) + <str> ; } } public final static class MetadataSnapshot implements Iterable < StoreFileMetaData > , Writeable < MetadataSnapshot > { private static final ESLogger logger = Loggers . getLogger ( MetadataSnapshot . class ) ; private static final Version FIRST_LUCENE_CHECKSUM_VERSION = Version . LUCENE_4_8 ; private final Map < String , StoreFileMetaData > metadata ; public static final MetadataSnapshot EMPTY = new MetadataSnapshot ( ) ; private final Map < String , String > commitUserData ; private final long numDocs ; public MetadataSnapshot ( Map < String , StoreFileMetaData > metadata , Map < String , String > commitUserData , long numDocs ) { this . metadata = metadata ; this . commitUserData = commitUserData ; this . numDocs = numDocs ; } MetadataSnapshot ( ) { metadata = emptyMap ( ) ; commitUserData = emptyMap ( ) ; numDocs = <int> ; } MetadataSnapshot ( IndexCommit commit , Directory directory , ESLogger logger ) throws IOException { LoadedMetadata loadedMetadata = loadMetadata ( commit , directory , logger ) ; metadata = loadedMetadata . fileMetadata ; commitUserData = loadedMetadata . userData ; numDocs = loadedMetadata . numDocs ; assert metadata . isEmpty ( ) | | numSegmentFiles ( ) = = <int> : <str> + numSegmentFiles ( ) ; } public MetadataSnapshot ( StreamInput in ) throws IOException { final int size = in . readVInt ( ) ; Map < String , StoreFileMetaData > metadata = new HashMap < > ( ) ; for ( int i = <int> ; i < size ; i + + ) { StoreFileMetaData meta = StoreFileMetaData . readStoreFileMetaData ( in ) ; metadata . put ( meta . name ( ) , meta ) ; } Map < String , String > commitUserData = new HashMap < > ( ) ; int num = in . readVInt ( ) ; for ( int i = num ; i > <int> ; i - - ) { commitUserData . put ( in . readString ( ) , in . readString ( ) ) ; } this . metadata = unmodifiableMap ( metadata ) ; this . commitUserData = unmodifiableMap ( commitUserData ) ; this . numDocs = in . readLong ( ) ; assert metadata . isEmpty ( ) | | numSegmentFiles ( ) = = <int> : <str> + numSegmentFiles ( ) ; } public long getNumDocs ( ) { return numDocs ; } static class LoadedMetadata { final Map < String , StoreFileMetaData > fileMetadata ; final Map < String , String > userData ; final long numDocs ; LoadedMetadata ( Map < String , StoreFileMetaData > fileMetadata , Map < String , String > userData , long numDocs ) { this . fileMetadata = fileMetadata ; this . userData = userData ; this . numDocs = numDocs ; } } static LoadedMetadata loadMetadata ( IndexCommit commit , Directory directory , ESLogger logger ) throws IOException { long numDocs ; Map < String , StoreFileMetaData > builder = new HashMap < > ( ) ; Map < String , String > checksumMap = readLegacyChecksums ( directory ) . v1 ( ) ; Map < String , String > commitUserDataBuilder = new HashMap < > ( ) ; try { final SegmentInfos segmentCommitInfos = Store . readSegmentsInfo ( commit , directory ) ; numDocs = Lucene . getNumDocs ( segmentCommitInfos ) ; commitUserDataBuilder . putAll ( segmentCommitInfos . getUserData ( ) ) ; Version maxVersion = Version . LUCENE_4_0 ; for ( SegmentCommitInfo info : segmentCommitInfos ) { final Version version = info . info . getVersion ( ) ; if ( version = = null ) { throw new IllegalArgumentException ( <str> + info . info . toString ( ) ) ; } if ( version . onOrAfter ( maxVersion ) ) { maxVersion = version ; } for ( String file : info . files ( ) ) { String legacyChecksum = checksumMap . get ( file ) ; if ( version . onOrAfter ( FIRST_LUCENE_CHECKSUM_VERSION ) ) { checksumFromLuceneFile ( directory , file , builder , logger , version , SEGMENT_INFO_EXTENSION . equals ( IndexFileNames . getExtension ( file ) ) ) ; } else { builder . put ( file , new StoreFileMetaData ( file , directory . fileLength ( file ) , legacyChecksum , version ) ) ; } } } final String segmentsFile = segmentCommitInfos . getSegmentsFileName ( ) ; String legacyChecksum = checksumMap . get ( segmentsFile ) ; if ( maxVersion . onOrAfter ( FIRST_LUCENE_CHECKSUM_VERSION ) ) { checksumFromLuceneFile ( directory , segmentsFile , builder , logger , maxVersion , true ) ; } else { final BytesRefBuilder fileHash = new BytesRefBuilder ( ) ; final long length ; try ( final IndexInput in = directory . openInput ( segmentsFile , IOContext . READONCE ) ) { length = in . length ( ) ; hashFile ( fileHash , new InputStreamIndexInput ( in , length ) , length ) ; } builder . put ( segmentsFile , new StoreFileMetaData ( segmentsFile , length , legacyChecksum , maxVersion , fileHash . get ( ) ) ) ; } } catch ( CorruptIndexException | IndexNotFoundException | IndexFormatTooOldException | IndexFormatTooNewException ex ) { throw ex ; } catch ( Throwable ex ) { try { logger . warn ( <str> , ex , commit = = null ? <str> : <str> ) ; Lucene . checkSegmentInfoIntegrity ( directory ) ; } catch ( CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException cex ) { cex . addSuppressed ( ex ) ; throw cex ; } catch ( Throwable e ) { } throw ex ; } return new LoadedMetadata ( unmodifiableMap ( builder ) , unmodifiableMap ( commitUserDataBuilder ) , numDocs ) ; } static Tuple < Map < String , String > , Long > readLegacyChecksums ( Directory directory ) throws IOException { synchronized ( directory ) { long lastFound = - <int> ; for ( String name : directory . listAll ( ) ) { if ( ! isChecksum ( name ) ) { continue ; } long current = Long . parseLong ( name . substring ( CHECKSUMS_PREFIX . length ( ) ) ) ; if ( current > lastFound ) { lastFound = current ; } } if ( lastFound > - <int> ) { try ( IndexInput indexInput = directory . openInput ( CHECKSUMS_PREFIX + lastFound , IOContext . READONCE ) ) { indexInput . readInt ( ) ; return new Tuple ( indexInput . readStringStringMap ( ) , lastFound ) ; } } return new Tuple ( new HashMap < > ( ) , - <int> ) ; } } static void cleanLegacyChecksums ( Directory directory , long newVersion ) throws IOException { synchronized ( directory ) { for ( String name : directory . listAll ( ) ) { if ( isChecksum ( name ) ) { long current = Long . parseLong ( name . substring ( CHECKSUMS_PREFIX . length ( ) ) ) ; if ( current < newVersion ) { try { directory . deleteFile ( name ) ; } catch ( IOException ex ) { logger . debug ( <str> , ex , name ) ; } } } } } } private static void checksumFromLuceneFile ( Directory directory , String file , Map < String , StoreFileMetaData > builder , ESLogger logger , Version version , boolean readFileAsHash ) throws IOException { final String checksum ; final BytesRefBuilder fileHash = new BytesRefBuilder ( ) ; try ( final IndexInput in = directory . openInput ( file , IOContext . READONCE ) ) { final long length ; try { length = in . length ( ) ; if ( length < CodecUtil . footerLength ( ) ) { throw new CorruptIndexException ( <str> + file + <str> + CodecUtil . footerLength ( ) + <str> + in . length ( ) , in ) ; } if ( readFileAsHash ) { final VerifyingIndexInput verifyingIndexInput = new VerifyingIndexInput ( in ) ; hashFile ( fileHash , new InputStreamIndexInput ( verifyingIndexInput , length ) , length ) ; checksum = digestToString ( verifyingIndexInput . verify ( ) ) ; } else { checksum = digestToString ( CodecUtil . retrieveChecksum ( in ) ) ; } } catch ( Throwable ex ) { logger . debug ( <str> , ex , file ) ; throw ex ; } builder . put ( file , new StoreFileMetaData ( file , length , checksum , version , fileHash . get ( ) ) ) ; } } public static BytesRef hashFile ( Directory directory , String file ) throws IOException { final BytesRefBuilder fileHash = new BytesRefBuilder ( ) ; try ( final IndexInput in = directory . openInput ( file , IOContext . READONCE ) ) { hashFile ( fileHash , new InputStreamIndexInput ( in , in . length ( ) ) , in . length ( ) ) ; } return fileHash . get ( ) ; } public static void hashFile ( BytesRefBuilder fileHash , InputStream in , long size ) throws IOException { final int len = ( int ) Math . min ( <int> * <int> , size ) ; fileHash . grow ( len ) ; fileHash . setLength ( len ) ; final int readBytes = Streams . readFully ( in , fileHash . bytes ( ) , <int> , len ) ; assert readBytes = = len : Integer . toString ( readBytes ) + <str> + Integer . toString ( len ) ; assert fileHash . length ( ) = = len : Integer . toString ( fileHash . length ( ) ) + <str> + Integer . toString ( len ) ; } @Override public Iterator < StoreFileMetaData > iterator ( ) { return metadata . values ( ) . iterator ( ) ; } public StoreFileMetaData get ( String name ) { return metadata . get ( name ) ; } public Map < String , StoreFileMetaData > asMap ( ) { return metadata ; } private static final String DEL_FILE_EXTENSION = <str> ; private static final String LIV_FILE_EXTENSION = <str> ; private static final String FIELD_INFOS_FILE_EXTENSION = <str> ; private static final String SEGMENT_INFO_EXTENSION = <str> ; public RecoveryDiff recoveryDiff ( MetadataSnapshot recoveryTargetSnapshot ) { final List < StoreFileMetaData > identical = new ArrayList < > ( ) ; final List < StoreFileMetaData > different = new ArrayList < > ( ) ; final List < StoreFileMetaData > missing = new ArrayList < > ( ) ; final Map < String , List < StoreFileMetaData > > perSegment = new HashMap < > ( ) ; final List < StoreFileMetaData > perCommitStoreFiles = new ArrayList < > ( ) ; for ( StoreFileMetaData meta : this ) { if ( IndexFileNames . OLD_SEGMENTS_GEN . equals ( meta . name ( ) ) ) { continue ; } final String segmentId = IndexFileNames . parseSegmentName ( meta . name ( ) ) ; final String extension = IndexFileNames . getExtension ( meta . name ( ) ) ; assert FIELD_INFOS_FILE_EXTENSION . equals ( extension ) = = false | | IndexFileNames . stripExtension ( IndexFileNames . stripSegmentName ( meta . name ( ) ) ) . isEmpty ( ) : <str> ; if ( IndexFileNames . SEGMENTS . equals ( segmentId ) | | DEL_FILE_EXTENSION . equals ( extension ) | | LIV_FILE_EXTENSION . equals ( extension ) ) { perCommitStoreFiles . add ( meta ) ; } else { List < StoreFileMetaData > perSegStoreFiles = perSegment . get ( segmentId ) ; if ( perSegStoreFiles = = null ) { perSegStoreFiles = new ArrayList < > ( ) ; perSegment . put ( segmentId , perSegStoreFiles ) ; } perSegStoreFiles . add ( meta ) ; } } final ArrayList < StoreFileMetaData > identicalFiles = new ArrayList < > ( ) ; for ( List < StoreFileMetaData > segmentFiles : Iterables . concat ( perSegment . values ( ) , Collections . singleton ( perCommitStoreFiles ) ) ) { identicalFiles . clear ( ) ; boolean consistent = true ; for ( StoreFileMetaData meta : segmentFiles ) { StoreFileMetaData storeFileMetaData = recoveryTargetSnapshot . get ( meta . name ( ) ) ; if ( storeFileMetaData = = null ) { consistent = false ; missing . add ( meta ) ; } else if ( storeFileMetaData . isSame ( meta ) = = false ) { consistent = false ; different . add ( meta ) ; } else { identicalFiles . add ( meta ) ; } } if ( consistent ) { identical . addAll ( identicalFiles ) ; } else { different . addAll ( identicalFiles ) ; } } RecoveryDiff recoveryDiff = new RecoveryDiff ( Collections . unmodifiableList ( identical ) , Collections . unmodifiableList ( different ) , Collections . unmodifiableList ( missing ) ) ; assert recoveryDiff . size ( ) = = this . metadata . size ( ) - ( metadata . containsKey ( IndexFileNames . OLD_SEGMENTS_GEN ) ? <int> : <int> ) : <str> + recoveryDiff . size ( ) + <str> + this . metadata . size ( ) + <str> + metadata . containsKey ( IndexFileNames . OLD_SEGMENTS_GEN ) + <str> ; return recoveryDiff ; } public int size ( ) { return metadata . size ( ) ; } @Override public void writeTo ( StreamOutput out ) throws IOException { out . writeVInt ( this . metadata . size ( ) ) ; for ( StoreFileMetaData meta : this ) { meta . writeTo ( out ) ; } out . writeVInt ( commitUserData . size ( ) ) ; for ( Map . Entry < String , String > entry : commitUserData . entrySet ( ) ) { out . writeString ( entry . getKey ( ) ) ; out . writeString ( entry . getValue ( ) ) ; } out . writeLong ( numDocs ) ; } public Map < String , String > getCommitUserData ( ) { return commitUserData ; } public boolean contains ( String existingFile ) { return metadata . containsKey ( existingFile ) ; } public StoreFileMetaData getSegmentsFile ( ) { for ( StoreFileMetaData file : this ) { if ( file . name ( ) . startsWith ( IndexFileNames . SEGMENTS ) ) { return file ; } } assert metadata . isEmpty ( ) ; return null ; } private final int numSegmentFiles ( ) { int count = <int> ; for ( StoreFileMetaData file : this ) { if ( file . name ( ) . startsWith ( IndexFileNames . SEGMENTS ) ) { count + + ; } } return count ; } public String getSyncId ( ) { return commitUserData . get ( Engine . SYNC_COMMIT_ID ) ; } @Override public MetadataSnapshot readFrom ( StreamInput in ) throws IOException { return new MetadataSnapshot ( in ) ; } } public static final class RecoveryDiff { public final List < StoreFileMetaData > identical ; public final List < StoreFileMetaData > different ; public final List < StoreFileMetaData > missing ; RecoveryDiff ( List < StoreFileMetaData > identical , List < StoreFileMetaData > different , List < StoreFileMetaData > missing ) { this . identical = identical ; this . different = different ; this . missing = missing ; } public int size ( ) { return identical . size ( ) + different . size ( ) + missing . size ( ) ; } @Override public String toString ( ) { return <str> + <str> + identical + <str> + different + <str> + missing + <str> ; } } public final static class LegacyChecksums { private final Map < String , String > legacyChecksums = new HashMap < > ( ) ; public void add ( StoreFileMetaData metaData ) throws IOException { if ( metaData . hasLegacyChecksum ( ) ) { synchronized ( this ) { legacyChecksums . put ( metaData . name ( ) , metaData . checksum ( ) ) ; } } } public synchronized void write ( Store store ) throws IOException { synchronized ( store . directory ) { Tuple < Map < String , String > , Long > tuple = MetadataSnapshot . readLegacyChecksums ( store . directory ) ; tuple . v1 ( ) . putAll ( legacyChecksums ) ; if ( ! tuple . v1 ( ) . isEmpty ( ) ) { writeChecksums ( store . directory , tuple . v1 ( ) , tuple . v2 ( ) ) ; } } } synchronized void writeChecksums ( Directory directory , Map < String , String > checksums , long lastVersion ) throws IOException { long nextVersion = Math . max ( lastVersion + <int> , System . currentTimeMillis ( ) ) ; final String checksumName = CHECKSUMS_PREFIX + nextVersion ; try ( IndexOutput output = directory . createOutput ( checksumName , IOContext . DEFAULT ) ) { output . writeInt ( <int> ) ; output . writeStringStringMap ( checksums ) ; } directory . sync ( Collections . singleton ( checksumName ) ) ; MetadataSnapshot . cleanLegacyChecksums ( directory , nextVersion ) ; } public void clear ( ) { this . legacyChecksums . clear ( ) ; } public void remove ( String name ) { legacyChecksums . remove ( name ) ; } } public static final String CHECKSUMS_PREFIX = <str> ; public static boolean isChecksum ( String name ) { return name . startsWith ( CHECKSUMS_PREFIX ) | | name . endsWith ( <str> ) ; } public static boolean isAutogenerated ( String name ) { return IndexWriter . WRITE_LOCK_NAME . equals ( name ) | | isChecksum ( name ) ; } public static String digestToString ( long digest ) { return Long . toString ( digest , Character . MAX_RADIX ) ; } static class LuceneVerifyingIndexOutput extends VerifyingIndexOutput { private final StoreFileMetaData metadata ; private long writtenBytes ; private final long checksumPosition ; private String actualChecksum ; private final byte [ ] footerChecksum = new byte [ <int> ] ; LuceneVerifyingIndexOutput ( StoreFileMetaData metadata , IndexOutput out ) { super ( out ) ; this . metadata = metadata ; checksumPosition = metadata . length ( ) - <int> ; } @Override public void verify ( ) throws IOException { String footerDigest = null ; if ( metadata . checksum ( ) . equals ( actualChecksum ) & & writtenBytes = = metadata . length ( ) ) { ByteArrayIndexInput indexInput = new ByteArrayIndexInput ( <str> , this . footerChecksum ) ; footerDigest = digestToString ( indexInput . readLong ( ) ) ; if ( metadata . checksum ( ) . equals ( footerDigest ) ) { return ; } } throw new CorruptIndexException ( <str> + metadata . checksum ( ) + <str> + actualChecksum + <str> + footerDigest + <str> + writtenBytes + <str> + metadata . length ( ) + <str> + metadata . toString ( ) + <str> , <str> + metadata . name ( ) + <str> ) ; } @Override public void writeByte ( byte b ) throws IOException { final long writtenBytes = this . writtenBytes + + ; if ( writtenBytes > = checksumPosition ) { if ( writtenBytes = = checksumPosition ) { readAndCompareChecksum ( ) ; } final int index = Math . toIntExact ( writtenBytes - checksumPosition ) ; if ( index < footerChecksum . length ) { footerChecksum [ index ] = b ; if ( index = = footerChecksum . length - <int> ) { verify ( ) ; } } else { verify ( ) ; throw new AssertionError ( <str> + metadata . length ( ) + <str> + writtenBytes ) ; } } out . writeByte ( b ) ; } private void readAndCompareChecksum ( ) throws IOException { actualChecksum = digestToString ( getChecksum ( ) ) ; if ( ! metadata . checksum ( ) . equals ( actualChecksum ) ) { throw new CorruptIndexException ( <str> + metadata . checksum ( ) + <str> + actualChecksum + <str> + metadata . toString ( ) + <str> , <str> + metadata . name ( ) + <str> ) ; } } @Override public void writeBytes ( byte [ ] b , int offset , int length ) throws IOException { if ( writtenBytes + length > checksumPosition ) { for ( int i = <int> ; i < length ; i + + ) { writeByte ( b [ offset + i ] ) ; } } else { out . writeBytes ( b , offset , length ) ; writtenBytes + = length ; } } } static class VerifyingIndexInput extends ChecksumIndexInput { private final IndexInput input ; private final Checksum digest ; private final long checksumPosition ; private final byte [ ] checksum = new byte [ <int> ] ; private long verifiedPosition = <int> ; public VerifyingIndexInput ( IndexInput input ) { this ( input , new BufferedChecksum ( new CRC32 ( ) ) ) ; } public VerifyingIndexInput ( IndexInput input , Checksum digest ) { super ( <str> + input + <str> ) ; this . input = input ; this . digest = digest ; checksumPosition = input . length ( ) - <int> ; } @Override public byte readByte ( ) throws IOException { long pos = input . getFilePointer ( ) ; final byte b = input . readByte ( ) ; pos + + ; if ( pos > verifiedPosition ) { if ( pos < = checksumPosition ) { digest . update ( b ) ; } else { checksum [ ( int ) ( pos - checksumPosition - <int> ) ] = b ; } verifiedPosition = pos ; } return b ; } @Override public void readBytes ( byte [ ] b , int offset , int len ) throws IOException { long pos = input . getFilePointer ( ) ; input . readBytes ( b , offset , len ) ; if ( pos + len > verifiedPosition ) { int alreadyVerified = ( int ) Math . max ( <int> , verifiedPosition - pos ) ; if ( pos < checksumPosition ) { if ( pos + len < checksumPosition ) { digest . update ( b , offset + alreadyVerified , len - alreadyVerified ) ; } else { int checksumOffset = ( int ) ( checksumPosition - pos ) ; if ( checksumOffset - alreadyVerified > <int> ) { digest . update ( b , offset + alreadyVerified , checksumOffset - alreadyVerified ) ; } System . arraycopy ( b , offset + checksumOffset , checksum , <int> , len - checksumOffset ) ; } } else { assert pos - checksumPosition < <int> ; System . arraycopy ( b , offset , checksum , ( int ) ( pos - checksumPosition ) , len ) ; } verifiedPosition = pos + len ; } } @Override public long getChecksum ( ) { return digest . getValue ( ) ; } @Override public void seek ( long pos ) throws IOException { if ( pos < verifiedPosition ) { input . seek ( pos ) ; } else { if ( verifiedPosition > getFilePointer ( ) ) { input . seek ( verifiedPosition ) ; skipBytes ( pos - verifiedPosition ) ; } else { skipBytes ( pos - getFilePointer ( ) ) ; } } } @Override public void close ( ) throws IOException { input . close ( ) ; } @Override public long getFilePointer ( ) { return input . getFilePointer ( ) ; } @Override public long length ( ) { return input . length ( ) ; } @Override public IndexInput clone ( ) { throw new UnsupportedOperationException ( ) ; } @Override public IndexInput slice ( String sliceDescription , long offset , long length ) throws IOException { throw new UnsupportedOperationException ( ) ; } public long getStoredChecksum ( ) { return new ByteArrayDataInput ( checksum ) . readLong ( ) ; } public long verify ( ) throws CorruptIndexException { long storedChecksum = getStoredChecksum ( ) ; if ( getChecksum ( ) = = storedChecksum ) { return storedChecksum ; } throw new CorruptIndexException ( <str> + Store . digestToString ( getChecksum ( ) ) + <str> + Store . digestToString ( storedChecksum ) , this ) ; } } public void deleteQuiet ( String . . . files ) { for ( String file : files ) { try { directory ( ) . deleteFile ( file ) ; } catch ( Throwable ex ) { } } } public void markStoreCorrupted ( IOException exception ) throws IOException { ensureOpen ( ) ; if ( ! isMarkedCorrupted ( ) ) { String uuid = CORRUPTED + Strings . randomBase64UUID ( ) ; try ( IndexOutput output = this . directory ( ) . createOutput ( uuid , IOContext . DEFAULT ) ) { CodecUtil . writeHeader ( output , CODEC , VERSION ) ; BytesStreamOutput out = new BytesStreamOutput ( ) ; out . writeThrowable ( exception ) ; BytesReference bytes = out . bytes ( ) ; output . writeVInt ( bytes . length ( ) ) ; output . writeBytes ( bytes . array ( ) , bytes . arrayOffset ( ) , bytes . length ( ) ) ; CodecUtil . writeFooter ( output ) ; } catch ( IOException ex ) { logger . warn ( <str> , ex ) ; } directory ( ) . sync ( Collections . singleton ( uuid ) ) ; } } public static interface OnClose extends Callback < ShardLock > { static final OnClose EMPTY = new OnClose ( ) { @Override public void handle ( ShardLock Lock ) { } } ; } private static class StoreStatsCache extends SingleObjectCache < StoreStats > { private final Directory directory ; private final DirectoryService directoryService ; public StoreStatsCache ( TimeValue refreshInterval , Directory directory , DirectoryService directoryService ) throws IOException { super ( refreshInterval , new StoreStats ( estimateSize ( directory ) , directoryService . throttleTimeInNanos ( ) ) ) ; this . directory = directory ; this . directoryService = directoryService ; } @Override protected StoreStats refresh ( ) { try { return new StoreStats ( estimateSize ( directory ) , directoryService . throttleTimeInNanos ( ) ) ; } catch ( IOException ex ) { throw new ElasticsearchException ( <str> , ex ) ; } } private static long estimateSize ( Directory directory ) throws IOException { long estimatedSize = <int> ; String [ ] files = directory . listAll ( ) ; for ( String file : files ) { try { estimatedSize + = directory . fileLength ( file ) ; } catch ( NoSuchFileException | FileNotFoundException e ) { } } return estimatedSize ; } } } 
