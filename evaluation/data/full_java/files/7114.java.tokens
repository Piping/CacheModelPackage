package org . elasticsearch . search ; import com . carrotsearch . hppc . ObjectFloatHashMap ; import com . carrotsearch . hppc . ObjectHashSet ; import com . carrotsearch . hppc . ObjectSet ; import com . carrotsearch . hppc . cursors . ObjectCursor ; import org . apache . lucene . index . IndexOptions ; import org . apache . lucene . index . LeafReaderContext ; import org . apache . lucene . index . NumericDocValues ; import org . apache . lucene . search . TopDocs ; import org . elasticsearch . ExceptionsHelper ; import org . elasticsearch . action . search . SearchType ; import org . elasticsearch . cache . recycler . PageCacheRecycler ; import org . elasticsearch . cluster . ClusterService ; import org . elasticsearch . cluster . metadata . IndexMetaData ; import org . elasticsearch . common . Nullable ; import org . elasticsearch . common . ParseFieldMatcher ; import org . elasticsearch . common . bytes . BytesReference ; import org . elasticsearch . common . component . AbstractLifecycleComponent ; import org . elasticsearch . common . inject . Inject ; import org . elasticsearch . common . lucene . Lucene ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . common . unit . TimeValue ; import org . elasticsearch . common . util . BigArrays ; import org . elasticsearch . common . util . concurrent . ConcurrentCollections ; import org . elasticsearch . common . util . concurrent . ConcurrentMapLong ; import org . elasticsearch . common . util . concurrent . FutureUtils ; import org . elasticsearch . common . xcontent . XContentBuilder ; import org . elasticsearch . common . xcontent . XContentFactory ; import org . elasticsearch . common . xcontent . XContentLocation ; import org . elasticsearch . common . xcontent . XContentParser ; import org . elasticsearch . index . Index ; import org . elasticsearch . index . IndexService ; import org . elasticsearch . index . engine . Engine ; import org . elasticsearch . index . fielddata . FieldDataType ; import org . elasticsearch . index . fielddata . IndexFieldData ; import org . elasticsearch . index . fielddata . IndexFieldDataService ; import org . elasticsearch . index . mapper . DocumentMapper ; import org . elasticsearch . index . mapper . FieldMapper ; import org . elasticsearch . index . mapper . MappedFieldType ; import org . elasticsearch . index . mapper . MappedFieldType . Loading ; import org . elasticsearch . index . mapper . MapperService ; import org . elasticsearch . index . mapper . internal . ParentFieldMapper ; import org . elasticsearch . index . query . QueryParseContext ; import org . elasticsearch . index . query . QueryShardContext ; import org . elasticsearch . index . search . stats . ShardSearchStats ; import org . elasticsearch . index . search . stats . StatsGroupsParseElement ; import org . elasticsearch . index . shard . IndexEventListener ; import org . elasticsearch . index . shard . IndexShard ; import org . elasticsearch . indices . IndicesService ; import org . elasticsearch . indices . IndicesWarmer ; import org . elasticsearch . indices . IndicesWarmer . TerminationHandle ; import org . elasticsearch . indices . cache . request . IndicesRequestCache ; import org . elasticsearch . node . settings . NodeSettingsService ; import org . elasticsearch . script . ExecutableScript ; import org . elasticsearch . script . ScriptContext ; import org . elasticsearch . script . ScriptService ; import org . elasticsearch . script . SearchScript ; import org . elasticsearch . search . builder . SearchSourceBuilder ; import org . elasticsearch . search . dfs . DfsPhase ; import org . elasticsearch . search . dfs . DfsSearchResult ; import org . elasticsearch . search . fetch . * ; import org . elasticsearch . search . fetch . fielddata . FieldDataFieldsContext ; import org . elasticsearch . search . fetch . fielddata . FieldDataFieldsContext . FieldDataField ; import org . elasticsearch . search . fetch . fielddata . FieldDataFieldsFetchSubPhase ; import org . elasticsearch . search . fetch . script . ScriptFieldsContext . ScriptField ; import org . elasticsearch . search . internal . * ; import org . elasticsearch . search . internal . SearchContext . Lifetime ; import org . elasticsearch . search . query . * ; import org . elasticsearch . search . warmer . IndexWarmersMetaData ; import org . elasticsearch . threadpool . ThreadPool ; import java . util . HashMap ; import java . util . Map ; import java . util . concurrent . CountDownLatch ; import java . util . concurrent . ExecutionException ; import java . util . concurrent . Executor ; import java . util . concurrent . ScheduledFuture ; import java . util . concurrent . atomic . AtomicLong ; import static java . util . Collections . unmodifiableMap ; import static org . elasticsearch . common . unit . TimeValue . timeValueMillis ; import static org . elasticsearch . common . unit . TimeValue . timeValueMinutes ; public class SearchService extends AbstractLifecycleComponent < SearchService > implements IndexEventListener { public static final String NORMS_LOADING_KEY = <str> ; public static final String DEFAULT_KEEPALIVE_KEY = <str> ; public static final String KEEPALIVE_INTERVAL_KEY = <str> ; public static final String DEFAULT_SEARCH_TIMEOUT = <str> ; public static final TimeValue NO_TIMEOUT = timeValueMillis ( - <int> ) ; private final ThreadPool threadPool ; private final ClusterService clusterService ; private final IndicesService indicesService ; private final IndicesWarmer indicesWarmer ; private final ScriptService scriptService ; private final PageCacheRecycler pageCacheRecycler ; private final BigArrays bigArrays ; private final DfsPhase dfsPhase ; private final QueryPhase queryPhase ; private final FetchPhase fetchPhase ; private final IndicesRequestCache indicesQueryCache ; private final long defaultKeepAlive ; private volatile TimeValue defaultSearchTimeout ; private final ScheduledFuture < ? > keepAliveReaper ; private final AtomicLong idGenerator = new AtomicLong ( ) ; private final ConcurrentMapLong < SearchContext > activeContexts = ConcurrentCollections . newConcurrentMapLongWithAggressiveConcurrency ( ) ; private final Map < String , SearchParseElement > elementParsers ; private final ParseFieldMatcher parseFieldMatcher ; @Inject public SearchService ( Settings settings , NodeSettingsService nodeSettingsService , ClusterService clusterService , IndicesService indicesService , IndicesWarmer indicesWarmer , ThreadPool threadPool , ScriptService scriptService , PageCacheRecycler pageCacheRecycler , BigArrays bigArrays , DfsPhase dfsPhase , QueryPhase queryPhase , FetchPhase fetchPhase , IndicesRequestCache indicesQueryCache ) { super ( settings ) ; this . parseFieldMatcher = new ParseFieldMatcher ( settings ) ; this . threadPool = threadPool ; this . clusterService = clusterService ; this . indicesService = indicesService ; this . indicesWarmer = indicesWarmer ; this . scriptService = scriptService ; this . pageCacheRecycler = pageCacheRecycler ; this . bigArrays = bigArrays ; this . dfsPhase = dfsPhase ; this . queryPhase = queryPhase ; this . fetchPhase = fetchPhase ; this . indicesQueryCache = indicesQueryCache ; TimeValue keepAliveInterval = settings . getAsTime ( KEEPALIVE_INTERVAL_KEY , timeValueMinutes ( <int> ) ) ; this . defaultKeepAlive = settings . getAsTime ( DEFAULT_KEEPALIVE_KEY , timeValueMinutes ( <int> ) ) . millis ( ) ; Map < String , SearchParseElement > elementParsers = new HashMap < > ( ) ; elementParsers . putAll ( dfsPhase . parseElements ( ) ) ; elementParsers . putAll ( queryPhase . parseElements ( ) ) ; elementParsers . putAll ( fetchPhase . parseElements ( ) ) ; elementParsers . put ( <str> , new StatsGroupsParseElement ( ) ) ; this . elementParsers = unmodifiableMap ( elementParsers ) ; this . keepAliveReaper = threadPool . scheduleWithFixedDelay ( new Reaper ( ) , keepAliveInterval ) ; this . indicesWarmer . addListener ( new NormsWarmer ( indicesWarmer ) ) ; this . indicesWarmer . addListener ( new FieldDataWarmer ( indicesWarmer ) ) ; this . indicesWarmer . addListener ( new SearchWarmer ( ) ) ; defaultSearchTimeout = settings . getAsTime ( DEFAULT_SEARCH_TIMEOUT , NO_TIMEOUT ) ; nodeSettingsService . addListener ( new SearchSettingsListener ( ) ) ; } class SearchSettingsListener implements NodeSettingsService . Listener { @Override public void onRefreshSettings ( Settings settings ) { final TimeValue maybeNewDefaultSearchTimeout = settings . getAsTime ( SearchService . DEFAULT_SEARCH_TIMEOUT , SearchService . this . defaultSearchTimeout ) ; if ( ! maybeNewDefaultSearchTimeout . equals ( SearchService . this . defaultSearchTimeout ) ) { logger . info ( <str> , SearchService . DEFAULT_SEARCH_TIMEOUT , SearchService . this . defaultSearchTimeout , maybeNewDefaultSearchTimeout ) ; SearchService . this . defaultSearchTimeout = maybeNewDefaultSearchTimeout ; } } } @Override public void afterIndexClosed ( Index index , Settings indexSettings ) { IndexMetaData idxMeta = SearchService . this . clusterService . state ( ) . metaData ( ) . index ( index . getName ( ) ) ; if ( idxMeta ! = null & & idxMeta . getState ( ) = = IndexMetaData . State . CLOSE ) { afterIndexDeleted ( index , indexSettings ) ; } } @Override public void afterIndexDeleted ( Index index , Settings indexSettings ) { freeAllContextForIndex ( index ) ; } protected void putContext ( SearchContext context ) { final SearchContext previous = activeContexts . put ( context . id ( ) , context ) ; assert previous = = null ; } protected SearchContext removeContext ( long id ) { return activeContexts . remove ( id ) ; } @Override protected void doStart ( ) { } @Override protected void doStop ( ) { for ( final SearchContext context : activeContexts . values ( ) ) { freeContext ( context . id ( ) ) ; } } @Override protected void doClose ( ) { doStop ( ) ; FutureUtils . cancel ( keepAliveReaper ) ; } public DfsSearchResult executeDfsPhase ( ShardSearchRequest request ) { final SearchContext context = createAndPutContext ( request ) ; try { contextProcessing ( context ) ; dfsPhase . execute ( context ) ; contextProcessedSuccessfully ( context ) ; return context . dfsResult ( ) ; } catch ( Throwable e ) { logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } private void loadOrExecuteQueryPhase ( final ShardSearchRequest request , final SearchContext context , final QueryPhase queryPhase ) throws Exception { final boolean canCache = indicesQueryCache . canCache ( request , context ) ; if ( canCache ) { indicesQueryCache . loadIntoContext ( request , context , queryPhase ) ; } else { queryPhase . execute ( context ) ; } } public QuerySearchResultProvider executeQueryPhase ( ShardSearchRequest request ) { final SearchContext context = createAndPutContext ( request ) ; final ShardSearchStats shardSearchStats = context . indexShard ( ) . searchService ( ) ; try { shardSearchStats . onPreQueryPhase ( context ) ; long time = System . nanoTime ( ) ; contextProcessing ( context ) ; loadOrExecuteQueryPhase ( request , context , queryPhase ) ; if ( context . queryResult ( ) . topDocs ( ) . scoreDocs . length = = <int> & & context . scrollContext ( ) = = null ) { freeContext ( context . id ( ) ) ; } else { contextProcessedSuccessfully ( context ) ; } shardSearchStats . onQueryPhase ( context , System . nanoTime ( ) - time ) ; return context . queryResult ( ) ; } catch ( Throwable e ) { if ( e instanceof ExecutionException ) { e = e . getCause ( ) ; } shardSearchStats . onFailedQueryPhase ( context ) ; logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } public ScrollQuerySearchResult executeQueryPhase ( InternalScrollSearchRequest request ) { final SearchContext context = findContext ( request . id ( ) ) ; ShardSearchStats shardSearchStats = context . indexShard ( ) . searchService ( ) ; try { shardSearchStats . onPreQueryPhase ( context ) ; long time = System . nanoTime ( ) ; contextProcessing ( context ) ; processScroll ( request , context ) ; queryPhase . execute ( context ) ; contextProcessedSuccessfully ( context ) ; shardSearchStats . onQueryPhase ( context , System . nanoTime ( ) - time ) ; return new ScrollQuerySearchResult ( context . queryResult ( ) , context . shardTarget ( ) ) ; } catch ( Throwable e ) { shardSearchStats . onFailedQueryPhase ( context ) ; logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } public QuerySearchResult executeQueryPhase ( QuerySearchRequest request ) { final SearchContext context = findContext ( request . id ( ) ) ; contextProcessing ( context ) ; context . searcher ( ) . setAggregatedDfs ( request . dfs ( ) ) ; IndexShard indexShard = context . indexShard ( ) ; ShardSearchStats shardSearchStats = indexShard . searchService ( ) ; try { shardSearchStats . onPreQueryPhase ( context ) ; long time = System . nanoTime ( ) ; queryPhase . execute ( context ) ; if ( context . queryResult ( ) . topDocs ( ) . scoreDocs . length = = <int> & & context . scrollContext ( ) = = null ) { freeContext ( context . id ( ) ) ; } else { contextProcessedSuccessfully ( context ) ; } shardSearchStats . onQueryPhase ( context , System . nanoTime ( ) - time ) ; return context . queryResult ( ) ; } catch ( Throwable e ) { shardSearchStats . onFailedQueryPhase ( context ) ; logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } private boolean fetchPhaseShouldFreeContext ( SearchContext context ) { if ( context . scrollContext ( ) = = null ) { return true ; } else { return context . scrollContext ( ) . scroll = = null ; } } public QueryFetchSearchResult executeFetchPhase ( ShardSearchRequest request ) { final SearchContext context = createAndPutContext ( request ) ; contextProcessing ( context ) ; try { ShardSearchStats shardSearchStats = context . indexShard ( ) . searchService ( ) ; shardSearchStats . onPreQueryPhase ( context ) ; long time = System . nanoTime ( ) ; try { loadOrExecuteQueryPhase ( request , context , queryPhase ) ; } catch ( Throwable e ) { shardSearchStats . onFailedQueryPhase ( context ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } long time2 = System . nanoTime ( ) ; shardSearchStats . onQueryPhase ( context , time2 - time ) ; shardSearchStats . onPreFetchPhase ( context ) ; try { shortcutDocIdsToLoad ( context ) ; fetchPhase . execute ( context ) ; if ( fetchPhaseShouldFreeContext ( context ) ) { freeContext ( context . id ( ) ) ; } else { contextProcessedSuccessfully ( context ) ; } } catch ( Throwable e ) { shardSearchStats . onFailedFetchPhase ( context ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } shardSearchStats . onFetchPhase ( context , System . nanoTime ( ) - time2 ) ; return new QueryFetchSearchResult ( context . queryResult ( ) , context . fetchResult ( ) ) ; } catch ( Throwable e ) { logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } public QueryFetchSearchResult executeFetchPhase ( QuerySearchRequest request ) { final SearchContext context = findContext ( request . id ( ) ) ; contextProcessing ( context ) ; context . searcher ( ) . setAggregatedDfs ( request . dfs ( ) ) ; try { ShardSearchStats shardSearchStats = context . indexShard ( ) . searchService ( ) ; shardSearchStats . onPreQueryPhase ( context ) ; long time = System . nanoTime ( ) ; try { queryPhase . execute ( context ) ; } catch ( Throwable e ) { shardSearchStats . onFailedQueryPhase ( context ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } long time2 = System . nanoTime ( ) ; shardSearchStats . onQueryPhase ( context , time2 - time ) ; shardSearchStats . onPreFetchPhase ( context ) ; try { shortcutDocIdsToLoad ( context ) ; fetchPhase . execute ( context ) ; if ( fetchPhaseShouldFreeContext ( context ) ) { freeContext ( request . id ( ) ) ; } else { contextProcessedSuccessfully ( context ) ; } } catch ( Throwable e ) { shardSearchStats . onFailedFetchPhase ( context ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } shardSearchStats . onFetchPhase ( context , System . nanoTime ( ) - time2 ) ; return new QueryFetchSearchResult ( context . queryResult ( ) , context . fetchResult ( ) ) ; } catch ( Throwable e ) { logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } public ScrollQueryFetchSearchResult executeFetchPhase ( InternalScrollSearchRequest request ) { final SearchContext context = findContext ( request . id ( ) ) ; contextProcessing ( context ) ; try { ShardSearchStats shardSearchStats = context . indexShard ( ) . searchService ( ) ; processScroll ( request , context ) ; shardSearchStats . onPreQueryPhase ( context ) ; long time = System . nanoTime ( ) ; try { queryPhase . execute ( context ) ; } catch ( Throwable e ) { shardSearchStats . onFailedQueryPhase ( context ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } long time2 = System . nanoTime ( ) ; shardSearchStats . onQueryPhase ( context , time2 - time ) ; shardSearchStats . onPreFetchPhase ( context ) ; try { shortcutDocIdsToLoad ( context ) ; fetchPhase . execute ( context ) ; if ( fetchPhaseShouldFreeContext ( context ) ) { freeContext ( request . id ( ) ) ; } else { contextProcessedSuccessfully ( context ) ; } } catch ( Throwable e ) { shardSearchStats . onFailedFetchPhase ( context ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } shardSearchStats . onFetchPhase ( context , System . nanoTime ( ) - time2 ) ; return new ScrollQueryFetchSearchResult ( new QueryFetchSearchResult ( context . queryResult ( ) , context . fetchResult ( ) ) , context . shardTarget ( ) ) ; } catch ( Throwable e ) { logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } public FetchSearchResult executeFetchPhase ( ShardFetchRequest request ) { final SearchContext context = findContext ( request . id ( ) ) ; contextProcessing ( context ) ; final ShardSearchStats shardSearchStats = context . indexShard ( ) . searchService ( ) ; try { if ( request . lastEmittedDoc ( ) ! = null ) { context . scrollContext ( ) . lastEmittedDoc = request . lastEmittedDoc ( ) ; } context . docIdsToLoad ( request . docIds ( ) , <int> , request . docIdsSize ( ) ) ; shardSearchStats . onPreFetchPhase ( context ) ; long time = System . nanoTime ( ) ; fetchPhase . execute ( context ) ; if ( fetchPhaseShouldFreeContext ( context ) ) { freeContext ( request . id ( ) ) ; } else { contextProcessedSuccessfully ( context ) ; } shardSearchStats . onFetchPhase ( context , System . nanoTime ( ) - time ) ; return context . fetchResult ( ) ; } catch ( Throwable e ) { shardSearchStats . onFailedFetchPhase ( context ) ; logger . trace ( <str> , e ) ; processFailure ( context , e ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } finally { cleanContext ( context ) ; } } private SearchContext findContext ( long id ) throws SearchContextMissingException { SearchContext context = activeContexts . get ( id ) ; if ( context = = null ) { throw new SearchContextMissingException ( id ) ; } SearchContext . setCurrent ( context ) ; return context ; } final SearchContext createAndPutContext ( ShardSearchRequest request ) { SearchContext context = createContext ( request , null ) ; boolean success = false ; try { putContext ( context ) ; if ( request . scroll ( ) ! = null ) { context . indexShard ( ) . searchService ( ) . onNewScrollContext ( context ) ; } context . indexShard ( ) . searchService ( ) . onNewContext ( context ) ; success = true ; return context ; } finally { if ( ! success ) { freeContext ( context . id ( ) ) ; } } } final SearchContext createContext ( ShardSearchRequest request , @Nullable Engine . Searcher searcher ) { IndexService indexService = indicesService . indexServiceSafe ( request . index ( ) ) ; IndexShard indexShard = indexService . getShard ( request . shardId ( ) ) ; SearchShardTarget shardTarget = new SearchShardTarget ( clusterService . localNode ( ) . id ( ) , request . index ( ) , request . shardId ( ) ) ; Engine . Searcher engineSearcher = searcher = = null ? indexShard . acquireSearcher ( <str> ) : searcher ; SearchContext context = new DefaultSearchContext ( idGenerator . incrementAndGet ( ) , request , shardTarget , engineSearcher , indexService , indexShard , scriptService , pageCacheRecycler , bigArrays , threadPool . estimatedTimeInMillisCounter ( ) , parseFieldMatcher , defaultSearchTimeout ) ; SearchContext . setCurrent ( context ) ; try { if ( request . scroll ( ) ! = null ) { context . scrollContext ( new ScrollContext ( ) ) ; context . scrollContext ( ) . scroll = request . scroll ( ) ; } if ( request . template ( ) ! = null ) { ExecutableScript executable = this . scriptService . executable ( request . template ( ) , ScriptContext . Standard . SEARCH , context ) ; BytesReference run = ( BytesReference ) executable . run ( ) ; try ( XContentParser parser = XContentFactory . xContent ( run ) . createParser ( run ) ) { QueryParseContext queryParseContext = new QueryParseContext ( indicesService . getIndicesQueryRegistry ( ) ) ; queryParseContext . reset ( parser ) ; queryParseContext . parseFieldMatcher ( parseFieldMatcher ) ; parseSource ( context , SearchSourceBuilder . parseSearchSource ( parser , queryParseContext ) ) ; } } parseSource ( context , request . source ( ) ) ; if ( context . from ( ) = = - <int> ) { context . from ( <int> ) ; } if ( context . size ( ) = = - <int> ) { context . size ( <int> ) ; } dfsPhase . preProcess ( context ) ; queryPhase . preProcess ( context ) ; fetchPhase . preProcess ( context ) ; long keepAlive = defaultKeepAlive ; if ( request . scroll ( ) ! = null & & request . scroll ( ) . keepAlive ( ) ! = null ) { keepAlive = request . scroll ( ) . keepAlive ( ) . millis ( ) ; } context . keepAlive ( keepAlive ) ; } catch ( Throwable e ) { context . close ( ) ; throw ExceptionsHelper . convertToRuntime ( e ) ; } return context ; } private void freeAllContextForIndex ( Index index ) { assert index ! = null ; for ( SearchContext ctx : activeContexts . values ( ) ) { if ( index . equals ( ctx . indexShard ( ) . shardId ( ) . index ( ) ) ) { freeContext ( ctx . id ( ) ) ; } } } public boolean freeContext ( long id ) { final SearchContext context = removeContext ( id ) ; if ( context ! = null ) { try { context . indexShard ( ) . searchService ( ) . onFreeContext ( context ) ; if ( context . scrollContext ( ) ! = null ) { context . indexShard ( ) . searchService ( ) . onFreeScrollContext ( context ) ; } } finally { context . close ( ) ; } return true ; } return false ; } public void freeAllScrollContexts ( ) { for ( SearchContext searchContext : activeContexts . values ( ) ) { if ( searchContext . scrollContext ( ) ! = null ) { freeContext ( searchContext . id ( ) ) ; } } } private void contextProcessing ( SearchContext context ) { context . accessed ( - <int> ) ; } private void contextProcessedSuccessfully ( SearchContext context ) { context . accessed ( threadPool . estimatedTimeInMillis ( ) ) ; } private void cleanContext ( SearchContext context ) { assert context = = SearchContext . current ( ) ; context . clearReleasables ( Lifetime . PHASE ) ; SearchContext . removeCurrent ( ) ; } private void processFailure ( SearchContext context , Throwable t ) { freeContext ( context . id ( ) ) ; try { if ( Lucene . isCorruptionException ( t ) ) { context . indexShard ( ) . failShard ( <str> , t ) ; } } catch ( Throwable e ) { logger . warn ( <str> , e ) ; } } private void parseSource ( SearchContext context , SearchSourceBuilder source ) throws SearchParseException { if ( source = = null ) { return ; } final IndexShard indexShard = context . indexShard ( ) ; QueryShardContext queryShardContext = indexShard . getQueryShardContext ( ) ; context . from ( source . from ( ) ) ; context . size ( source . size ( ) ) ; ObjectFloatHashMap < String > indexBoostMap = source . indexBoost ( ) ; if ( indexBoostMap ! = null ) { Float indexBoost = indexBoostMap . get ( context . shardTarget ( ) . index ( ) ) ; if ( indexBoost ! = null ) { context . queryBoost ( indexBoost ) ; } } if ( source . query ( ) ! = null ) { context . parsedQuery ( queryShardContext . toQuery ( source . query ( ) ) ) ; } if ( source . postFilter ( ) ! = null ) { context . parsedPostFilter ( queryShardContext . toQuery ( source . postFilter ( ) ) ) ; } if ( source . sorts ( ) ! = null ) { XContentParser completeSortParser = null ; try { XContentBuilder completeSortBuilder = XContentFactory . jsonBuilder ( ) ; completeSortBuilder . startObject ( ) ; completeSortBuilder . startArray ( <str> ) ; for ( BytesReference sort : source . sorts ( ) ) { XContentParser parser = XContentFactory . xContent ( sort ) . createParser ( sort ) ; parser . nextToken ( ) ; completeSortBuilder . copyCurrentStructure ( parser ) ; } completeSortBuilder . endArray ( ) ; completeSortBuilder . endObject ( ) ; BytesReference completeSortBytes = completeSortBuilder . bytes ( ) ; completeSortParser = XContentFactory . xContent ( completeSortBytes ) . createParser ( completeSortBytes ) ; completeSortParser . nextToken ( ) ; completeSortParser . nextToken ( ) ; completeSortParser . nextToken ( ) ; this . elementParsers . get ( <str> ) . parse ( completeSortParser , context ) ; } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = completeSortParser ! = null ? completeSortParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } context . trackScores ( source . trackScores ( ) ) ; if ( source . minScore ( ) ! = null ) { context . minimumScore ( source . minScore ( ) ) ; } context . timeoutInMillis ( source . timeoutInMillis ( ) ) ; context . terminateAfter ( source . terminateAfter ( ) ) ; if ( source . aggregations ( ) ! = null ) { XContentParser completeAggregationsParser = null ; try { XContentBuilder completeAggregationsBuilder = XContentFactory . jsonBuilder ( ) ; completeAggregationsBuilder . startObject ( ) ; for ( BytesReference agg : source . aggregations ( ) ) { XContentParser parser = XContentFactory . xContent ( agg ) . createParser ( agg ) ; parser . nextToken ( ) ; parser . nextToken ( ) ; completeAggregationsBuilder . field ( parser . currentName ( ) ) ; parser . nextToken ( ) ; completeAggregationsBuilder . copyCurrentStructure ( parser ) ; } completeAggregationsBuilder . endObject ( ) ; BytesReference completeAggregationsBytes = completeAggregationsBuilder . bytes ( ) ; completeAggregationsParser = XContentFactory . xContent ( completeAggregationsBytes ) . createParser ( completeAggregationsBytes ) ; completeAggregationsParser . nextToken ( ) ; this . elementParsers . get ( <str> ) . parse ( completeAggregationsParser , context ) ; } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = completeAggregationsParser ! = null ? completeAggregationsParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } if ( source . suggest ( ) ! = null ) { XContentParser suggestParser = null ; try { suggestParser = XContentFactory . xContent ( source . suggest ( ) ) . createParser ( source . suggest ( ) ) ; suggestParser . nextToken ( ) ; this . elementParsers . get ( <str> ) . parse ( suggestParser , context ) ; } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = suggestParser ! = null ? suggestParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } if ( source . rescores ( ) ! = null ) { XContentParser completeRescoreParser = null ; try { XContentBuilder completeRescoreBuilder = XContentFactory . jsonBuilder ( ) ; completeRescoreBuilder . startObject ( ) ; completeRescoreBuilder . startArray ( <str> ) ; for ( BytesReference rescore : source . rescores ( ) ) { XContentParser parser = XContentFactory . xContent ( rescore ) . createParser ( rescore ) ; parser . nextToken ( ) ; completeRescoreBuilder . copyCurrentStructure ( parser ) ; } completeRescoreBuilder . endArray ( ) ; completeRescoreBuilder . endObject ( ) ; BytesReference completeRescoreBytes = completeRescoreBuilder . bytes ( ) ; completeRescoreParser = XContentFactory . xContent ( completeRescoreBytes ) . createParser ( completeRescoreBytes ) ; completeRescoreParser . nextToken ( ) ; completeRescoreParser . nextToken ( ) ; completeRescoreParser . nextToken ( ) ; this . elementParsers . get ( <str> ) . parse ( completeRescoreParser , context ) ; } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = completeRescoreParser ! = null ? completeRescoreParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } if ( source . fields ( ) ! = null ) { context . fieldNames ( ) . addAll ( source . fields ( ) ) ; } if ( source . explain ( ) ! = null ) { context . explain ( source . explain ( ) ) ; } if ( source . fetchSource ( ) ! = null ) { context . fetchSourceContext ( source . fetchSource ( ) ) ; } if ( source . fieldDataFields ( ) ! = null ) { FieldDataFieldsContext fieldDataFieldsContext = context . getFetchSubPhaseContext ( FieldDataFieldsFetchSubPhase . CONTEXT_FACTORY ) ; for ( String field : source . fieldDataFields ( ) ) { fieldDataFieldsContext . add ( new FieldDataField ( field ) ) ; } fieldDataFieldsContext . setHitExecutionNeeded ( true ) ; } if ( source . highlighter ( ) ! = null ) { XContentParser highlighterParser = null ; try { highlighterParser = XContentFactory . xContent ( source . highlighter ( ) ) . createParser ( source . highlighter ( ) ) ; this . elementParsers . get ( <str> ) . parse ( highlighterParser , context ) ; } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = highlighterParser ! = null ? highlighterParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } if ( source . innerHits ( ) ! = null ) { XContentParser innerHitsParser = null ; try { innerHitsParser = XContentFactory . xContent ( source . innerHits ( ) ) . createParser ( source . innerHits ( ) ) ; innerHitsParser . nextToken ( ) ; this . elementParsers . get ( <str> ) . parse ( innerHitsParser , context ) ; } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = innerHitsParser ! = null ? innerHitsParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } if ( source . scriptFields ( ) ! = null ) { for ( org . elasticsearch . search . builder . SearchSourceBuilder . ScriptField field : source . scriptFields ( ) ) { SearchScript searchScript = context . scriptService ( ) . search ( context . lookup ( ) , field . script ( ) , ScriptContext . Standard . SEARCH ) ; context . scriptFields ( ) . add ( new ScriptField ( field . fieldName ( ) , searchScript , field . ignoreFailure ( ) ) ) ; } } if ( source . ext ( ) ! = null ) { XContentParser extParser = null ; try { extParser = XContentFactory . xContent ( source . ext ( ) ) . createParser ( source . ext ( ) ) ; XContentParser . Token token = extParser . nextToken ( ) ; String currentFieldName = null ; while ( ( token = extParser . nextToken ( ) ) ! = XContentParser . Token . END_OBJECT ) { if ( token = = XContentParser . Token . FIELD_NAME ) { currentFieldName = extParser . currentName ( ) ; } else { SearchParseElement parseElement = this . elementParsers . get ( currentFieldName ) ; if ( parseElement = = null ) { throw new SearchParseException ( context , <str> + currentFieldName + <str> , extParser . getTokenLocation ( ) ) ; } else { parseElement . parse ( extParser , context ) ; } } } } catch ( Exception e ) { String sSource = <str> ; try { sSource = source . toString ( ) ; } catch ( Throwable e1 ) { } XContentLocation location = extParser ! = null ? extParser . getTokenLocation ( ) : null ; throw new SearchParseException ( context , <str> + sSource + <str> , location , e ) ; } } if ( source . version ( ) ! = null ) { context . version ( source . version ( ) ) ; } if ( source . stats ( ) ! = null ) { context . groupStats ( source . stats ( ) ) ; } } private static final int [ ] EMPTY_DOC_IDS = new int [ <int> ] ; private void shortcutDocIdsToLoad ( SearchContext context ) { if ( context . request ( ) . scroll ( ) ! = null ) { TopDocs topDocs = context . queryResult ( ) . topDocs ( ) ; int [ ] docIdsToLoad = new int [ topDocs . scoreDocs . length ] ; for ( int i = <int> ; i < topDocs . scoreDocs . length ; i + + ) { docIdsToLoad [ i ] = topDocs . scoreDocs [ i ] . doc ; } context . docIdsToLoad ( docIdsToLoad , <int> , docIdsToLoad . length ) ; } else { TopDocs topDocs = context . queryResult ( ) . topDocs ( ) ; if ( topDocs . scoreDocs . length < context . from ( ) ) { context . docIdsToLoad ( EMPTY_DOC_IDS , <int> , <int> ) ; return ; } int totalSize = context . from ( ) + context . size ( ) ; int [ ] docIdsToLoad = new int [ Math . min ( topDocs . scoreDocs . length - context . from ( ) , context . size ( ) ) ] ; int counter = <int> ; for ( int i = context . from ( ) ; i < totalSize ; i + + ) { if ( i < topDocs . scoreDocs . length ) { docIdsToLoad [ counter ] = topDocs . scoreDocs [ i ] . doc ; } else { break ; } counter + + ; } context . docIdsToLoad ( docIdsToLoad , <int> , counter ) ; } } private void shortcutDocIdsToLoadForScanning ( SearchContext context ) { TopDocs topDocs = context . queryResult ( ) . topDocs ( ) ; if ( topDocs . scoreDocs . length = = <int> ) { context . docIdsToLoad ( EMPTY_DOC_IDS , <int> , <int> ) ; return ; } int [ ] docIdsToLoad = new int [ topDocs . scoreDocs . length ] ; for ( int i = <int> ; i < docIdsToLoad . length ; i + + ) { docIdsToLoad [ i ] = topDocs . scoreDocs [ i ] . doc ; } context . docIdsToLoad ( docIdsToLoad , <int> , docIdsToLoad . length ) ; } private void processScroll ( InternalScrollSearchRequest request , SearchContext context ) { context . from ( context . from ( ) + context . size ( ) ) ; context . scrollContext ( ) . scroll = request . scroll ( ) ; if ( request . scroll ( ) ! = null & & request . scroll ( ) . keepAlive ( ) ! = null ) { context . keepAlive ( request . scroll ( ) . keepAlive ( ) . millis ( ) ) ; } } public int getActiveContexts ( ) { return this . activeContexts . size ( ) ; } static class NormsWarmer implements IndicesWarmer . Listener { private final IndicesWarmer indicesWarmer ; public NormsWarmer ( IndicesWarmer indicesWarmer ) { this . indicesWarmer = indicesWarmer ; } @Override public TerminationHandle warmNewReaders ( final IndexShard indexShard , final Engine . Searcher searcher ) { final Loading defaultLoading = Loading . parse ( indexShard . getIndexSettings ( ) . getSettings ( ) . get ( NORMS_LOADING_KEY ) , Loading . LAZY ) ; final MapperService mapperService = indexShard . mapperService ( ) ; final ObjectSet < String > warmUp = new ObjectHashSet < > ( ) ; for ( DocumentMapper docMapper : mapperService . docMappers ( false ) ) { for ( FieldMapper fieldMapper : docMapper . mappers ( ) ) { final String indexName = fieldMapper . fieldType ( ) . names ( ) . indexName ( ) ; Loading normsLoading = fieldMapper . fieldType ( ) . normsLoading ( ) ; if ( normsLoading = = null ) { normsLoading = defaultLoading ; } if ( fieldMapper . fieldType ( ) . indexOptions ( ) ! = IndexOptions . NONE & & ! fieldMapper . fieldType ( ) . omitNorms ( ) & & normsLoading = = Loading . EAGER ) { warmUp . add ( indexName ) ; } } } final CountDownLatch latch = new CountDownLatch ( <int> ) ; indicesWarmer . getExecutor ( ) . execute ( new Runnable ( ) { @Override public void run ( ) { try { for ( ObjectCursor < String > stringObjectCursor : warmUp ) { final String indexName = stringObjectCursor . value ; final long start = System . nanoTime ( ) ; for ( final LeafReaderContext ctx : searcher . reader ( ) . leaves ( ) ) { final NumericDocValues values = ctx . reader ( ) . getNormValues ( indexName ) ; if ( values ! = null ) { values . get ( <int> ) ; } } if ( indexShard . warmerService ( ) . logger ( ) . isTraceEnabled ( ) ) { indexShard . warmerService ( ) . logger ( ) . trace ( <str> , indexName , TimeValue . timeValueNanos ( System . nanoTime ( ) - start ) ) ; } } } catch ( Throwable t ) { indexShard . warmerService ( ) . logger ( ) . warn ( <str> , t ) ; } finally { latch . countDown ( ) ; } } } ) ; return new TerminationHandle ( ) { @Override public void awaitTermination ( ) throws InterruptedException { latch . await ( ) ; } } ; } @Override public TerminationHandle warmTopReader ( IndexShard indexShard , final Engine . Searcher searcher ) { return TerminationHandle . NO_WAIT ; } } static class FieldDataWarmer implements IndicesWarmer . Listener { private final IndicesWarmer indicesWarmer ; public FieldDataWarmer ( IndicesWarmer indicesWarmer ) { this . indicesWarmer = indicesWarmer ; } @Override public TerminationHandle warmNewReaders ( final IndexShard indexShard , final Engine . Searcher searcher ) { final MapperService mapperService = indexShard . mapperService ( ) ; final Map < String , MappedFieldType > warmUp = new HashMap < > ( ) ; for ( DocumentMapper docMapper : mapperService . docMappers ( false ) ) { for ( FieldMapper fieldMapper : docMapper . mappers ( ) ) { final FieldDataType fieldDataType ; final String indexName ; if ( fieldMapper instanceof ParentFieldMapper ) { MappedFieldType joinFieldType = ( ( ParentFieldMapper ) fieldMapper ) . getChildJoinFieldType ( ) ; if ( joinFieldType = = null ) { continue ; } fieldDataType = joinFieldType . fieldDataType ( ) ; indexName = fieldMapper . fieldType ( ) . names ( ) . indexName ( ) ; } else { fieldDataType = fieldMapper . fieldType ( ) . fieldDataType ( ) ; indexName = fieldMapper . fieldType ( ) . names ( ) . indexName ( ) ; } if ( fieldDataType = = null ) { continue ; } if ( fieldDataType . getLoading ( ) = = Loading . LAZY ) { continue ; } if ( warmUp . containsKey ( indexName ) ) { continue ; } warmUp . put ( indexName , fieldMapper . fieldType ( ) ) ; } } final IndexFieldDataService indexFieldDataService = indexShard . indexFieldDataService ( ) ; final Executor executor = indicesWarmer . getExecutor ( ) ; final CountDownLatch latch = new CountDownLatch ( searcher . reader ( ) . leaves ( ) . size ( ) * warmUp . size ( ) ) ; for ( final LeafReaderContext ctx : searcher . reader ( ) . leaves ( ) ) { for ( final MappedFieldType fieldType : warmUp . values ( ) ) { executor . execute ( new Runnable ( ) { @Override public void run ( ) { try { final long start = System . nanoTime ( ) ; indexFieldDataService . getForField ( fieldType ) . load ( ctx ) ; if ( indexShard . warmerService ( ) . logger ( ) . isTraceEnabled ( ) ) { indexShard . warmerService ( ) . logger ( ) . trace ( <str> , fieldType . names ( ) . fullName ( ) , TimeValue . timeValueNanos ( System . nanoTime ( ) - start ) ) ; } } catch ( Throwable t ) { indexShard . warmerService ( ) . logger ( ) . warn ( <str> , t , fieldType . names ( ) . fullName ( ) ) ; } finally { latch . countDown ( ) ; } } } ) ; } } return new TerminationHandle ( ) { @Override public void awaitTermination ( ) throws InterruptedException { latch . await ( ) ; } } ; } @Override public TerminationHandle warmTopReader ( final IndexShard indexShard , final Engine . Searcher searcher ) { final MapperService mapperService = indexShard . mapperService ( ) ; final Map < String , MappedFieldType > warmUpGlobalOrdinals = new HashMap < > ( ) ; for ( DocumentMapper docMapper : mapperService . docMappers ( false ) ) { for ( FieldMapper fieldMapper : docMapper . mappers ( ) ) { final FieldDataType fieldDataType ; final String indexName ; if ( fieldMapper instanceof ParentFieldMapper ) { MappedFieldType joinFieldType = ( ( ParentFieldMapper ) fieldMapper ) . getChildJoinFieldType ( ) ; if ( joinFieldType = = null ) { continue ; } fieldDataType = joinFieldType . fieldDataType ( ) ; indexName = fieldMapper . fieldType ( ) . names ( ) . indexName ( ) ; } else { fieldDataType = fieldMapper . fieldType ( ) . fieldDataType ( ) ; indexName = fieldMapper . fieldType ( ) . names ( ) . indexName ( ) ; } if ( fieldDataType = = null ) { continue ; } if ( fieldDataType . getLoading ( ) ! = Loading . EAGER_GLOBAL_ORDINALS ) { continue ; } if ( warmUpGlobalOrdinals . containsKey ( indexName ) ) { continue ; } warmUpGlobalOrdinals . put ( indexName , fieldMapper . fieldType ( ) ) ; } } final IndexFieldDataService indexFieldDataService = indexShard . indexFieldDataService ( ) ; final Executor executor = indicesWarmer . getExecutor ( ) ; final CountDownLatch latch = new CountDownLatch ( warmUpGlobalOrdinals . size ( ) ) ; for ( final MappedFieldType fieldType : warmUpGlobalOrdinals . values ( ) ) { executor . execute ( new Runnable ( ) { @Override public void run ( ) { try { final long start = System . nanoTime ( ) ; IndexFieldData . Global ifd = indexFieldDataService . getForField ( fieldType ) ; ifd . loadGlobal ( searcher . getDirectoryReader ( ) ) ; if ( indexShard . warmerService ( ) . logger ( ) . isTraceEnabled ( ) ) { indexShard . warmerService ( ) . logger ( ) . trace ( <str> , fieldType . names ( ) . fullName ( ) , TimeValue . timeValueNanos ( System . nanoTime ( ) - start ) ) ; } } catch ( Throwable t ) { indexShard . warmerService ( ) . logger ( ) . warn ( <str> , t , fieldType . names ( ) . fullName ( ) ) ; } finally { latch . countDown ( ) ; } } } ) ; } return new TerminationHandle ( ) { @Override public void awaitTermination ( ) throws InterruptedException { latch . await ( ) ; } } ; } } class SearchWarmer implements IndicesWarmer . Listener { @Override public TerminationHandle warmNewReaders ( IndexShard indexShard , final Engine . Searcher searcher ) { return internalWarm ( indexShard , searcher , false ) ; } @Override public TerminationHandle warmTopReader ( IndexShard indexShard , final Engine . Searcher searcher ) { return internalWarm ( indexShard , searcher , true ) ; } public TerminationHandle internalWarm ( final IndexShard indexShard , final Engine . Searcher searcher , final boolean top ) { IndexWarmersMetaData custom = indexShard . getIndexSettings ( ) . getIndexMetaData ( ) . custom ( IndexWarmersMetaData . TYPE ) ; if ( custom = = null ) { return TerminationHandle . NO_WAIT ; } final Executor executor = indicesWarmer . getExecutor ( ) ; final CountDownLatch latch = new CountDownLatch ( custom . entries ( ) . size ( ) ) ; for ( final IndexWarmersMetaData . Entry entry : custom . entries ( ) ) { executor . execute ( ( ) - > { SearchContext context = null ; try { long now = System . nanoTime ( ) ; final IndexService indexService = indicesService . indexServiceSafe ( indexShard . shardId ( ) . index ( ) . name ( ) ) ; QueryParseContext queryParseContext = new QueryParseContext ( indicesService . getIndicesQueryRegistry ( ) ) ; queryParseContext . parseFieldMatcher ( indexService . getIndexSettings ( ) . getParseFieldMatcher ( ) ) ; ShardSearchRequest request = new ShardSearchLocalRequest ( indexShard . shardId ( ) , indexShard . getIndexSettings ( ) . getNumberOfShards ( ) , SearchType . QUERY_THEN_FETCH , entry . source ( ) . build ( queryParseContext ) , entry . types ( ) , entry . requestCache ( ) ) ; context = createContext ( request , searcher ) ; if ( context . sort ( ) = = null ) { context . size ( <int> ) ; } boolean canCache = indicesQueryCache . canCache ( request , context ) ; if ( canCache ! = top ) { return ; } loadOrExecuteQueryPhase ( request , context , queryPhase ) ; long took = System . nanoTime ( ) - now ; if ( indexShard . warmerService ( ) . logger ( ) . isTraceEnabled ( ) ) { indexShard . warmerService ( ) . logger ( ) . trace ( <str> , entry . name ( ) , TimeValue . timeValueNanos ( took ) ) ; } } catch ( Throwable t ) { indexShard . warmerService ( ) . logger ( ) . warn ( <str> , t , entry . name ( ) ) ; } finally { try { if ( context ! = null ) { freeContext ( context . id ( ) ) ; cleanContext ( context ) ; } } finally { latch . countDown ( ) ; } } } ) ; } return ( ) - > latch . await ( ) ; } } class Reaper implements Runnable { @Override public void run ( ) { final long time = threadPool . estimatedTimeInMillis ( ) ; for ( SearchContext context : activeContexts . values ( ) ) { final long lastAccessTime = context . lastAccessTime ( ) ; if ( lastAccessTime = = - <int> ) { continue ; } if ( ( time - lastAccessTime > context . keepAlive ( ) ) ) { logger . debug ( <str> , context . id ( ) , time , lastAccessTime , context . keepAlive ( ) ) ; freeContext ( context . id ( ) ) ; } } } } } 
