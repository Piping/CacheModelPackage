package org . apache . cassandra . streaming . compression ; import java . io . * ; import java . util . * ; import java . util . concurrent . SynchronousQueue ; import java . util . concurrent . TimeUnit ; import org . junit . Test ; import org . apache . cassandra . db . ClusteringComparator ; import org . apache . cassandra . db . marshal . BytesType ; import org . apache . cassandra . io . compress . CompressedSequentialWriter ; import org . apache . cassandra . io . compress . CompressionMetadata ; import org . apache . cassandra . schema . CompressionParams ; import org . apache . cassandra . io . sstable . Component ; import org . apache . cassandra . io . sstable . Descriptor ; import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; import org . apache . cassandra . streaming . compress . CompressedInputStream ; import org . apache . cassandra . streaming . compress . CompressionInfo ; import org . apache . cassandra . utils . ChecksumType ; import org . apache . cassandra . utils . Pair ; import static org . junit . Assert . assertEquals ; public class CompressedInputStreamTest { @Test public void testCompressedRead ( ) throws Exception { testCompressedReadWith ( new long [ ] { <int> L } , false ) ; testCompressedReadWith ( new long [ ] { <int> } , false ) ; testCompressedReadWith ( new long [ ] { <int> } , false ) ; testCompressedReadWith ( new long [ ] { <int> , <int> , <int> , <int> , <int> } , false ) ; } @Test ( expected = EOFException . class ) public void testTruncatedRead ( ) throws Exception { testCompressedReadWith ( new long [ ] { <int> , <int> , <int> , <int> , <int> } , true ) ; } @Test ( expected = EOFException . class ) public void testClose ( ) throws IOException { CompressionParams param = CompressionParams . snappy ( <int> ) ; CompressionMetadata . Chunk [ ] chunks = { new CompressionMetadata . Chunk ( <int> , <int> ) } ; final SynchronousQueue < Integer > blocker = new SynchronousQueue < > ( ) ; InputStream blockingInput = new InputStream ( ) { @Override public int read ( ) throws IOException { try { return Objects . requireNonNull ( blocker . poll ( <int> , TimeUnit . SECONDS ) ) ; } catch ( InterruptedException e ) { throw new IOException ( <str> , e ) ; } } } ; CompressionInfo info = new CompressionInfo ( chunks , param ) ; try ( CompressedInputStream cis = new CompressedInputStream ( blockingInput , info , ChecksumType . CRC32 , ( ) - > <float> ) ) { new Thread ( new Runnable ( ) { @Override public void run ( ) { try { cis . close ( ) ; } catch ( Exception ignore ) { } } } ) . start ( ) ; cis . read ( ) ; } } private void testCompressedReadWith ( long [ ] valuesToCheck , boolean testTruncate ) throws Exception { assert valuesToCheck ! = null & & valuesToCheck . length > <int> ; File tmp = new File ( File . createTempFile ( <str> , <str> ) . getParent ( ) , <str> ) ; Descriptor desc = Descriptor . fromFilename ( tmp . getAbsolutePath ( ) ) ; MetadataCollector collector = new MetadataCollector ( new ClusteringComparator ( BytesType . instance ) ) ; CompressionParams param = CompressionParams . snappy ( <int> ) ; Map < Long , Long > index = new HashMap < Long , Long > ( ) ; try ( CompressedSequentialWriter writer = new CompressedSequentialWriter ( tmp , desc . filenameFor ( Component . COMPRESSION_INFO ) , param , collector ) ) { for ( long l = <int> L ; l < <int> ; l + + ) { index . put ( l , writer . position ( ) ) ; writer . writeLong ( l ) ; } writer . finish ( ) ; } CompressionMetadata comp = CompressionMetadata . create ( tmp . getAbsolutePath ( ) ) ; List < Pair < Long , Long > > sections = new ArrayList < > ( ) ; for ( long l : valuesToCheck ) { long position = index . get ( l ) ; sections . add ( Pair . create ( position , position + <int> ) ) ; } CompressionMetadata . Chunk [ ] chunks = comp . getChunksForSections ( sections ) ; long totalSize = comp . getTotalSizeForSections ( sections ) ; long expectedSize = <int> ; for ( CompressionMetadata . Chunk c : chunks ) expectedSize + = c . length + <int> ; assertEquals ( expectedSize , totalSize ) ; int size = <int> ; for ( CompressionMetadata . Chunk c : chunks ) size + = ( c . length + <int> ) ; byte [ ] toRead = new byte [ size ] ; try ( RandomAccessFile f = new RandomAccessFile ( tmp , <str> ) ) { int pos = <int> ; for ( CompressionMetadata . Chunk c : chunks ) { f . seek ( c . offset ) ; pos + = f . read ( toRead , pos , c . length + <int> ) ; } } if ( testTruncate ) { byte [ ] actuallyRead = new byte [ <int> ] ; System . arraycopy ( toRead , <int> , actuallyRead , <int> , <int> ) ; toRead = actuallyRead ; } CompressionInfo info = new CompressionInfo ( chunks , param ) ; CompressedInputStream input = new CompressedInputStream ( new ByteArrayInputStream ( toRead ) , info , ChecksumType . CRC32 , ( ) - > <float> ) ; try ( DataInputStream in = new DataInputStream ( input ) ) { for ( int i = <int> ; i < sections . size ( ) ; i + + ) { input . position ( sections . get ( i ) . left ) ; long readValue = in . readLong ( ) ; assertEquals ( <str> + valuesToCheck [ i ] + <str> + readValue , valuesToCheck [ i ] , readValue ) ; } } } } 
