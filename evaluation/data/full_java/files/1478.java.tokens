package org . apache . cassandra . db . compaction ; import java . nio . ByteBuffer ; import java . util . * ; import com . google . common . primitives . Longs ; import org . junit . * ; import org . apache . cassandra . cql3 . CQLTester ; import org . apache . cassandra . cql3 . QueryProcessor ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . compaction . writers . CompactionAwareWriter ; import org . apache . cassandra . db . compaction . writers . DefaultCompactionWriter ; import org . apache . cassandra . db . compaction . writers . MajorLeveledCompactionWriter ; import org . apache . cassandra . db . compaction . writers . MaxSSTableSizeWriter ; import org . apache . cassandra . db . compaction . writers . SplittingSizeTieredCompactionWriter ; import org . apache . cassandra . db . lifecycle . LifecycleTransaction ; import org . apache . cassandra . io . sstable . format . SSTableReader ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . UUIDGen ; import static org . junit . Assert . assertEquals ; public class CompactionAwareWriterTest extends CQLTester { private static final String KEYSPACE = <str> ; private static final String TABLE = <str> ; private static final int ROW_PER_PARTITION = <int> ; @BeforeClass public static void beforeClass ( ) throws Throwable { schemaChange ( <str> + KEYSPACE + <str> ) ; schemaChange ( String . format ( <str> , KEYSPACE , TABLE ) ) ; } @AfterClass public static void tearDownClass ( ) { QueryProcessor . executeInternal ( <str> + KEYSPACE ) ; } private ColumnFamilyStore getColumnFamilyStore ( ) { return Keyspace . open ( KEYSPACE ) . getColumnFamilyStore ( TABLE ) ; } @Test public void testDefaultCompactionWriter ( ) throws Throwable { Keyspace ks = Keyspace . open ( KEYSPACE ) ; ColumnFamilyStore cfs = ks . getColumnFamilyStore ( TABLE ) ; int rowCount = <int> ; cfs . disableAutoCompaction ( ) ; populate ( rowCount ) ; LifecycleTransaction txn = cfs . getTracker ( ) . tryModify ( cfs . getLiveSSTables ( ) , OperationType . COMPACTION ) ; long beforeSize = txn . originals ( ) . iterator ( ) . next ( ) . onDiskLength ( ) ; CompactionAwareWriter writer = new DefaultCompactionWriter ( cfs , cfs . getDirectories ( ) , txn , txn . originals ( ) ) ; int rows = compact ( cfs , txn , writer ) ; assertEquals ( <int> , cfs . getLiveSSTables ( ) . size ( ) ) ; assertEquals ( rowCount , rows ) ; assertEquals ( beforeSize , cfs . getLiveSSTables ( ) . iterator ( ) . next ( ) . onDiskLength ( ) ) ; validateData ( cfs , rowCount ) ; cfs . truncateBlocking ( ) ; } @Test public void testMaxSSTableSizeWriter ( ) throws Throwable { ColumnFamilyStore cfs = getColumnFamilyStore ( ) ; cfs . disableAutoCompaction ( ) ; int rowCount = <int> ; populate ( rowCount ) ; LifecycleTransaction txn = cfs . getTracker ( ) . tryModify ( cfs . getLiveSSTables ( ) , OperationType . COMPACTION ) ; long beforeSize = txn . originals ( ) . iterator ( ) . next ( ) . onDiskLength ( ) ; int sstableSize = ( int ) beforeSize / <int> ; CompactionAwareWriter writer = new MaxSSTableSizeWriter ( cfs , cfs . getDirectories ( ) , txn , txn . originals ( ) , sstableSize , <int> ) ; int rows = compact ( cfs , txn , writer ) ; assertEquals ( <int> , cfs . getLiveSSTables ( ) . size ( ) ) ; assertEquals ( rowCount , rows ) ; validateData ( cfs , rowCount ) ; cfs . truncateBlocking ( ) ; } @Test public void testSplittingSizeTieredCompactionWriter ( ) throws Throwable { ColumnFamilyStore cfs = getColumnFamilyStore ( ) ; cfs . disableAutoCompaction ( ) ; int rowCount = <int> ; populate ( rowCount ) ; LifecycleTransaction txn = cfs . getTracker ( ) . tryModify ( cfs . getLiveSSTables ( ) , OperationType . COMPACTION ) ; long beforeSize = txn . originals ( ) . iterator ( ) . next ( ) . onDiskLength ( ) ; CompactionAwareWriter writer = new SplittingSizeTieredCompactionWriter ( cfs , cfs . getDirectories ( ) , txn , txn . originals ( ) , <int> ) ; int rows = compact ( cfs , txn , writer ) ; long expectedSize = beforeSize / <int> ; List < SSTableReader > sortedSSTables = new ArrayList < > ( cfs . getLiveSSTables ( ) ) ; Collections . sort ( sortedSSTables , new Comparator < SSTableReader > ( ) { @Override public int compare ( SSTableReader o1 , SSTableReader o2 ) { return Longs . compare ( o2 . onDiskLength ( ) , o1 . onDiskLength ( ) ) ; } } ) ; for ( SSTableReader sstable : sortedSSTables ) { if ( expectedSize > SplittingSizeTieredCompactionWriter . DEFAULT_SMALLEST_SSTABLE_BYTES ) assertEquals ( expectedSize , sstable . onDiskLength ( ) , expectedSize / <int> ) ; expectedSize / = <int> ; } assertEquals ( rowCount , rows ) ; validateData ( cfs , rowCount ) ; cfs . truncateBlocking ( ) ; } @Test public void testMajorLeveledCompactionWriter ( ) throws Throwable { ColumnFamilyStore cfs = getColumnFamilyStore ( ) ; cfs . disableAutoCompaction ( ) ; int rowCount = <int> ; int targetSSTableCount = <int> ; populate ( rowCount ) ; LifecycleTransaction txn = cfs . getTracker ( ) . tryModify ( cfs . getLiveSSTables ( ) , OperationType . COMPACTION ) ; long beforeSize = txn . originals ( ) . iterator ( ) . next ( ) . onDiskLength ( ) ; int sstableSize = ( int ) beforeSize / targetSSTableCount ; CompactionAwareWriter writer = new MajorLeveledCompactionWriter ( cfs , cfs . getDirectories ( ) , txn , txn . originals ( ) , sstableSize ) ; int rows = compact ( cfs , txn , writer ) ; assertEquals ( targetSSTableCount , cfs . getLiveSSTables ( ) . size ( ) ) ; int [ ] levelCounts = new int [ <int> ] ; assertEquals ( rowCount , rows ) ; for ( SSTableReader sstable : cfs . getLiveSSTables ( ) ) { levelCounts [ sstable . getSSTableLevel ( ) ] + + ; } assertEquals ( <int> , levelCounts [ <int> ] ) ; assertEquals ( <int> , levelCounts [ <int> ] ) ; assertEquals ( targetSSTableCount - <int> , levelCounts [ <int> ] ) ; for ( int i = <int> ; i < levelCounts . length ; i + + ) assertEquals ( <int> , levelCounts [ i ] ) ; validateData ( cfs , rowCount ) ; cfs . truncateBlocking ( ) ; } private int compact ( ColumnFamilyStore cfs , LifecycleTransaction txn , CompactionAwareWriter writer ) { assert txn . originals ( ) . size ( ) = = <int> ; int rowsWritten = <int> ; int nowInSec = FBUtilities . nowInSeconds ( ) ; try ( AbstractCompactionStrategy . ScannerList scanners = cfs . getCompactionStrategyManager ( ) . getScanners ( txn . originals ( ) ) ; CompactionController controller = new CompactionController ( cfs , txn . originals ( ) , cfs . gcBefore ( nowInSec ) ) ; CompactionIterator ci = new CompactionIterator ( OperationType . COMPACTION , scanners . scanners , controller , nowInSec , UUIDGen . getTimeUUID ( ) ) ) { while ( ci . hasNext ( ) ) { if ( writer . append ( ci . next ( ) ) ) rowsWritten + + ; } } writer . finish ( ) ; return rowsWritten ; } private void populate ( int count ) throws Throwable { byte [ ] payload = new byte [ <int> ] ; new Random ( <int> ) . nextBytes ( payload ) ; ByteBuffer b = ByteBuffer . wrap ( payload ) ; for ( int i = <int> ; i < count ; i + + ) for ( int j = <int> ; j < ROW_PER_PARTITION ; j + + ) execute ( String . format ( <str> , KEYSPACE , TABLE ) , i , j , b ) ; ColumnFamilyStore cfs = getColumnFamilyStore ( ) ; cfs . forceBlockingFlush ( ) ; if ( cfs . getLiveSSTables ( ) . size ( ) > <int> ) { try { cfs . forceMajorCompaction ( ) ; } catch ( Throwable t ) { throw new RuntimeException ( t ) ; } } assert cfs . getLiveSSTables ( ) . size ( ) = = <int> : cfs . getLiveSSTables ( ) ; } private void validateData ( ColumnFamilyStore cfs , int rowCount ) throws Throwable { for ( int i = <int> ; i < rowCount ; i + + ) { Object [ ] [ ] expected = new Object [ ROW_PER_PARTITION ] [ ] ; for ( int j = <int> ; j < ROW_PER_PARTITION ; j + + ) expected [ j ] = row ( i , j ) ; assertRows ( execute ( String . format ( <str> , KEYSPACE , TABLE ) , i ) , expected ) ; } } } 
