package org . apache . cassandra . streaming . compress ; import java . io . DataInputStream ; import java . io . IOException ; import java . nio . channels . Channels ; import java . nio . channels . ReadableByteChannel ; import com . google . common . base . Throwables ; import org . apache . cassandra . io . sstable . SSTableMultiWriter ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . config . Schema ; import org . apache . cassandra . db . ColumnFamilyStore ; import org . apache . cassandra . db . Keyspace ; import org . apache . cassandra . io . compress . CompressionMetadata ; import org . apache . cassandra . streaming . ProgressInfo ; import org . apache . cassandra . streaming . StreamReader ; import org . apache . cassandra . streaming . StreamSession ; import org . apache . cassandra . streaming . messages . FileMessageHeader ; import org . apache . cassandra . utils . BytesReadTracker ; import org . apache . cassandra . utils . Pair ; public class CompressedStreamReader extends StreamReader { private static final Logger logger = LoggerFactory . getLogger ( CompressedStreamReader . class ) ; protected final CompressionInfo compressionInfo ; public CompressedStreamReader ( FileMessageHeader header , StreamSession session ) { super ( header , session ) ; this . compressionInfo = header . compressionInfo ; } @Override @SuppressWarnings ( <str> ) public SSTableMultiWriter read ( ReadableByteChannel channel ) throws IOException { logger . debug ( <str> , session . peer , repairedAt ) ; long totalSize = totalSize ( ) ; Pair < String , String > kscf = Schema . instance . getCF ( cfId ) ; if ( kscf = = null ) { throw new IOException ( <str> + cfId + <str> ) ; } ColumnFamilyStore cfs = Keyspace . open ( kscf . left ) . getColumnFamilyStore ( kscf . right ) ; CompressedInputStream cis = new CompressedInputStream ( Channels . newInputStream ( channel ) , compressionInfo , inputVersion . compressedChecksumType ( ) , cfs : : getCrcCheckChance ) ; BytesReadTracker in = new BytesReadTracker ( new DataInputStream ( cis ) ) ; StreamDeserializer deserializer = new StreamDeserializer ( cfs . metadata , in , inputVersion , header . toHeader ( cfs . metadata ) ) ; SSTableMultiWriter writer = null ; try { writer = createWriter ( cfs , totalSize , repairedAt , format ) ; for ( Pair < Long , Long > section : sections ) { assert cis . getTotalCompressedBytesRead ( ) < = totalSize ; int sectionLength = ( int ) ( section . right - section . left ) ; cis . position ( section . left ) ; in . reset ( <int> ) ; while ( in . getBytesRead ( ) < sectionLength ) { writePartition ( deserializer , writer ) ; session . progress ( desc , ProgressInfo . Direction . IN , cis . getTotalCompressedBytesRead ( ) , totalSize ) ; } } return writer ; } catch ( Throwable e ) { if ( writer ! = null ) { writer . abort ( e ) ; } drain ( cis , in . getBytesRead ( ) ) ; if ( e instanceof IOException ) throw ( IOException ) e ; else throw Throwables . propagate ( e ) ; } finally { cis . close ( ) ; } } @Override protected long totalSize ( ) { long size = <int> ; for ( CompressionMetadata . Chunk chunk : compressionInfo . chunks ) size + = chunk . length + <int> ; return size ; } } 
