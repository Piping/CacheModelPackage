package org . apache . cassandra . tools ; import java . io . File ; import java . io . IOException ; import java . io . PrintStream ; import java . util . EnumSet ; import java . util . Map ; import org . apache . cassandra . io . compress . CompressionMetadata ; import org . apache . cassandra . io . sstable . Component ; import org . apache . cassandra . io . sstable . Descriptor ; import org . apache . cassandra . io . sstable . metadata . * ; public class SSTableMetadataViewer { public static void main ( String [ ] args ) throws IOException { PrintStream out = System . out ; if ( args . length = = <int> ) { out . println ( <str> ) ; System . exit ( <int> ) ; } Util . initDatabaseDescriptor ( ) ; for ( String fname : args ) { if ( new File ( fname ) . exists ( ) ) { Descriptor descriptor = Descriptor . fromFilename ( fname ) ; Map < MetadataType , MetadataComponent > metadata = descriptor . getMetadataSerializer ( ) . deserialize ( descriptor , EnumSet . allOf ( MetadataType . class ) ) ; ValidationMetadata validation = ( ValidationMetadata ) metadata . get ( MetadataType . VALIDATION ) ; StatsMetadata stats = ( StatsMetadata ) metadata . get ( MetadataType . STATS ) ; CompactionMetadata compaction = ( CompactionMetadata ) metadata . get ( MetadataType . COMPACTION ) ; CompressionMetadata compression = null ; File compressionFile = new File ( descriptor . filenameFor ( Component . COMPRESSION_INFO ) ) ; if ( compressionFile . exists ( ) ) compression = CompressionMetadata . create ( fname ) ; out . printf ( <str> , descriptor ) ; if ( validation ! = null ) { out . printf ( <str> , validation . partitioner ) ; out . printf ( <str> , validation . bloomFilterFPChance ) ; } if ( stats ! = null ) { out . printf ( <str> , stats . minTimestamp ) ; out . printf ( <str> , stats . maxTimestamp ) ; out . printf ( <str> , stats . maxLocalDeletionTime ) ; out . printf ( <str> , compression ! = null ? compression . compressor ( ) . getClass ( ) . getName ( ) : <str> ) ; if ( compression ! = null ) out . printf ( <str> , stats . compressionRatio ) ; out . printf ( <str> , stats . getEstimatedDroppableTombstoneRatio ( ( int ) ( System . currentTimeMillis ( ) / <int> ) ) ) ; out . printf ( <str> , stats . sstableLevel ) ; out . printf ( <str> , stats . repairedAt ) ; out . println ( stats . replayPosition ) ; out . println ( <str> ) ; for ( Map . Entry < Double , Long > entry : stats . estimatedTombstoneDropTime . getAsMap ( ) . entrySet ( ) ) { out . printf ( <str> , entry . getKey ( ) . intValue ( ) , entry . getValue ( ) ) ; } printHistograms ( stats , out ) ; } if ( compaction ! = null ) { out . printf ( <str> , compaction . cardinalityEstimator . cardinality ( ) ) ; } } else { out . println ( <str> + fname ) ; } } } private static void printHistograms ( StatsMetadata metadata , PrintStream out ) { long [ ] offsets = metadata . estimatedPartitionSize . getBucketOffsets ( ) ; long [ ] ersh = metadata . estimatedPartitionSize . getBuckets ( false ) ; long [ ] ecch = metadata . estimatedColumnCount . getBuckets ( false ) ; out . println ( String . format ( <str> , <str> , <str> , <str> ) ) ; for ( int i = <int> ; i < offsets . length ; i + + ) { out . println ( String . format ( <str> , offsets [ i ] , ( i < ersh . length ? ersh [ i ] : <str> ) , ( i < ecch . length ? ecch [ i ] : <str> ) ) ) ; } } } 
