package org . nd4j . linalg . learning ; import lombok . Data ; import org . apache . commons . math3 . util . FastMath ; import org . nd4j . linalg . api . ndarray . INDArray ; import org . nd4j . linalg . factory . Nd4j ; import org . nd4j . linalg . ops . transforms . Transforms ; import java . io . Serializable ; import lombok . NoArgsConstructor ; @Data @NoArgsConstructor public class Adam implements Serializable , GradientUpdater { private double learningRate = <float> ; private double beta1 = <float> ; private double beta2 = <float> ; private double epsilon = <float> ; private INDArray m , v ; public Adam ( double alpha , double beta1 , double beta2 , double epsilon ) { this . learningRate = alpha ; this . beta1 = beta1 ; this . beta2 = beta2 ; this . epsilon = epsilon ; } public Adam ( double alpha , double beta1 , double beta2 ) { this . learningRate = alpha ; this . beta1 = beta1 ; this . beta2 = beta2 ; } public Adam ( double alpha ) { this . learningRate = alpha ; } @Override public void update ( Object . . . args ) { if ( args . length > <int> ) { learningRate = ( Double ) args [ <int> ] ; } } @Override public INDArray getGradient ( INDArray gradient , int iteration ) { if ( m = = null ) m = Nd4j . zeros ( gradient . shape ( ) ) ; if ( v = = null ) v = Nd4j . zeros ( gradient . shape ( ) ) ; INDArray oneMinusBeta1Grad = gradient . mul ( <float> - beta1 ) ; m . muli ( beta1 ) . addi ( oneMinusBeta1Grad ) ; INDArray oneMinusBeta2GradSquared = gradient . mul ( gradient ) . muli ( <int> - beta2 ) ; v . muli ( beta2 ) . addi ( oneMinusBeta2GradSquared ) ; double beta1t = FastMath . pow ( beta1 , iteration ) ; double beta2t = FastMath . pow ( beta2 , iteration ) ; double alphat = learningRate * FastMath . sqrt ( <int> - beta2t ) / ( <int> - beta1t ) ; if ( Double . isNaN ( alphat ) | | alphat = = <float> ) alphat = Nd4j . EPS_THRESHOLD ; INDArray sqrtV = Transforms . sqrt ( v ) . addi ( epsilon ) ; INDArray ret = m . mul ( alphat ) . divi ( sqrtV ) ; return ret ; } @Override public GradientUpdaterAggregator getAggregator ( boolean addThis ) { AdamAggregator ag = new AdamAggregator ( ) ; if ( addThis ) ag . aggregate ( this ) ; return ag ; } public static class AdamAggregator implements GradientUpdaterAggregator { private INDArray mSum ; private INDArray vSum ; private double lrSum ; private double beta1Sum ; private double beta2Sum ; private double epsilonSum ; private int count = <int> ; @Override public GradientUpdater getUpdater ( ) { Adam adam = new Adam ( lrSum / count , beta1Sum / count , beta2Sum / count , epsilonSum / count ) ; adam . setM ( mSum . div ( count ) ) ; adam . setV ( vSum . div ( count ) ) ; return adam ; } @Override public void aggregate ( GradientUpdater updater ) { if ( ! ( updater instanceof Adam ) ) throw new UnsupportedOperationException ( <str> + updater ) ; Adam adam = ( Adam ) updater ; if ( mSum = = null ) { mSum = adam . m . dup ( ) ; vSum = adam . v . dup ( ) ; lrSum = adam . learningRate ; beta1Sum = adam . beta1 ; beta2Sum = adam . beta2 ; epsilonSum = adam . epsilon ; } else { mSum . addi ( adam . m ) ; vSum . addi ( adam . v ) ; lrSum + = adam . learningRate ; beta1Sum + = adam . beta1 ; beta2Sum + = adam . beta2 ; epsilonSum + = adam . epsilon ; } count + + ; } @Override public GradientUpdaterAggregator combine ( GradientUpdaterAggregator other ) { if ( ! ( other instanceof AdamAggregator ) ) throw new IllegalArgumentException ( <str> + other ) ; AdamAggregator aggregator = ( AdamAggregator ) other ; mSum . addi ( aggregator . mSum ) ; vSum . addi ( aggregator . vSum ) ; lrSum + = aggregator . lrSum ; beta1Sum + = aggregator . beta1Sum ; beta2Sum + = aggregator . beta2Sum ; epsilonSum + = aggregator . epsilonSum ; count + = aggregator . count ; return this ; } } } 
