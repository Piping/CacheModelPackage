package org . elasticsearch . search . fetch . innerhits ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . document . IntField ; import org . apache . lucene . document . StringField ; import org . apache . lucene . index . IndexReader ; import org . apache . lucene . index . LeafReaderContext ; import org . apache . lucene . index . RandomIndexWriter ; import org . apache . lucene . index . Term ; import org . apache . lucene . search . ConstantScoreQuery ; import org . apache . lucene . search . DocIdSetIterator ; import org . apache . lucene . search . IndexSearcher ; import org . apache . lucene . search . Query ; import org . apache . lucene . search . TermQuery ; import org . apache . lucene . search . TotalHitCountCollector ; import org . apache . lucene . search . Weight ; import org . apache . lucene . search . join . BitSetProducer ; import org . apache . lucene . search . join . QueryBitSetProducer ; import org . apache . lucene . store . Directory ; import org . elasticsearch . search . fetch . FetchSubPhase ; import org . elasticsearch . search . fetch . innerhits . InnerHitsContext . NestedInnerHits . NestedChildrenQuery ; import org . elasticsearch . test . ESTestCase ; import java . util . ArrayList ; import java . util . List ; import static org . hamcrest . Matchers . equalTo ; public class NestedChildrenFilterTests extends ESTestCase { public void testNestedChildrenFilter ( ) throws Exception { int numParentDocs = scaledRandomIntBetween ( <int> , <int> ) ; int maxChildDocsPerParent = scaledRandomIntBetween ( <int> , <int> ) ; Directory dir = newDirectory ( ) ; RandomIndexWriter writer = new RandomIndexWriter ( random ( ) , dir ) ; for ( int i = <int> ; i < numParentDocs ; i + + ) { int numChildDocs = scaledRandomIntBetween ( <int> , maxChildDocsPerParent ) ; List < Document > docs = new ArrayList < > ( numChildDocs + <int> ) ; for ( int j = <int> ; j < numChildDocs ; j + + ) { Document childDoc = new Document ( ) ; childDoc . add ( new StringField ( <str> , <str> , Field . Store . NO ) ) ; docs . add ( childDoc ) ; } Document parenDoc = new Document ( ) ; parenDoc . add ( new StringField ( <str> , <str> , Field . Store . NO ) ) ; parenDoc . add ( new IntField ( <str> , numChildDocs , Field . Store . YES ) ) ; docs . add ( parenDoc ) ; writer . addDocuments ( docs ) ; } IndexReader reader = writer . getReader ( ) ; writer . close ( ) ; IndexSearcher searcher = new IndexSearcher ( reader ) ; FetchSubPhase . HitContext hitContext = new FetchSubPhase . HitContext ( ) ; BitSetProducer parentFilter = new QueryBitSetProducer ( new TermQuery ( new Term ( <str> , <str> ) ) ) ; Query childFilter = new TermQuery ( new Term ( <str> , <str> ) ) ; int checkedParents = <int> ; final Weight parentsWeight = searcher . createNormalizedWeight ( new TermQuery ( new Term ( <str> , <str> ) ) , false ) ; for ( LeafReaderContext leaf : reader . leaves ( ) ) { DocIdSetIterator parents = parentsWeight . scorer ( leaf ) ; for ( int parentDoc = parents . nextDoc ( ) ; parentDoc ! = DocIdSetIterator . NO_MORE_DOCS ; parentDoc = parents . nextDoc ( ) ) { int expectedChildDocs = leaf . reader ( ) . document ( parentDoc ) . getField ( <str> ) . numericValue ( ) . intValue ( ) ; hitContext . reset ( null , leaf , parentDoc , searcher ) ; NestedChildrenQuery nestedChildrenFilter = new NestedChildrenQuery ( parentFilter , childFilter , hitContext ) ; TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector ( ) ; searcher . search ( new ConstantScoreQuery ( nestedChildrenFilter ) , totalHitCountCollector ) ; assertThat ( totalHitCountCollector . getTotalHits ( ) , equalTo ( expectedChildDocs ) ) ; checkedParents + + ; } } assertThat ( checkedParents , equalTo ( numParentDocs ) ) ; reader . close ( ) ; dir . close ( ) ; } } 
