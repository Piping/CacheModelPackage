package org . apache . cassandra . batchlog ; import java . io . IOException ; import java . lang . management . ManagementFactory ; import java . net . InetAddress ; import java . nio . ByteBuffer ; import java . util . * ; import java . util . concurrent . * ; import javax . management . MBeanServer ; import javax . management . ObjectName ; import com . google . common . annotations . VisibleForTesting ; import com . google . common . collect . * ; import com . google . common . util . concurrent . RateLimiter ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . concurrent . DebuggableScheduledThreadPoolExecutor ; import org . apache . cassandra . config . DatabaseDescriptor ; import org . apache . cassandra . cql3 . UntypedResultSet ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . marshal . BytesType ; import org . apache . cassandra . db . marshal . UUIDType ; import org . apache . cassandra . db . partitions . PartitionUpdate ; import org . apache . cassandra . dht . Token ; import org . apache . cassandra . exceptions . WriteFailureException ; import org . apache . cassandra . exceptions . WriteTimeoutException ; import org . apache . cassandra . gms . FailureDetector ; import org . apache . cassandra . hints . Hint ; import org . apache . cassandra . hints . HintsService ; import org . apache . cassandra . io . util . DataInputBuffer ; import org . apache . cassandra . io . util . DataOutputBuffer ; import org . apache . cassandra . net . MessageIn ; import org . apache . cassandra . net . MessageOut ; import org . apache . cassandra . net . MessagingService ; import org . apache . cassandra . service . StorageService ; import org . apache . cassandra . service . WriteResponseHandler ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . UUIDGen ; import static com . google . common . collect . Iterables . transform ; import static org . apache . cassandra . cql3 . QueryProcessor . executeInternal ; import static org . apache . cassandra . cql3 . QueryProcessor . executeInternalWithPaging ; public class BatchlogManager implements BatchlogManagerMBean { public static final String MBEAN_NAME = <str> ; private static final long REPLAY_INTERVAL = <int> * <int> ; static final int DEFAULT_PAGE_SIZE = <int> ; private static final Logger logger = LoggerFactory . getLogger ( BatchlogManager . class ) ; public static final BatchlogManager instance = new BatchlogManager ( ) ; private volatile long totalBatchesReplayed = <int> ; private volatile UUID lastReplayedUuid = UUIDGen . minTimeUUID ( <int> ) ; private final ScheduledExecutorService batchlogTasks = new DebuggableScheduledThreadPoolExecutor ( <str> ) ; public void start ( ) { MBeanServer mbs = ManagementFactory . getPlatformMBeanServer ( ) ; try { mbs . registerMBean ( this , new ObjectName ( MBEAN_NAME ) ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } batchlogTasks . scheduleWithFixedDelay ( this : : replayFailedBatches , StorageService . RING_DELAY , REPLAY_INTERVAL , TimeUnit . MILLISECONDS ) ; } public void shutdown ( ) throws InterruptedException { batchlogTasks . shutdown ( ) ; batchlogTasks . awaitTermination ( <int> , TimeUnit . SECONDS ) ; } public static void remove ( UUID id ) { new Mutation ( PartitionUpdate . fullPartitionDelete ( SystemKeyspace . Batches , UUIDType . instance . decompose ( id ) , FBUtilities . timestampMicros ( ) , FBUtilities . nowInSeconds ( ) ) ) . apply ( ) ; } public static void store ( Batch batch ) { store ( batch , true ) ; } public static void store ( Batch batch , boolean durableWrites ) { RowUpdateBuilder builder = new RowUpdateBuilder ( SystemKeyspace . Batches , batch . creationTime , batch . id ) . clustering ( ) . add ( <str> , MessagingService . current_version ) ; for ( ByteBuffer mutation : batch . encodedMutations ) builder . addListEntry ( <str> , mutation ) ; for ( Mutation mutation : batch . decodedMutations ) { try ( DataOutputBuffer buffer = new DataOutputBuffer ( ) ) { Mutation . serializer . serialize ( mutation , buffer , MessagingService . current_version ) ; builder . addListEntry ( <str> , buffer . buffer ( ) ) ; } catch ( IOException e ) { throw new AssertionError ( e ) ; } } builder . build ( ) . apply ( durableWrites ) ; } @VisibleForTesting public int countAllBatches ( ) { String query = String . format ( <str> , SystemKeyspace . NAME , SystemKeyspace . BATCHES ) ; UntypedResultSet results = executeInternal ( query ) ; if ( results = = null | | results . isEmpty ( ) ) return <int> ; return ( int ) results . one ( ) . getLong ( <str> ) ; } public long getTotalBatchesReplayed ( ) { return totalBatchesReplayed ; } public void forceBatchlogReplay ( ) throws Exception { startBatchlogReplay ( ) . get ( ) ; } public Future < ? > startBatchlogReplay ( ) { return batchlogTasks . submit ( this : : replayFailedBatches ) ; } void performInitialReplay ( ) throws InterruptedException , ExecutionException { batchlogTasks . submit ( this : : replayFailedBatches ) . get ( ) ; } private void replayFailedBatches ( ) { logger . trace ( <str> ) ; int endpointsCount = StorageService . instance . getTokenMetadata ( ) . getAllEndpoints ( ) . size ( ) ; if ( endpointsCount < = <int> ) { logger . trace ( <str> ) ; return ; } int throttleInKB = DatabaseDescriptor . getBatchlogReplayThrottleInKB ( ) / endpointsCount ; RateLimiter rateLimiter = RateLimiter . create ( throttleInKB = = <int> ? Double . MAX_VALUE : throttleInKB * <int> ) ; UUID limitUuid = UUIDGen . maxTimeUUID ( System . currentTimeMillis ( ) - getBatchlogTimeout ( ) ) ; ColumnFamilyStore store = Keyspace . open ( SystemKeyspace . NAME ) . getColumnFamilyStore ( SystemKeyspace . BATCHES ) ; int pageSize = calculatePageSize ( store ) ; String query = String . format ( <str> , SystemKeyspace . NAME , SystemKeyspace . BATCHES ) ; UntypedResultSet batches = executeInternalWithPaging ( query , pageSize , lastReplayedUuid , limitUuid ) ; processBatchlogEntries ( batches , pageSize , rateLimiter ) ; lastReplayedUuid = limitUuid ; logger . trace ( <str> ) ; } static int calculatePageSize ( ColumnFamilyStore store ) { double averageRowSize = store . getMeanPartitionSize ( ) ; if ( averageRowSize < = <int> ) return DEFAULT_PAGE_SIZE ; return ( int ) Math . max ( <int> , Math . min ( DEFAULT_PAGE_SIZE , <int> * <int> * <int> / averageRowSize ) ) ; } private void processBatchlogEntries ( UntypedResultSet batches , int pageSize , RateLimiter rateLimiter ) { int positionInPage = <int> ; ArrayList < ReplayingBatch > unfinishedBatches = new ArrayList < > ( pageSize ) ; Set < InetAddress > hintedNodes = new HashSet < > ( ) ; Set < UUID > replayedBatches = new HashSet < > ( ) ; for ( UntypedResultSet . Row row : batches ) { UUID id = row . getUUID ( <str> ) ; int version = row . getInt ( <str> ) ; try { ReplayingBatch batch = new ReplayingBatch ( id , version , row . getList ( <str> , BytesType . instance ) ) ; if ( batch . replay ( rateLimiter , hintedNodes ) > <int> ) { unfinishedBatches . add ( batch ) ; } else { remove ( id ) ; + + totalBatchesReplayed ; } } catch ( IOException e ) { logger . warn ( <str> , id , e ) ; remove ( id ) ; } if ( + + positionInPage = = pageSize ) { finishAndClearBatches ( unfinishedBatches , hintedNodes , replayedBatches ) ; positionInPage = <int> ; } } finishAndClearBatches ( unfinishedBatches , hintedNodes , replayedBatches ) ; HintsService . instance . flushAndFsyncBlockingly ( transform ( hintedNodes , StorageService . instance : : getHostIdForEndpoint ) ) ; replayedBatches . forEach ( BatchlogManager : : remove ) ; } private void finishAndClearBatches ( ArrayList < ReplayingBatch > batches , Set < InetAddress > hintedNodes , Set < UUID > replayedBatches ) { for ( ReplayingBatch batch : batches ) { batch . finish ( hintedNodes ) ; replayedBatches . add ( batch . id ) ; } totalBatchesReplayed + = batches . size ( ) ; batches . clear ( ) ; } public static long getBatchlogTimeout ( ) { return DatabaseDescriptor . getWriteRpcTimeout ( ) * <int> ; } private static class ReplayingBatch { private final UUID id ; private final long writtenAt ; private final List < Mutation > mutations ; private final int replayedBytes ; private List < ReplayWriteResponseHandler < Mutation > > replayHandlers ; ReplayingBatch ( UUID id , int version , List < ByteBuffer > serializedMutations ) throws IOException { this . id = id ; this . writtenAt = UUIDGen . unixTimestamp ( id ) ; this . mutations = new ArrayList < > ( serializedMutations . size ( ) ) ; this . replayedBytes = addMutations ( version , serializedMutations ) ; } public int replay ( RateLimiter rateLimiter , Set < InetAddress > hintedNodes ) throws IOException { logger . trace ( <str> , id ) ; if ( mutations . isEmpty ( ) ) return <int> ; int gcgs = gcgs ( mutations ) ; if ( TimeUnit . MILLISECONDS . toSeconds ( writtenAt ) + gcgs < = FBUtilities . nowInSeconds ( ) ) return <int> ; replayHandlers = sendReplays ( mutations , writtenAt , hintedNodes ) ; rateLimiter . acquire ( replayedBytes ) ; return replayHandlers . size ( ) ; } public void finish ( Set < InetAddress > hintedNodes ) { for ( int i = <int> ; i < replayHandlers . size ( ) ; i + + ) { ReplayWriteResponseHandler < Mutation > handler = replayHandlers . get ( i ) ; try { handler . get ( ) ; } catch ( WriteTimeoutException | WriteFailureException e ) { logger . trace ( <str> ) ; logger . trace ( <str> , e . getMessage ( ) ) ; writeHintsForUndeliveredEndpoints ( i , hintedNodes ) ; return ; } } } private int addMutations ( int version , List < ByteBuffer > serializedMutations ) throws IOException { int ret = <int> ; for ( ByteBuffer serializedMutation : serializedMutations ) { ret + = serializedMutation . remaining ( ) ; try ( DataInputBuffer in = new DataInputBuffer ( serializedMutation , true ) ) { addMutation ( Mutation . serializer . deserialize ( in , version ) ) ; } } return ret ; } private void addMutation ( Mutation mutation ) { for ( UUID cfId : mutation . getColumnFamilyIds ( ) ) if ( writtenAt < = SystemKeyspace . getTruncatedAt ( cfId ) ) mutation = mutation . without ( cfId ) ; if ( ! mutation . isEmpty ( ) ) mutations . add ( mutation ) ; } private void writeHintsForUndeliveredEndpoints ( int startFrom , Set < InetAddress > hintedNodes ) { int gcgs = gcgs ( mutations ) ; if ( TimeUnit . MILLISECONDS . toSeconds ( writtenAt ) + gcgs < = FBUtilities . nowInSeconds ( ) ) return ; for ( int i = startFrom ; i < replayHandlers . size ( ) ; i + + ) { ReplayWriteResponseHandler < Mutation > handler = replayHandlers . get ( i ) ; Mutation undeliveredMutation = mutations . get ( i ) ; if ( handler ! = null ) { hintedNodes . addAll ( handler . undelivered ) ; HintsService . instance . write ( transform ( handler . undelivered , StorageService . instance : : getHostIdForEndpoint ) , Hint . create ( undeliveredMutation , writtenAt ) ) ; } } } private static List < ReplayWriteResponseHandler < Mutation > > sendReplays ( List < Mutation > mutations , long writtenAt , Set < InetAddress > hintedNodes ) { List < ReplayWriteResponseHandler < Mutation > > handlers = new ArrayList < > ( mutations . size ( ) ) ; for ( Mutation mutation : mutations ) { ReplayWriteResponseHandler < Mutation > handler = sendSingleReplayMutation ( mutation , writtenAt , hintedNodes ) ; if ( handler ! = null ) handlers . add ( handler ) ; } return handlers ; } private static ReplayWriteResponseHandler < Mutation > sendSingleReplayMutation ( final Mutation mutation , long writtenAt , Set < InetAddress > hintedNodes ) { Set < InetAddress > liveEndpoints = new HashSet < > ( ) ; String ks = mutation . getKeyspaceName ( ) ; Token tk = mutation . key ( ) . getToken ( ) ; for ( InetAddress endpoint : Iterables . concat ( StorageService . instance . getNaturalEndpoints ( ks , tk ) , StorageService . instance . getTokenMetadata ( ) . pendingEndpointsFor ( tk , ks ) ) ) { if ( endpoint . equals ( FBUtilities . getBroadcastAddress ( ) ) ) { mutation . apply ( ) ; } else if ( FailureDetector . instance . isAlive ( endpoint ) ) { liveEndpoints . add ( endpoint ) ; } else { hintedNodes . add ( endpoint ) ; HintsService . instance . write ( StorageService . instance . getHostIdForEndpoint ( endpoint ) , Hint . create ( mutation , writtenAt ) ) ; } } if ( liveEndpoints . isEmpty ( ) ) return null ; ReplayWriteResponseHandler < Mutation > handler = new ReplayWriteResponseHandler < > ( liveEndpoints ) ; MessageOut < Mutation > message = mutation . createMessage ( ) ; for ( InetAddress endpoint : liveEndpoints ) MessagingService . instance ( ) . sendRR ( message , endpoint , handler , false ) ; return handler ; } private static int gcgs ( Collection < Mutation > mutations ) { int gcgs = Integer . MAX_VALUE ; for ( Mutation mutation : mutations ) gcgs = Math . min ( gcgs , mutation . smallestGCGS ( ) ) ; return gcgs ; } private static class ReplayWriteResponseHandler < T > extends WriteResponseHandler < T > { private final Set < InetAddress > undelivered = Collections . newSetFromMap ( new ConcurrentHashMap < > ( ) ) ; ReplayWriteResponseHandler ( Collection < InetAddress > writeEndpoints ) { super ( writeEndpoints , Collections . < InetAddress > emptySet ( ) , null , null , null , WriteType . UNLOGGED_BATCH ) ; undelivered . addAll ( writeEndpoints ) ; } @Override protected int totalBlockFor ( ) { return this . naturalEndpoints . size ( ) ; } @Override public void response ( MessageIn < T > m ) { boolean removed = undelivered . remove ( m = = null ? FBUtilities . getBroadcastAddress ( ) : m . from ) ; assert removed ; super . response ( m ) ; } } } public static class EndpointFilter { private final String localRack ; private final Multimap < String , InetAddress > endpoints ; public EndpointFilter ( String localRack , Multimap < String , InetAddress > endpoints ) { this . localRack = localRack ; this . endpoints = endpoints ; } public Collection < InetAddress > filter ( ) { if ( endpoints . values ( ) . size ( ) = = <int> ) return endpoints . values ( ) ; ListMultimap < String , InetAddress > validated = ArrayListMultimap . create ( ) ; for ( Map . Entry < String , InetAddress > entry : endpoints . entries ( ) ) if ( isValid ( entry . getValue ( ) ) ) validated . put ( entry . getKey ( ) , entry . getValue ( ) ) ; if ( validated . size ( ) < = <int> ) return validated . values ( ) ; if ( validated . size ( ) - validated . get ( localRack ) . size ( ) > = <int> ) { validated . removeAll ( localRack ) ; } if ( validated . keySet ( ) . size ( ) = = <int> ) { Collection < InetAddress > otherRack = Iterables . getOnlyElement ( validated . asMap ( ) . values ( ) ) ; return Lists . newArrayList ( Iterables . limit ( otherRack , <int> ) ) ; } Collection < String > racks ; if ( validated . keySet ( ) . size ( ) = = <int> ) { racks = validated . keySet ( ) ; } else { racks = Lists . newArrayList ( validated . keySet ( ) ) ; Collections . shuffle ( ( List < String > ) racks ) ; } List < InetAddress > result = new ArrayList < > ( <int> ) ; for ( String rack : Iterables . limit ( racks , <int> ) ) { List < InetAddress > rackMembers = validated . get ( rack ) ; result . add ( rackMembers . get ( getRandomInt ( rackMembers . size ( ) ) ) ) ; } return result ; } @VisibleForTesting protected boolean isValid ( InetAddress input ) { return ! input . equals ( FBUtilities . getBroadcastAddress ( ) ) & & FailureDetector . instance . isAlive ( input ) ; } @VisibleForTesting protected int getRandomInt ( int bound ) { return ThreadLocalRandom . current ( ) . nextInt ( bound ) ; } } } 
