package org . apache . cassandra . index ; import java . lang . reflect . Constructor ; import java . util . * ; import java . util . concurrent . * ; import java . util . function . Function ; import java . util . stream . Collectors ; import java . util . stream . Stream ; import com . google . common . base . Joiner ; import com . google . common . base . Strings ; import com . google . common . collect . ImmutableSet ; import com . google . common . collect . Iterables ; import com . google . common . collect . Maps ; import com . google . common . collect . Sets ; import com . google . common . primitives . Longs ; import com . google . common . util . concurrent . Futures ; import com . google . common . util . concurrent . MoreExecutors ; import org . apache . commons . lang3 . StringUtils ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . concurrent . JMXEnabledThreadPoolExecutor ; import org . apache . cassandra . concurrent . NamedThreadFactory ; import org . apache . cassandra . concurrent . StageManager ; import org . apache . cassandra . config . ColumnDefinition ; import org . apache . cassandra . cql3 . statements . IndexTarget ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . compaction . CompactionManager ; import org . apache . cassandra . db . filter . RowFilter ; import org . apache . cassandra . db . lifecycle . SSTableSet ; import org . apache . cassandra . db . lifecycle . View ; import org . apache . cassandra . db . partitions . PartitionUpdate ; import org . apache . cassandra . db . rows . * ; import org . apache . cassandra . exceptions . InvalidRequestException ; import org . apache . cassandra . index . internal . CassandraIndex ; import org . apache . cassandra . index . transactions . * ; import org . apache . cassandra . io . sstable . format . SSTableReader ; import org . apache . cassandra . schema . IndexMetadata ; import org . apache . cassandra . schema . Indexes ; import org . apache . cassandra . tracing . Tracing ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . concurrent . OpOrder ; import org . apache . cassandra . utils . concurrent . Refs ; public class SecondaryIndexManager implements IndexRegistry { private static final Logger logger = LoggerFactory . getLogger ( SecondaryIndexManager . class ) ; private Map < String , Index > indexes = Maps . newConcurrentMap ( ) ; private Set < String > builtIndexes = Sets . newConcurrentHashSet ( ) ; private static final ExecutorService asyncExecutor = new JMXEnabledThreadPoolExecutor ( <int> , StageManager . KEEPALIVE , TimeUnit . SECONDS , new LinkedBlockingQueue < > ( ) , new NamedThreadFactory ( <str> ) , <str> ) ; private static final ExecutorService blockingExecutor = MoreExecutors . newDirectExecutorService ( ) ; public final ColumnFamilyStore baseCfs ; public SecondaryIndexManager ( ColumnFamilyStore baseCfs ) { this . baseCfs = baseCfs ; } public void reload ( ) { Indexes tableIndexes = baseCfs . metadata . getIndexes ( ) ; indexes . keySet ( ) . stream ( ) . filter ( indexName - > ! tableIndexes . has ( indexName ) ) . forEach ( this : : removeIndex ) ; for ( IndexMetadata tableIndex : tableIndexes ) addIndex ( tableIndex ) ; } private Future < ? > reloadIndex ( IndexMetadata indexDef ) { Index index = indexes . get ( indexDef . name ) ; Callable < ? > reloadTask = index . getMetadataReloadTask ( indexDef ) ; return reloadTask = = null ? Futures . immediateFuture ( null ) : blockingExecutor . submit ( reloadTask ) ; } private Future < ? > createIndex ( IndexMetadata indexDef ) { Index index = createInstance ( indexDef ) ; index . register ( this ) ; final Callable < ? > initialBuildTask = indexes . containsKey ( indexDef . name ) ? index . getInitializationTask ( ) : null ; if ( initialBuildTask = = null ) { markIndexBuilt ( indexDef . name ) ; return Futures . immediateFuture ( null ) ; } return asyncExecutor . submit ( index . getInitializationTask ( ) ) ; } public synchronized Future < ? > addIndex ( IndexMetadata indexDef ) { if ( indexes . containsKey ( indexDef . name ) ) return reloadIndex ( indexDef ) ; else return createIndex ( indexDef ) ; } public boolean isIndexQueryable ( Index index ) { return builtIndexes . contains ( index . getIndexMetadata ( ) . name ) ; } public synchronized void removeIndex ( String indexName ) { Index index = unregisterIndex ( indexName ) ; if ( null ! = index ) { markIndexRemoved ( indexName ) ; executeBlocking ( index . getInvalidateTask ( ) ) ; } } public Set < IndexMetadata > getDependentIndexes ( ColumnDefinition column ) { if ( indexes . isEmpty ( ) ) return Collections . emptySet ( ) ; Set < IndexMetadata > dependentIndexes = new HashSet < > ( ) ; for ( Index index : indexes . values ( ) ) if ( index . dependsOn ( column ) ) dependentIndexes . add ( index . getIndexMetadata ( ) ) ; return dependentIndexes ; } public void markAllIndexesRemoved ( ) { getBuiltIndexNames ( ) . forEach ( this : : markIndexRemoved ) ; } public void rebuildIndexesBlocking ( Collection < SSTableReader > sstables , Set < String > indexNames ) { Set < Index > toRebuild = indexes . values ( ) . stream ( ) . filter ( index - > indexNames . contains ( index . getIndexMetadata ( ) . name ) ) . filter ( Index : : shouldBuildBlocking ) . collect ( Collectors . toSet ( ) ) ; if ( toRebuild . isEmpty ( ) ) { logger . info ( <str> , Joiner . on ( <str> ) . join ( indexNames ) ) ; return ; } toRebuild . forEach ( indexer - > markIndexRemoved ( indexer . getIndexMetadata ( ) . name ) ) ; buildIndexesBlocking ( sstables , toRebuild ) ; toRebuild . forEach ( indexer - > markIndexBuilt ( indexer . getIndexMetadata ( ) . name ) ) ; } public void buildAllIndexesBlocking ( Collection < SSTableReader > sstables ) { buildIndexesBlocking ( sstables , indexes . values ( ) . stream ( ) . filter ( Index : : shouldBuildBlocking ) . collect ( Collectors . toSet ( ) ) ) ; } public void buildIndexBlocking ( Index index ) { if ( index . shouldBuildBlocking ( ) ) { try ( ColumnFamilyStore . RefViewFragment viewFragment = baseCfs . selectAndReference ( View . select ( SSTableSet . CANONICAL ) ) ; Refs < SSTableReader > sstables = viewFragment . refs ) { buildIndexesBlocking ( sstables , Collections . singleton ( index ) ) ; markIndexBuilt ( index . getIndexMetadata ( ) . name ) ; } } } public static boolean isIndexColumnFamilyStore ( ColumnFamilyStore cfs ) { return isIndexColumnFamily ( cfs . name ) ; } public static boolean isIndexColumnFamily ( String cfName ) { return cfName . contains ( Directories . SECONDARY_INDEX_NAME_SEPARATOR ) ; } public static ColumnFamilyStore getParentCfs ( ColumnFamilyStore cfs ) { String parentCfs = getParentCfsName ( cfs . name ) ; return cfs . keyspace . getColumnFamilyStore ( parentCfs ) ; } public static String getParentCfsName ( String cfName ) { assert isIndexColumnFamily ( cfName ) ; return StringUtils . substringBefore ( cfName , Directories . SECONDARY_INDEX_NAME_SEPARATOR ) ; } public static String getIndexName ( ColumnFamilyStore cfs ) { return getIndexName ( cfs . name ) ; } public static String getIndexName ( String cfName ) { assert isIndexColumnFamily ( cfName ) ; return StringUtils . substringAfter ( cfName , Directories . SECONDARY_INDEX_NAME_SEPARATOR ) ; } private void buildIndexesBlocking ( Collection < SSTableReader > sstables , Set < Index > indexes ) { if ( indexes . isEmpty ( ) ) return ; logger . info ( <str> , indexes . stream ( ) . map ( i - > i . getIndexMetadata ( ) . name ) . collect ( Collectors . joining ( <str> ) ) , sstables . stream ( ) . map ( SSTableReader : : toString ) . collect ( Collectors . joining ( <str> ) ) ) ; Map < Index . IndexBuildingSupport , Set < Index > > byType = new HashMap < > ( ) ; for ( Index index : indexes ) { Set < Index > stored = byType . computeIfAbsent ( index . getBuildTaskSupport ( ) , i - > new HashSet < > ( ) ) ; stored . add ( index ) ; } List < Future < ? > > futures = byType . entrySet ( ) . stream ( ) . map ( ( e ) - > e . getKey ( ) . getIndexBuildTask ( baseCfs , e . getValue ( ) , sstables ) ) . map ( CompactionManager . instance : : submitIndexBuild ) . collect ( Collectors . toList ( ) ) ; FBUtilities . waitOnFutures ( futures ) ; flushIndexesBlocking ( indexes ) ; logger . info ( <str> , indexes . stream ( ) . map ( i - > i . getIndexMetadata ( ) . name ) . collect ( Collectors . joining ( <str> ) ) ) ; } public void markIndexBuilt ( String indexName ) { builtIndexes . add ( indexName ) ; SystemKeyspace . setIndexBuilt ( baseCfs . keyspace . getName ( ) , indexName ) ; } public void markIndexRemoved ( String indexName ) { SystemKeyspace . setIndexRemoved ( baseCfs . keyspace . getName ( ) , indexName ) ; } public Index getIndexByName ( String indexName ) { return indexes . get ( indexName ) ; } private Index createInstance ( IndexMetadata indexDef ) { Index newIndex ; if ( indexDef . isCustom ( ) ) { assert indexDef . options ! = null ; String className = indexDef . options . get ( IndexTarget . CUSTOM_INDEX_OPTION_NAME ) ; assert ! Strings . isNullOrEmpty ( className ) ; try { Class < ? extends Index > indexClass = FBUtilities . classForName ( className , <str> ) ; Constructor < ? extends Index > ctor = indexClass . getConstructor ( ColumnFamilyStore . class , IndexMetadata . class ) ; newIndex = ( Index ) ctor . newInstance ( baseCfs , indexDef ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } } else { newIndex = CassandraIndex . newIndex ( baseCfs , indexDef ) ; } return newIndex ; } public void truncateAllIndexesBlocking ( final long truncatedAt ) { executeAllBlocking ( indexes . values ( ) . stream ( ) , ( index ) - > index . getTruncateTask ( truncatedAt ) ) ; } public void invalidateAllIndexesBlocking ( ) { markAllIndexesRemoved ( ) ; executeAllBlocking ( indexes . values ( ) . stream ( ) , Index : : getInvalidateTask ) ; } public void flushAllIndexesBlocking ( ) { flushIndexesBlocking ( ImmutableSet . copyOf ( indexes . values ( ) ) ) ; } public void flushIndexesBlocking ( Set < Index > indexes ) { if ( indexes . isEmpty ( ) ) return ; List < Future < ? > > wait = new ArrayList < > ( ) ; List < Index > nonCfsIndexes = new ArrayList < > ( ) ; synchronized ( baseCfs . getTracker ( ) ) { indexes . forEach ( index - > index . getBackingTable ( ) . map ( cfs - > wait . add ( cfs . forceFlush ( ) ) ) . orElseGet ( ( ) - > nonCfsIndexes . add ( index ) ) ) ; } executeAllBlocking ( nonCfsIndexes . stream ( ) , Index : : getBlockingFlushTask ) ; FBUtilities . waitOnFutures ( wait ) ; } public void flushAllNonCFSBackedIndexesBlocking ( ) { Set < Index > customIndexers = indexes . values ( ) . stream ( ) . filter ( index - > ! ( index . getBackingTable ( ) . isPresent ( ) ) ) . collect ( Collectors . toSet ( ) ) ; flushIndexesBlocking ( customIndexers ) ; } public List < String > getBuiltIndexNames ( ) { Set < String > allIndexNames = new HashSet < > ( ) ; indexes . values ( ) . stream ( ) . map ( i - > i . getIndexMetadata ( ) . name ) . forEach ( allIndexNames : : add ) ; return SystemKeyspace . getBuiltIndexes ( baseCfs . keyspace . getName ( ) , allIndexNames ) ; } public Set < ColumnFamilyStore > getAllIndexColumnFamilyStores ( ) { Set < ColumnFamilyStore > backingTables = new HashSet < > ( ) ; indexes . values ( ) . forEach ( index - > index . getBackingTable ( ) . ifPresent ( backingTables : : add ) ) ; return backingTables ; } public boolean hasIndexes ( ) { return ! indexes . isEmpty ( ) ; } public void indexPartition ( UnfilteredRowIterator partition , OpOrder . Group opGroup , Set < Index > indexes , int nowInSec ) { if ( ! indexes . isEmpty ( ) ) { DecoratedKey key = partition . partitionKey ( ) ; Set < Index . Indexer > indexers = indexes . stream ( ) . map ( index - > index . indexerFor ( key , partition . columns ( ) , nowInSec , opGroup , IndexTransaction . Type . UPDATE ) ) . filter ( Objects : : nonNull ) . collect ( Collectors . toSet ( ) ) ; indexers . forEach ( Index . Indexer : : begin ) ; try ( RowIterator filtered = UnfilteredRowIterators . filter ( partition , nowInSec ) ) { if ( ! filtered . staticRow ( ) . isEmpty ( ) ) indexers . forEach ( indexer - > indexer . insertRow ( filtered . staticRow ( ) ) ) ; while ( filtered . hasNext ( ) ) { Row row = filtered . next ( ) ; indexers . forEach ( indexer - > indexer . insertRow ( row ) ) ; } } indexers . forEach ( Index . Indexer : : finish ) ; } } public void deletePartition ( UnfilteredRowIterator partition , int nowInSec ) { CleanupTransaction indexTransaction = newCleanupTransaction ( partition . partitionKey ( ) , partition . columns ( ) , nowInSec ) ; indexTransaction . start ( ) ; indexTransaction . onPartitionDeletion ( new DeletionTime ( FBUtilities . timestampMicros ( ) , nowInSec ) ) ; indexTransaction . commit ( ) ; while ( partition . hasNext ( ) ) { Unfiltered unfiltered = partition . next ( ) ; if ( unfiltered . kind ( ) ! = Unfiltered . Kind . ROW ) continue ; indexTransaction = newCleanupTransaction ( partition . partitionKey ( ) , partition . columns ( ) , nowInSec ) ; indexTransaction . start ( ) ; indexTransaction . onRowDelete ( ( Row ) unfiltered ) ; indexTransaction . commit ( ) ; } } public Index getBestIndexFor ( ReadCommand command ) { if ( indexes . isEmpty ( ) | | command . rowFilter ( ) . isEmpty ( ) ) return null ; Set < Index > searchableIndexes = new HashSet < > ( ) ; for ( RowFilter . Expression expression : command . rowFilter ( ) ) { if ( expression . isCustom ( ) ) { RowFilter . CustomExpression customExpression = ( RowFilter . CustomExpression ) expression ; logger . trace ( <str> , customExpression . getTargetIndex ( ) . name ) ; Tracing . trace ( <str> , customExpression . getTargetIndex ( ) . name ) ; return indexes . get ( customExpression . getTargetIndex ( ) . name ) ; } else { indexes . values ( ) . stream ( ) . filter ( index - > index . supportsExpression ( expression . column ( ) , expression . operator ( ) ) ) . forEach ( searchableIndexes : : add ) ; } } if ( searchableIndexes . isEmpty ( ) ) { logger . trace ( <str> ) ; Tracing . trace ( <str> ) ; return null ; } Index selected = searchableIndexes . size ( ) = = <int> ? Iterables . getOnlyElement ( searchableIndexes ) : searchableIndexes . stream ( ) . min ( ( a , b ) - > Longs . compare ( a . getEstimatedResultRows ( ) , b . getEstimatedResultRows ( ) ) ) . orElseThrow ( ( ) - > new AssertionError ( <str> ) ) ; if ( Tracing . isTracing ( ) ) { Tracing . trace ( <str> , searchableIndexes . stream ( ) . map ( i - > i . getIndexMetadata ( ) . name + <str> + i . getEstimatedResultRows ( ) ) . collect ( Collectors . joining ( <str> ) ) , selected . getIndexMetadata ( ) . name ) ; } return selected ; } public void validate ( PartitionUpdate update ) throws InvalidRequestException { for ( Index index : indexes . values ( ) ) index . validate ( update ) ; } public void registerIndex ( Index index ) { String name = index . getIndexMetadata ( ) . name ; indexes . put ( name , index ) ; logger . trace ( <str> , name ) ; } public void unregisterIndex ( Index index ) { unregisterIndex ( index . getIndexMetadata ( ) . name ) ; } private Index unregisterIndex ( String name ) { Index removed = indexes . remove ( name ) ; builtIndexes . remove ( name ) ; logger . trace ( removed = = null ? <str> : <str> , name ) ; return removed ; } public Index getIndex ( IndexMetadata metadata ) { return indexes . get ( metadata . name ) ; } public Collection < Index > listIndexes ( ) { return ImmutableSet . copyOf ( indexes . values ( ) ) ; } public UpdateTransaction newUpdateTransaction ( PartitionUpdate update , OpOrder . Group opGroup , int nowInSec ) { if ( ! hasIndexes ( ) ) return UpdateTransaction . NO_OP ; Index . Indexer [ ] indexers = indexes . values ( ) . stream ( ) . map ( i - > i . indexerFor ( update . partitionKey ( ) , update . columns ( ) , nowInSec , opGroup , IndexTransaction . Type . UPDATE ) ) . filter ( Objects : : nonNull ) . toArray ( Index . Indexer [ ] : : new ) ; return indexers . length = = <int> ? UpdateTransaction . NO_OP : new WriteTimeTransaction ( indexers ) ; } public CompactionTransaction newCompactionTransaction ( DecoratedKey key , PartitionColumns partitionColumns , int versions , int nowInSec ) { return new IndexGCTransaction ( key , partitionColumns , versions , nowInSec , listIndexes ( ) ) ; } public CleanupTransaction newCleanupTransaction ( DecoratedKey key , PartitionColumns partitionColumns , int nowInSec ) { if ( ! hasIndexes ( ) ) return CleanupTransaction . NO_OP ; return new CleanupGCTransaction ( key , partitionColumns , nowInSec , listIndexes ( ) ) ; } private static final class WriteTimeTransaction implements UpdateTransaction { private final Index . Indexer [ ] indexers ; private WriteTimeTransaction ( Index . Indexer . . . indexers ) { for ( Index . Indexer indexer : indexers ) assert indexer ! = null ; this . indexers = indexers ; } public void start ( ) { for ( Index . Indexer indexer : indexers ) indexer . begin ( ) ; } public void onPartitionDeletion ( DeletionTime deletionTime ) { for ( Index . Indexer indexer : indexers ) indexer . partitionDelete ( deletionTime ) ; } public void onRangeTombstone ( RangeTombstone tombstone ) { for ( Index . Indexer indexer : indexers ) indexer . rangeTombstone ( tombstone ) ; } public void onInserted ( Row row ) { for ( Index . Indexer indexer : indexers ) indexer . insertRow ( row ) ; } public void onUpdated ( Row existing , Row updated ) { final Row . Builder toRemove = BTreeRow . sortedBuilder ( ) ; toRemove . newRow ( existing . clustering ( ) ) ; toRemove . addPrimaryKeyLivenessInfo ( existing . primaryKeyLivenessInfo ( ) ) ; toRemove . addRowDeletion ( existing . deletion ( ) ) ; final Row . Builder toInsert = BTreeRow . sortedBuilder ( ) ; toInsert . newRow ( updated . clustering ( ) ) ; toInsert . addPrimaryKeyLivenessInfo ( updated . primaryKeyLivenessInfo ( ) ) ; toInsert . addRowDeletion ( updated . deletion ( ) ) ; RowDiffListener diffListener = new RowDiffListener ( ) { public void onPrimaryKeyLivenessInfo ( int i , Clustering clustering , LivenessInfo merged , LivenessInfo original ) { } public void onDeletion ( int i , Clustering clustering , Row . Deletion merged , Row . Deletion original ) { } public void onComplexDeletion ( int i , Clustering clustering , ColumnDefinition column , DeletionTime merged , DeletionTime original ) { } public void onCell ( int i , Clustering clustering , Cell merged , Cell original ) { if ( merged ! = null & & ! merged . equals ( original ) ) toInsert . addCell ( merged ) ; if ( merged = = null | | ( original ! = null & & shouldCleanupOldValue ( original , merged ) ) ) toRemove . addCell ( original ) ; } } ; Rows . diff ( diffListener , updated , existing ) ; Row oldRow = toRemove . build ( ) ; Row newRow = toInsert . build ( ) ; for ( Index . Indexer indexer : indexers ) indexer . updateRow ( oldRow , newRow ) ; } public void commit ( ) { for ( Index . Indexer indexer : indexers ) indexer . finish ( ) ; } private boolean shouldCleanupOldValue ( Cell oldCell , Cell newCell ) { return ! oldCell . value ( ) . equals ( newCell . value ( ) ) | | oldCell . timestamp ( ) ! = newCell . timestamp ( ) ; } } private static final class IndexGCTransaction implements CompactionTransaction { private final DecoratedKey key ; private final PartitionColumns columns ; private final int versions ; private final int nowInSec ; private final Collection < Index > indexes ; private Row [ ] rows ; private IndexGCTransaction ( DecoratedKey key , PartitionColumns columns , int versions , int nowInSec , Collection < Index > indexes ) { this . key = key ; this . columns = columns ; this . versions = versions ; this . indexes = indexes ; this . nowInSec = nowInSec ; } public void start ( ) { if ( versions > <int> ) rows = new Row [ versions ] ; } public void onRowMerge ( Row merged , Row . . . versions ) { final Row . Builder [ ] builders = new Row . Builder [ versions . length ] ; RowDiffListener diffListener = new RowDiffListener ( ) { public void onPrimaryKeyLivenessInfo ( int i , Clustering clustering , LivenessInfo merged , LivenessInfo original ) { } public void onDeletion ( int i , Clustering clustering , Row . Deletion merged , Row . Deletion original ) { } public void onComplexDeletion ( int i , Clustering clustering , ColumnDefinition column , DeletionTime merged , DeletionTime original ) { } public void onCell ( int i , Clustering clustering , Cell merged , Cell original ) { if ( original ! = null & & merged = = null ) { if ( builders [ i ] = = null ) { builders [ i ] = BTreeRow . sortedBuilder ( ) ; builders [ i ] . newRow ( clustering ) ; } builders [ i ] . addCell ( original ) ; } } } ; Rows . diff ( diffListener , merged , versions ) ; for ( int i = <int> ; i < builders . length ; i + + ) if ( builders [ i ] ! = null ) rows [ i ] = builders [ i ] . build ( ) ; } public void commit ( ) { if ( rows = = null ) return ; try ( OpOrder . Group opGroup = Keyspace . writeOrder . start ( ) ) { for ( Index index : indexes ) { Index . Indexer indexer = index . indexerFor ( key , columns , nowInSec , opGroup , Type . COMPACTION ) ; if ( indexer = = null ) continue ; indexer . begin ( ) ; for ( Row row : rows ) if ( row ! = null ) indexer . removeRow ( row ) ; indexer . finish ( ) ; } } } } private static final class CleanupGCTransaction implements CleanupTransaction { private final DecoratedKey key ; private final PartitionColumns columns ; private final int nowInSec ; private final Collection < Index > indexes ; private Row row ; private DeletionTime partitionDelete ; private CleanupGCTransaction ( DecoratedKey key , PartitionColumns columns , int nowInSec , Collection < Index > indexes ) { this . key = key ; this . columns = columns ; this . indexes = indexes ; this . nowInSec = nowInSec ; } public void start ( ) { } public void onPartitionDeletion ( DeletionTime deletionTime ) { partitionDelete = deletionTime ; } public void onRowDelete ( Row row ) { this . row = row ; } public void commit ( ) { if ( row = = null & & partitionDelete = = null ) return ; try ( OpOrder . Group opGroup = Keyspace . writeOrder . start ( ) ) { for ( Index index : indexes ) { Index . Indexer indexer = index . indexerFor ( key , columns , nowInSec , opGroup , Type . CLEANUP ) ; if ( indexer = = null ) continue ; indexer . begin ( ) ; if ( partitionDelete ! = null ) indexer . partitionDelete ( partitionDelete ) ; if ( row ! = null ) indexer . removeRow ( row ) ; indexer . finish ( ) ; } } } } private static void executeBlocking ( Callable < ? > task ) { if ( null ! = task ) FBUtilities . waitOnFuture ( blockingExecutor . submit ( task ) ) ; } private static void executeAllBlocking ( Stream < Index > indexers , Function < Index , Callable < ? > > function ) { List < Future < ? > > waitFor = new ArrayList < > ( ) ; indexers . forEach ( indexer - > { Callable < ? > task = function . apply ( indexer ) ; if ( null ! = task ) waitFor . add ( blockingExecutor . submit ( task ) ) ; } ) ; FBUtilities . waitOnFutures ( waitFor ) ; } } 
