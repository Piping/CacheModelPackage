package org . elasticsearch . search . highlight ; import org . apache . lucene . analysis . Analyzer ; import org . apache . lucene . analysis . TokenStream ; import org . apache . lucene . analysis . tokenattributes . CharTermAttribute ; import org . apache . lucene . analysis . tokenattributes . OffsetAttribute ; import org . apache . lucene . search . highlight . Encoder ; import org . apache . lucene . search . highlight . Formatter ; import org . apache . lucene . search . highlight . Fragmenter ; import org . apache . lucene . search . highlight . NullFragmenter ; import org . apache . lucene . search . highlight . QueryScorer ; import org . apache . lucene . search . highlight . SimpleFragmenter ; import org . apache . lucene . search . highlight . SimpleHTMLFormatter ; import org . apache . lucene . search . highlight . SimpleSpanFragmenter ; import org . apache . lucene . search . highlight . TextFragment ; import org . apache . lucene . util . BytesRefHash ; import org . apache . lucene . util . CollectionUtil ; import org . apache . lucene . util . IOUtils ; import org . elasticsearch . ExceptionsHelper ; import org . elasticsearch . common . text . StringText ; import org . elasticsearch . common . text . Text ; import org . elasticsearch . index . mapper . FieldMapper ; import org . elasticsearch . search . fetch . FetchPhaseExecutionException ; import org . elasticsearch . search . fetch . FetchSubPhase ; import org . elasticsearch . search . internal . SearchContext ; import java . io . IOException ; import java . util . ArrayList ; import java . util . Comparator ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; public class PlainHighlighter implements Highlighter { private static final String CACHE_KEY = <str> ; @Override public HighlightField highlight ( HighlighterContext highlighterContext ) { SearchContextHighlight . Field field = highlighterContext . field ; SearchContext context = highlighterContext . context ; FetchSubPhase . HitContext hitContext = highlighterContext . hitContext ; FieldMapper mapper = highlighterContext . mapper ; Encoder encoder = field . fieldOptions ( ) . encoder ( ) . equals ( <str> ) ? HighlightUtils . Encoders . HTML : HighlightUtils . Encoders . DEFAULT ; if ( ! hitContext . cache ( ) . containsKey ( CACHE_KEY ) ) { Map < FieldMapper , org . apache . lucene . search . highlight . Highlighter > mappers = new HashMap < > ( ) ; hitContext . cache ( ) . put ( CACHE_KEY , mappers ) ; } @SuppressWarnings ( <str> ) Map < FieldMapper , org . apache . lucene . search . highlight . Highlighter > cache = ( Map < FieldMapper , org . apache . lucene . search . highlight . Highlighter > ) hitContext . cache ( ) . get ( CACHE_KEY ) ; org . apache . lucene . search . highlight . Highlighter entry = cache . get ( mapper ) ; if ( entry = = null ) { QueryScorer queryScorer = new CustomQueryScorer ( highlighterContext . query , field . fieldOptions ( ) . requireFieldMatch ( ) ? mapper . fieldType ( ) . names ( ) . indexName ( ) : null ) ; queryScorer . setExpandMultiTermQuery ( true ) ; Fragmenter fragmenter ; if ( field . fieldOptions ( ) . numberOfFragments ( ) = = <int> ) { fragmenter = new NullFragmenter ( ) ; } else if ( field . fieldOptions ( ) . fragmenter ( ) = = null ) { fragmenter = new SimpleSpanFragmenter ( queryScorer , field . fieldOptions ( ) . fragmentCharSize ( ) ) ; } else if ( <str> . equals ( field . fieldOptions ( ) . fragmenter ( ) ) ) { fragmenter = new SimpleFragmenter ( field . fieldOptions ( ) . fragmentCharSize ( ) ) ; } else if ( <str> . equals ( field . fieldOptions ( ) . fragmenter ( ) ) ) { fragmenter = new SimpleSpanFragmenter ( queryScorer , field . fieldOptions ( ) . fragmentCharSize ( ) ) ; } else { throw new IllegalArgumentException ( <str> + field . fieldOptions ( ) . fragmenter ( ) + <str> + highlighterContext . fieldName + <str> ) ; } Formatter formatter = new SimpleHTMLFormatter ( field . fieldOptions ( ) . preTags ( ) [ <int> ] , field . fieldOptions ( ) . postTags ( ) [ <int> ] ) ; entry = new org . apache . lucene . search . highlight . Highlighter ( formatter , encoder , queryScorer ) ; entry . setTextFragmenter ( fragmenter ) ; entry . setMaxDocCharsToAnalyze ( Integer . MAX_VALUE ) ; cache . put ( mapper , entry ) ; } int numberOfFragments = field . fieldOptions ( ) . numberOfFragments ( ) = = <int> ? <int> : field . fieldOptions ( ) . numberOfFragments ( ) ; ArrayList < TextFragment > fragsList = new ArrayList < > ( ) ; List < Object > textsToHighlight ; Analyzer analyzer = context . mapperService ( ) . documentMapper ( hitContext . hit ( ) . type ( ) ) . mappers ( ) . indexAnalyzer ( ) ; try { textsToHighlight = HighlightUtils . loadFieldValues ( field , mapper , context , hitContext ) ; for ( Object textToHighlight : textsToHighlight ) { String text = textToHighlight . toString ( ) ; try ( TokenStream tokenStream = analyzer . tokenStream ( mapper . fieldType ( ) . names ( ) . indexName ( ) , text ) ) { if ( ! tokenStream . hasAttribute ( CharTermAttribute . class ) | | ! tokenStream . hasAttribute ( OffsetAttribute . class ) ) { continue ; } TextFragment [ ] bestTextFragments = entry . getBestTextFragments ( tokenStream , text , false , numberOfFragments ) ; for ( TextFragment bestTextFragment : bestTextFragments ) { if ( bestTextFragment ! = null & & bestTextFragment . getScore ( ) > <int> ) { fragsList . add ( bestTextFragment ) ; } } } } } catch ( Exception e ) { if ( ExceptionsHelper . unwrap ( e , BytesRefHash . MaxBytesLengthExceededException . class ) ! = null ) { return null ; } else { throw new FetchPhaseExecutionException ( context , <str> + highlighterContext . fieldName + <str> , e ) ; } } if ( field . fieldOptions ( ) . scoreOrdered ( ) ) { CollectionUtil . introSort ( fragsList , new Comparator < TextFragment > ( ) { @Override public int compare ( TextFragment o1 , TextFragment o2 ) { return Math . round ( o2 . getScore ( ) - o1 . getScore ( ) ) ; } } ) ; } String [ ] fragments ; if ( field . fieldOptions ( ) . numberOfFragments ( ) = = <int> & & textsToHighlight . size ( ) > <int> & & fragsList . size ( ) > <int> ) { fragments = new String [ fragsList . size ( ) ] ; for ( int i = <int> ; i < fragsList . size ( ) ; i + + ) { fragments [ i ] = fragsList . get ( i ) . toString ( ) ; } } else { numberOfFragments = fragsList . size ( ) < numberOfFragments ? fragsList . size ( ) : numberOfFragments ; fragments = new String [ numberOfFragments ] ; for ( int i = <int> ; i < fragments . length ; i + + ) { fragments [ i ] = fragsList . get ( i ) . toString ( ) ; } } if ( fragments . length > <int> ) { return new HighlightField ( highlighterContext . fieldName , StringText . convertFromStringArray ( fragments ) ) ; } int noMatchSize = highlighterContext . field . fieldOptions ( ) . noMatchSize ( ) ; if ( noMatchSize > <int> & & textsToHighlight . size ( ) > <int> ) { String fieldContents = textsToHighlight . get ( <int> ) . toString ( ) ; int end ; try { end = findGoodEndForNoHighlightExcerpt ( noMatchSize , analyzer , mapper . fieldType ( ) . names ( ) . indexName ( ) , fieldContents ) ; } catch ( Exception e ) { throw new FetchPhaseExecutionException ( context , <str> + highlighterContext . fieldName + <str> , e ) ; } if ( end > <int> ) { return new HighlightField ( highlighterContext . fieldName , new Text [ ] { new StringText ( fieldContents . substring ( <int> , end ) ) } ) ; } } return null ; } @Override public boolean canHighlight ( FieldMapper fieldMapper ) { return true ; } private static int findGoodEndForNoHighlightExcerpt ( int noMatchSize , Analyzer analyzer , String fieldName , String contents ) throws IOException { try ( TokenStream tokenStream = analyzer . tokenStream ( fieldName , contents ) ) { if ( ! tokenStream . hasAttribute ( OffsetAttribute . class ) ) { return - <int> ; } int end = - <int> ; tokenStream . reset ( ) ; while ( tokenStream . incrementToken ( ) ) { OffsetAttribute attr = tokenStream . getAttribute ( OffsetAttribute . class ) ; if ( attr . endOffset ( ) > = noMatchSize ) { if ( attr . endOffset ( ) = = noMatchSize ) { end = noMatchSize ; } return end ; } end = attr . endOffset ( ) ; } tokenStream . end ( ) ; return end ; } } } 
