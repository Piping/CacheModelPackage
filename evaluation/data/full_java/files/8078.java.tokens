package org . elasticsearch . index . analysis ; import org . apache . lucene . analysis . Tokenizer ; import org . apache . lucene . analysis . core . WhitespaceTokenizer ; import org . elasticsearch . test . ESTokenStreamTestCase ; import java . io . IOException ; import java . io . StringReader ; import static org . elasticsearch . common . settings . Settings . settingsBuilder ; public class ASCIIFoldingTokenFilterFactoryTests extends ESTokenStreamTestCase { public void testDefault ( ) throws IOException { AnalysisService analysisService = AnalysisTestsHelper . createAnalysisServiceFromSettings ( settingsBuilder ( ) . put ( <str> , createTempDir ( ) . toString ( ) ) . put ( <str> , <str> ) . build ( ) ) ; TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } public void testPreserveOriginal ( ) throws IOException { AnalysisService analysisService = AnalysisTestsHelper . createAnalysisServiceFromSettings ( settingsBuilder ( ) . put ( <str> , createTempDir ( ) . toString ( ) ) . put ( <str> , <str> ) . put ( <str> , true ) . build ( ) ) ; TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> , <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } } 
