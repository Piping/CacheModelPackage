package org . nd4j . linalg . learning ; import lombok . Data ; import lombok . NoArgsConstructor ; import org . nd4j . linalg . api . ndarray . INDArray ; import org . nd4j . linalg . api . shape . Shape ; import org . nd4j . linalg . factory . Nd4j ; import java . io . Serializable ; import static org . nd4j . linalg . ops . transforms . Transforms . sqrt ; @Data @NoArgsConstructor public class AdaGrad implements Serializable , GradientUpdater { public INDArray historicalGradient ; public int [ ] shape ; protected double learningRate = <float> ; protected int numIterations = <int> ; private double epsilon = <float> ; public AdaGrad ( int rows , int cols , double learningRate ) { this . shape = new int [ ] { rows , cols } ; this . learningRate = learningRate ; } public AdaGrad ( int rows , int cols ) { this ( rows , cols , <float> ) ; } public AdaGrad ( int [ ] shape , double learningRate ) { this . shape = shape ; this . learningRate = learningRate ; } public AdaGrad ( double learningRate ) { this . learningRate = learningRate ; } @Override public void update ( Object . . . args ) { if ( args . length > <int> ) { learningRate = ( Double ) args [ <int> ] ; } } @Override public INDArray getGradient ( INDArray gradient , int iteration ) { if ( historicalGradient = = null ) historicalGradient = gradient . mul ( gradient ) ; else historicalGradient . addi ( gradient . mul ( gradient ) ) ; INDArray sqrtHistory = sqrt ( historicalGradient . add ( epsilon ) ) ; INDArray ret = sqrtHistory . rdivi ( learningRate ) . muli ( gradient ) ; numIterations + + ; return ret ; } public double getGradient ( double gradient , int column , int [ ] shape ) { boolean historicalInitialized = false ; if ( this . historicalGradient = = null ) { this . historicalGradient = Nd4j . ones ( shape ) ; historicalInitialized = true ; } double sqrtHistory = ! historicalInitialized ? Math . sqrt ( historicalGradient . getDouble ( column ) ) : historicalGradient . getDouble ( column ) ; double learningRates = learningRate / ( sqrtHistory + epsilon ) ; double adjustedGradient = gradient * ( learningRates ) ; historicalGradient . putScalar ( column , historicalGradient . getDouble ( column ) + gradient * gradient ) ; numIterations + + ; return adjustedGradient ; } public INDArray getGradient ( INDArray gradient , int slice , int [ ] shape ) { boolean historicalInitialized = false ; INDArray sqrtHistory ; if ( this . historicalGradient = = null ) { this . historicalGradient = Nd4j . ones ( shape ) ; historicalInitialized = true ; } else if ( ! this . historicalGradient . isVector ( ) & & this . historicalGradient . slice ( slice ) . length ( ) ! = gradient . length ( ) ) throw new IllegalArgumentException ( <str> ) ; if ( historicalGradient . isVector ( ) ) sqrtHistory = sqrt ( historicalGradient ) ; else sqrtHistory = ! historicalInitialized ? sqrt ( historicalGradient . slice ( slice ) ) : historicalGradient ; INDArray learningRates = sqrtHistory . add ( epsilon ) . rdivi ( learningRate ) ; if ( gradient . length ( ) ! = learningRates . length ( ) ) gradient . muli ( learningRates . slice ( slice ) ) ; else gradient . muli ( learningRates ) ; this . historicalGradient . slice ( slice ) . addi ( gradient . mul ( gradient ) ) ; numIterations + + ; return gradient ; } public AdaGrad createSubset ( int index ) { if ( historicalGradient = = null ) this . historicalGradient = Nd4j . ones ( shape ) ; if ( Shape . isMatrix ( shape ) ) { AdaGrad a = new AdaGrad ( <int> , historicalGradient . columns ( ) ) ; INDArray slice = historicalGradient . slice ( index ) . dup ( ) ; a . historicalGradient = slice ; a . setLearningRate ( learningRate ) ; return a ; } else { AdaGrad a = new AdaGrad ( <int> , <int> ) ; INDArray slice = Nd4j . scalar ( historicalGradient . getDouble ( index ) ) ; a . historicalGradient = slice ; a . setLearningRate ( learningRate ) ; return a ; } } @Override public GradientUpdaterAggregator getAggregator ( boolean addThis ) { AdaGradAggregator ag = new AdaGradAggregator ( ) ; if ( addThis ) ag . aggregate ( this ) ; return ag ; } public static class AdaGradAggregator implements GradientUpdaterAggregator { private INDArray historicalGradientSum ; private double lrSum ; private long numIterationsSum = <int> ; private int count = <int> ; @Override public GradientUpdater getUpdater ( ) { AdaGrad adaGrad = new AdaGrad ( lrSum / count ) ; adaGrad . setHistoricalGradient ( historicalGradientSum . div ( count ) ) ; adaGrad . setNumIterations ( ( int ) ( numIterationsSum / count ) ) ; return adaGrad ; } @Override public void aggregate ( GradientUpdater updater ) { if ( ! ( updater instanceof AdaGrad ) ) throw new UnsupportedOperationException ( <str> + updater ) ; AdaGrad adagrad = ( AdaGrad ) updater ; if ( historicalGradientSum = = null ) { historicalGradientSum = adagrad . historicalGradient . dup ( ) ; lrSum = adagrad . learningRate ; numIterationsSum = adagrad . numIterations ; } else { historicalGradientSum . addi ( adagrad . historicalGradient ) ; lrSum + = adagrad . learningRate ; numIterationsSum + = adagrad . numIterations ; } count + + ; } @Override public GradientUpdaterAggregator combine ( GradientUpdaterAggregator other ) { if ( ! ( other instanceof AdaGradAggregator ) ) throw new IllegalArgumentException ( <str> + other ) ; AdaGradAggregator aggregator = ( AdaGradAggregator ) other ; historicalGradientSum . addi ( aggregator . historicalGradientSum ) ; lrSum + = aggregator . lrSum ; numIterationsSum + = aggregator . numIterationsSum ; count + = aggregator . count ; return this ; } } } 
