package org . elasticsearch . search . dfs ; import com . carrotsearch . hppc . ObjectHashSet ; import com . carrotsearch . hppc . ObjectObjectHashMap ; import com . carrotsearch . hppc . cursors . ObjectCursor ; import org . apache . lucene . index . IndexReaderContext ; import org . apache . lucene . index . Term ; import org . apache . lucene . index . TermContext ; import org . apache . lucene . search . CollectionStatistics ; import org . apache . lucene . search . TermStatistics ; import org . elasticsearch . common . collect . HppcMaps ; import org . elasticsearch . search . SearchParseElement ; import org . elasticsearch . search . SearchPhase ; import org . elasticsearch . search . internal . SearchContext ; import org . elasticsearch . search . rescore . RescoreSearchContext ; import java . util . AbstractSet ; import java . util . Collection ; import java . util . Iterator ; import java . util . Map ; import static java . util . Collections . emptyMap ; public class DfsPhase implements SearchPhase { @Override public Map < String , ? extends SearchParseElement > parseElements ( ) { return emptyMap ( ) ; } @Override public void preProcess ( SearchContext context ) { } @Override public void execute ( SearchContext context ) { final ObjectHashSet < Term > termsSet = new ObjectHashSet < > ( ) ; try { context . searcher ( ) . createNormalizedWeight ( context . query ( ) , true ) . extractTerms ( new DelegateSet ( termsSet ) ) ; for ( RescoreSearchContext rescoreContext : context . rescore ( ) ) { rescoreContext . rescorer ( ) . extractTerms ( context , rescoreContext , new DelegateSet ( termsSet ) ) ; } Term [ ] terms = termsSet . toArray ( Term . class ) ; TermStatistics [ ] termStatistics = new TermStatistics [ terms . length ] ; IndexReaderContext indexReaderContext = context . searcher ( ) . getTopReaderContext ( ) ; for ( int i = <int> ; i < terms . length ; i + + ) { TermContext termContext = TermContext . build ( indexReaderContext , terms [ i ] ) ; termStatistics [ i ] = context . searcher ( ) . termStatistics ( terms [ i ] , termContext ) ; } ObjectObjectHashMap < String , CollectionStatistics > fieldStatistics = HppcMaps . newNoNullKeysMap ( ) ; for ( Term term : terms ) { assert term . field ( ) ! = null : <str> ; if ( ! fieldStatistics . containsKey ( term . field ( ) ) ) { final CollectionStatistics collectionStatistics = context . searcher ( ) . collectionStatistics ( term . field ( ) ) ; fieldStatistics . put ( term . field ( ) , collectionStatistics ) ; } } context . dfsResult ( ) . termsStatistics ( terms , termStatistics ) . fieldStatistics ( fieldStatistics ) . maxDoc ( context . searcher ( ) . getIndexReader ( ) . maxDoc ( ) ) ; } catch ( Exception e ) { throw new DfsPhaseExecutionException ( context , <str> , e ) ; } finally { termsSet . clear ( ) ; } } private static class DelegateSet extends AbstractSet < Term > { private final ObjectHashSet < Term > delegate ; private DelegateSet ( ObjectHashSet < Term > delegate ) { this . delegate = delegate ; } @Override public boolean add ( Term term ) { return delegate . add ( term ) ; } @Override public boolean addAll ( Collection < ? extends Term > terms ) { boolean result = false ; for ( Term term : terms ) { result = delegate . add ( term ) ; } return result ; } @Override public Iterator < Term > iterator ( ) { final Iterator < ObjectCursor < Term > > iterator = delegate . iterator ( ) ; return new Iterator < Term > ( ) { @Override public boolean hasNext ( ) { return iterator . hasNext ( ) ; } @Override public Term next ( ) { return iterator . next ( ) . value ; } @Override public void remove ( ) { throw new UnsupportedOperationException ( ) ; } } ; } @Override public int size ( ) { return delegate . size ( ) ; } } } 
