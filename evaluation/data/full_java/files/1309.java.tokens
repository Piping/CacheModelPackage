package org . apache . cassandra . db . commitlog ; import java . io . File ; import java . io . FileInputStream ; import java . io . IOException ; import java . nio . ByteBuffer ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Iterator ; import java . util . List ; import java . util . Map ; import java . util . Random ; import java . util . concurrent . Executors ; import java . util . concurrent . ScheduledExecutorService ; import java . util . concurrent . ThreadLocalRandom ; import java . util . concurrent . TimeUnit ; import java . util . concurrent . atomic . AtomicLong ; import junit . framework . Assert ; import com . google . common . util . concurrent . RateLimiter ; import org . junit . Before ; import org . junit . BeforeClass ; import org . junit . Test ; import org . apache . cassandra . SchemaLoader ; import org . apache . cassandra . Util ; import org . apache . cassandra . UpdateBuilder ; import org . apache . cassandra . config . Config . CommitLogSync ; import org . apache . cassandra . config . DatabaseDescriptor ; import org . apache . cassandra . config . ParameterizedClass ; import org . apache . cassandra . config . Schema ; import org . apache . cassandra . db . Mutation ; import org . apache . cassandra . db . rows . Cell ; import org . apache . cassandra . db . rows . Row ; import org . apache . cassandra . db . rows . SerializationHelper ; import org . apache . cassandra . db . partitions . PartitionUpdate ; import org . apache . cassandra . db . marshal . UTF8Type ; import org . apache . cassandra . io . util . DataInputBuffer ; import org . apache . cassandra . io . util . DataInputPlus ; public class CommitLogStressTest { public static ByteBuffer dataSource ; public static int NUM_THREADS = <int> * Runtime . getRuntime ( ) . availableProcessors ( ) - <int> ; public static int numCells = <int> ; public static int cellSize = <int> ; public static int rateLimit = <int> ; public static int runTimeMs = <int> ; public static String location = DatabaseDescriptor . getCommitLogLocation ( ) + <str> ; public static int hash ( int hash , ByteBuffer bytes ) { int shift = <int> ; for ( int i = <int> ; i < bytes . limit ( ) ; i + + ) { hash + = ( bytes . get ( i ) & <hex> ) < < shift ; shift = ( shift + <int> ) & <hex> ; } return hash ; } public static void main ( String [ ] args ) throws Exception { try { if ( args . length > = <int> ) { NUM_THREADS = Integer . parseInt ( args [ <int> ] ) ; System . out . println ( <str> + NUM_THREADS ) ; } if ( args . length > = <int> ) { numCells = Integer . parseInt ( args [ <int> ] ) ; System . out . println ( <str> + numCells ) ; } if ( args . length > = <int> ) { cellSize = Integer . parseInt ( args [ <int> ] ) ; System . out . println ( <str> + cellSize + <str> ) ; } if ( args . length > = <int> ) { rateLimit = Integer . parseInt ( args [ <int> ] ) ; System . out . println ( <str> + rateLimit ) ; } initialize ( ) ; CommitLogStressTest tester = new CommitLogStressTest ( ) ; tester . testFixedSize ( ) ; } catch ( Throwable e ) { e . printStackTrace ( System . err ) ; } finally { System . exit ( <int> ) ; } } boolean failed = false ; volatile boolean stop = false ; boolean randomSize = false ; boolean discardedRun = false ; ReplayPosition discardedPos ; @BeforeClass static public void initialize ( ) throws IOException { try ( FileInputStream fis = new FileInputStream ( <str> ) ) { dataSource = ByteBuffer . allocateDirect ( ( int ) fis . getChannel ( ) . size ( ) ) ; while ( dataSource . hasRemaining ( ) ) { fis . getChannel ( ) . read ( dataSource ) ; } dataSource . flip ( ) ; } SchemaLoader . loadSchema ( ) ; SchemaLoader . schemaDefinition ( <str> ) ; } @Before public void cleanDir ( ) { File dir = new File ( location ) ; if ( dir . isDirectory ( ) ) { File [ ] files = dir . listFiles ( ) ; for ( File f : files ) if ( ! f . delete ( ) ) Assert . fail ( <str> + f ) ; } else { dir . mkdir ( ) ; } } @Test public void testRandomSize ( ) throws Exception { randomSize = true ; discardedRun = false ; testAllLogConfigs ( ) ; } @Test public void testFixedSize ( ) throws Exception { randomSize = false ; discardedRun = false ; testAllLogConfigs ( ) ; } @Test public void testDiscardedRun ( ) throws Exception { discardedRun = true ; randomSize = true ; testAllLogConfigs ( ) ; } public void testAllLogConfigs ( ) throws IOException , InterruptedException { failed = false ; DatabaseDescriptor . setCommitLogSyncBatchWindow ( <int> ) ; DatabaseDescriptor . setCommitLogSyncPeriod ( <int> ) ; DatabaseDescriptor . setCommitLogSegmentSize ( <int> ) ; for ( ParameterizedClass compressor : new ParameterizedClass [ ] { null , new ParameterizedClass ( <str> , null ) , new ParameterizedClass ( <str> , null ) , new ParameterizedClass ( <str> , null ) } ) { DatabaseDescriptor . setCommitLogCompression ( compressor ) ; for ( CommitLogSync sync : CommitLogSync . values ( ) ) { DatabaseDescriptor . setCommitLogSync ( sync ) ; CommitLog commitLog = new CommitLog ( location , CommitLogArchiver . disabled ( ) ) . start ( ) ; testLog ( commitLog ) ; } } assert ! failed ; } public void testLog ( CommitLog commitLog ) throws IOException , InterruptedException { System . out . format ( <str> , mb ( DatabaseDescriptor . getCommitLogSegmentSize ( ) ) , commitLog . compressor ! = null ? commitLog . compressor . getClass ( ) . getSimpleName ( ) : <str> , commitLog . executor . getClass ( ) . getSimpleName ( ) , randomSize ? <str> : <str> , discardedRun ? <str> : <str> ) ; commitLog . allocator . enableReserveSegmentCreation ( ) ; final List < CommitlogExecutor > threads = new ArrayList < > ( ) ; ScheduledExecutorService scheduled = startThreads ( commitLog , threads ) ; discardedPos = ReplayPosition . NONE ; if ( discardedRun ) { Thread . sleep ( runTimeMs / <int> ) ; stop = true ; scheduled . shutdown ( ) ; scheduled . awaitTermination ( <int> , TimeUnit . SECONDS ) ; for ( CommitlogExecutor t : threads ) { t . join ( ) ; if ( t . rp . compareTo ( discardedPos ) > <int> ) discardedPos = t . rp ; } verifySizes ( commitLog ) ; commitLog . discardCompletedSegments ( Schema . instance . getCFMetaData ( <str> , <str> ) . cfId , discardedPos ) ; threads . clear ( ) ; System . out . format ( <str> , discardedPos ) ; verifySizes ( commitLog ) ; scheduled = startThreads ( commitLog , threads ) ; } Thread . sleep ( runTimeMs ) ; stop = true ; scheduled . shutdown ( ) ; scheduled . awaitTermination ( <int> , TimeUnit . SECONDS ) ; int hash = <int> ; int cells = <int> ; for ( CommitlogExecutor t : threads ) { t . join ( ) ; hash + = t . hash ; cells + = t . cells ; } verifySizes ( commitLog ) ; commitLog . shutdownBlocking ( ) ; System . out . print ( <str> ) ; System . out . flush ( ) ; Replayer repl = new Replayer ( commitLog ) ; File [ ] files = new File ( location ) . listFiles ( ) ; repl . recover ( files ) ; for ( File f : files ) if ( ! f . delete ( ) ) Assert . fail ( <str> + f ) ; if ( hash = = repl . hash & & cells = = repl . cells ) System . out . println ( <str> ) ; else { System . out . format ( <str> , repl . cells , cells , repl . hash , hash ) ; failed = true ; } } private void verifySizes ( CommitLog commitLog ) { commitLog . executor . requestExtraSync ( ) . awaitUninterruptibly ( ) ; commitLog . executor . requestExtraSync ( ) . awaitUninterruptibly ( ) ; commitLog . allocator . awaitManagementTasksCompletion ( ) ; long combinedSize = <int> ; for ( File f : new File ( commitLog . location ) . listFiles ( ) ) combinedSize + = f . length ( ) ; Assert . assertEquals ( combinedSize , commitLog . getActiveOnDiskSize ( ) ) ; List < String > logFileNames = commitLog . getActiveSegmentNames ( ) ; Map < String , Double > ratios = commitLog . getActiveSegmentCompressionRatios ( ) ; Collection < CommitLogSegment > segments = commitLog . allocator . getActiveSegments ( ) ; for ( CommitLogSegment segment : segments ) { Assert . assertTrue ( logFileNames . remove ( segment . getName ( ) ) ) ; Double ratio = ratios . remove ( segment . getName ( ) ) ; Assert . assertEquals ( segment . logFile . length ( ) , segment . onDiskSize ( ) ) ; Assert . assertEquals ( segment . onDiskSize ( ) * <float> / segment . contentSize ( ) , ratio , <float> ) ; } Assert . assertTrue ( logFileNames . isEmpty ( ) ) ; Assert . assertTrue ( ratios . isEmpty ( ) ) ; } public ScheduledExecutorService startThreads ( final CommitLog commitLog , final List < CommitlogExecutor > threads ) { stop = false ; for ( int ii = <int> ; ii < NUM_THREADS ; ii + + ) { final CommitlogExecutor t = new CommitlogExecutor ( commitLog , new Random ( ii ) ) ; threads . add ( t ) ; t . start ( ) ; } final long start = System . currentTimeMillis ( ) ; Runnable printRunnable = new Runnable ( ) { long lastUpdate = <int> ; public void run ( ) { Runtime runtime = Runtime . getRuntime ( ) ; long maxMemory = runtime . maxMemory ( ) ; long allocatedMemory = runtime . totalMemory ( ) ; long freeMemory = runtime . freeMemory ( ) ; long temp = <int> ; long sz = <int> ; for ( CommitlogExecutor cle : threads ) { temp + = cle . counter . get ( ) ; sz + = cle . dataSize ; } double time = ( System . currentTimeMillis ( ) - start ) / <float> ; double avg = ( temp / time ) ; System . out . println ( String . format ( <str> , ( ( System . currentTimeMillis ( ) - start ) / <int> ) , mb ( maxMemory ) , mb ( allocatedMemory ) , mb ( freeMemory ) , ( temp - lastUpdate ) , lastUpdate , avg , mb ( commitLog . getActiveContentSize ( ) ) , mb ( commitLog . getActiveOnDiskSize ( ) ) , mb ( sz / time ) ) ) ; lastUpdate = temp ; } } ; ScheduledExecutorService scheduled = Executors . newScheduledThreadPool ( <int> ) ; scheduled . scheduleAtFixedRate ( printRunnable , <int> , <int> , TimeUnit . SECONDS ) ; return scheduled ; } private static double mb ( long maxMemory ) { return maxMemory / ( <float> * <int> ) ; } private static double mb ( double maxMemory ) { return maxMemory / ( <int> * <int> ) ; } public static ByteBuffer randomBytes ( int quantity , Random tlr ) { ByteBuffer slice = ByteBuffer . allocate ( quantity ) ; ByteBuffer source = dataSource . duplicate ( ) ; source . position ( tlr . nextInt ( source . capacity ( ) - quantity ) ) ; source . limit ( source . position ( ) + quantity ) ; slice . put ( source ) ; slice . flip ( ) ; return slice ; } public class CommitlogExecutor extends Thread { final AtomicLong counter = new AtomicLong ( ) ; int hash = <int> ; int cells = <int> ; int dataSize = <int> ; final CommitLog commitLog ; final Random random ; volatile ReplayPosition rp ; public CommitlogExecutor ( CommitLog commitLog , Random rand ) { this . commitLog = commitLog ; this . random = rand ; } public void run ( ) { RateLimiter rl = rateLimit ! = <int> ? RateLimiter . create ( rateLimit ) : null ; final Random rand = random ! = null ? random : ThreadLocalRandom . current ( ) ; while ( ! stop ) { if ( rl ! = null ) rl . acquire ( ) ; ByteBuffer key = randomBytes ( <int> , rand ) ; UpdateBuilder builder = UpdateBuilder . create ( Schema . instance . getCFMetaData ( <str> , <str> ) , Util . dk ( key ) ) ; for ( int ii = <int> ; ii < numCells ; ii + + ) { int sz = randomSize ? rand . nextInt ( cellSize ) : cellSize ; ByteBuffer bytes = randomBytes ( sz , rand ) ; builder . newRow ( <str> + ii ) . add ( <str> , bytes ) ; hash = hash ( hash , bytes ) ; + + cells ; dataSize + = sz ; } rp = commitLog . add ( new Mutation ( builder . build ( ) ) ) ; counter . incrementAndGet ( ) ; } } } class Replayer extends CommitLogReplayer { Replayer ( CommitLog log ) { super ( log , discardedPos , null , ReplayFilter . create ( ) ) ; } int hash = <int> ; int cells = <int> ; @Override void replayMutation ( byte [ ] inputBuffer , int size , final long entryLocation , final CommitLogDescriptor desc ) { if ( desc . id < discardedPos . segment ) { System . out . format ( <str> , desc . id , entryLocation ) ; return ; } else if ( desc . id = = discardedPos . segment & & entryLocation < = discardedPos . position ) return ; DataInputPlus bufIn = new DataInputBuffer ( inputBuffer , <int> , size ) ; Mutation mutation ; try { mutation = Mutation . serializer . deserialize ( bufIn , desc . getMessagingVersion ( ) , SerializationHelper . Flag . LOCAL ) ; } catch ( IOException e ) { throw new AssertionError ( e ) ; } for ( PartitionUpdate cf : mutation . getPartitionUpdates ( ) ) { Iterator < Row > rowIterator = cf . iterator ( ) ; while ( rowIterator . hasNext ( ) ) { Row row = rowIterator . next ( ) ; if ( ! ( UTF8Type . instance . compose ( row . clustering ( ) . get ( <int> ) ) . startsWith ( <str> ) ) ) continue ; for ( Cell cell : row . cells ( ) ) { hash = hash ( hash , cell . value ( ) ) ; + + cells ; } } } } } } 
