package org . elasticsearch . index . snapshots . blobstore ; import org . apache . lucene . index . CorruptIndexException ; import org . apache . lucene . index . IndexCommit ; import org . apache . lucene . index . IndexFormatTooNewException ; import org . apache . lucene . index . IndexFormatTooOldException ; import org . apache . lucene . index . SegmentInfos ; import org . apache . lucene . store . IOContext ; import org . apache . lucene . store . IndexInput ; import org . apache . lucene . store . IndexOutput ; import org . apache . lucene . store . RateLimiter ; import org . apache . lucene . util . BytesRef ; import org . apache . lucene . util . BytesRefBuilder ; import org . apache . lucene . util . IOUtils ; import org . elasticsearch . ExceptionsHelper ; import org . elasticsearch . Version ; import org . elasticsearch . cluster . ClusterService ; import org . elasticsearch . cluster . metadata . SnapshotId ; import org . elasticsearch . cluster . node . DiscoveryNode ; import org . elasticsearch . common . ParseFieldMatcher ; import org . elasticsearch . common . blobstore . BlobContainer ; import org . elasticsearch . common . blobstore . BlobMetaData ; import org . elasticsearch . common . blobstore . BlobPath ; import org . elasticsearch . common . blobstore . BlobStore ; import org . elasticsearch . common . bytes . BytesArray ; import org . elasticsearch . common . collect . Tuple ; import org . elasticsearch . common . component . AbstractComponent ; import org . elasticsearch . common . inject . Inject ; import org . elasticsearch . common . lucene . Lucene ; import org . elasticsearch . common . lucene . store . InputStreamIndexInput ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . common . unit . ByteSizeValue ; import org . elasticsearch . common . util . iterable . Iterables ; import org . elasticsearch . index . IndexService ; import org . elasticsearch . index . shard . ShardId ; import org . elasticsearch . index . snapshots . IndexShardRepository ; import org . elasticsearch . index . snapshots . IndexShardRestoreFailedException ; import org . elasticsearch . index . snapshots . IndexShardSnapshotException ; import org . elasticsearch . index . snapshots . IndexShardSnapshotFailedException ; import org . elasticsearch . index . snapshots . IndexShardSnapshotStatus ; import org . elasticsearch . index . snapshots . blobstore . BlobStoreIndexShardSnapshot . FileInfo ; import org . elasticsearch . index . store . Store ; import org . elasticsearch . index . store . StoreFileMetaData ; import org . elasticsearch . indices . IndicesService ; import org . elasticsearch . indices . recovery . RecoveryState ; import org . elasticsearch . repositories . RepositoryName ; import org . elasticsearch . repositories . RepositoryVerificationException ; import org . elasticsearch . repositories . blobstore . BlobStoreFormat ; import org . elasticsearch . repositories . blobstore . BlobStoreRepository ; import org . elasticsearch . repositories . blobstore . ChecksumBlobStoreFormat ; import org . elasticsearch . repositories . blobstore . LegacyBlobStoreFormat ; import java . io . FilterInputStream ; import java . io . IOException ; import java . io . InputStream ; import java . util . ArrayList ; import java . util . Collection ; import java . util . Collections ; import java . util . HashMap ; import java . util . List ; import java . util . Map ; import static java . util . Collections . emptyMap ; import static java . util . Collections . unmodifiableMap ; import static org . elasticsearch . repositories . blobstore . BlobStoreRepository . testBlobPrefix ; public class BlobStoreIndexShardRepository extends AbstractComponent implements IndexShardRepository { private static final int BUFFER_SIZE = <int> ; private BlobStore blobStore ; private BlobPath basePath ; private final String repositoryName ; private ByteSizeValue chunkSize ; private final IndicesService indicesService ; private final ClusterService clusterService ; private RateLimiter snapshotRateLimiter ; private RateLimiter restoreRateLimiter ; private RateLimiterListener rateLimiterListener ; private RateLimitingInputStream . Listener snapshotThrottleListener ; private RateLimitingInputStream . Listener restoreThrottleListener ; private boolean compress ; private final ParseFieldMatcher parseFieldMatcher ; protected static final String LEGACY_SNAPSHOT_PREFIX = <str> ; protected static final String LEGACY_SNAPSHOT_NAME_FORMAT = LEGACY_SNAPSHOT_PREFIX + <str> ; protected static final String SNAPSHOT_PREFIX = <str> ; protected static final String SNAPSHOT_NAME_FORMAT = SNAPSHOT_PREFIX + <str> ; protected static final String SNAPSHOT_CODEC = <str> ; protected static final String SNAPSHOT_INDEX_PREFIX = <str> ; protected static final String SNAPSHOT_INDEX_NAME_FORMAT = SNAPSHOT_INDEX_PREFIX + <str> ; protected static final String SNAPSHOT_INDEX_CODEC = <str> ; protected static final String DATA_BLOB_PREFIX = <str> ; private ChecksumBlobStoreFormat < BlobStoreIndexShardSnapshot > indexShardSnapshotFormat ; private LegacyBlobStoreFormat < BlobStoreIndexShardSnapshot > indexShardSnapshotLegacyFormat ; private ChecksumBlobStoreFormat < BlobStoreIndexShardSnapshots > indexShardSnapshotsFormat ; @Inject public BlobStoreIndexShardRepository ( Settings settings , RepositoryName repositoryName , IndicesService indicesService , ClusterService clusterService ) { super ( settings ) ; this . parseFieldMatcher = new ParseFieldMatcher ( settings ) ; this . repositoryName = repositoryName . name ( ) ; this . indicesService = indicesService ; this . clusterService = clusterService ; } public void initialize ( BlobStore blobStore , BlobPath basePath , ByteSizeValue chunkSize , RateLimiter snapshotRateLimiter , RateLimiter restoreRateLimiter , final RateLimiterListener rateLimiterListener , boolean compress ) { this . blobStore = blobStore ; this . basePath = basePath ; this . chunkSize = chunkSize ; this . snapshotRateLimiter = snapshotRateLimiter ; this . restoreRateLimiter = restoreRateLimiter ; this . rateLimiterListener = rateLimiterListener ; this . snapshotThrottleListener = nanos - > rateLimiterListener . onSnapshotPause ( nanos ) ; this . restoreThrottleListener = nanos - > rateLimiterListener . onRestorePause ( nanos ) ; this . compress = compress ; indexShardSnapshotFormat = new ChecksumBlobStoreFormat < > ( SNAPSHOT_CODEC , SNAPSHOT_NAME_FORMAT , BlobStoreIndexShardSnapshot . PROTO , parseFieldMatcher , isCompress ( ) ) ; indexShardSnapshotLegacyFormat = new LegacyBlobStoreFormat < > ( LEGACY_SNAPSHOT_NAME_FORMAT , BlobStoreIndexShardSnapshot . PROTO , parseFieldMatcher ) ; indexShardSnapshotsFormat = new ChecksumBlobStoreFormat < > ( SNAPSHOT_INDEX_CODEC , SNAPSHOT_INDEX_NAME_FORMAT , BlobStoreIndexShardSnapshots . PROTO , parseFieldMatcher , isCompress ( ) ) ; } @Override public void snapshot ( SnapshotId snapshotId , ShardId shardId , IndexCommit snapshotIndexCommit , IndexShardSnapshotStatus snapshotStatus ) { SnapshotContext snapshotContext = new SnapshotContext ( snapshotId , shardId , snapshotStatus ) ; snapshotStatus . startTime ( System . currentTimeMillis ( ) ) ; try { snapshotContext . snapshot ( snapshotIndexCommit ) ; snapshotStatus . time ( System . currentTimeMillis ( ) - snapshotStatus . startTime ( ) ) ; snapshotStatus . updateStage ( IndexShardSnapshotStatus . Stage . DONE ) ; } catch ( Throwable e ) { snapshotStatus . time ( System . currentTimeMillis ( ) - snapshotStatus . startTime ( ) ) ; snapshotStatus . updateStage ( IndexShardSnapshotStatus . Stage . FAILURE ) ; snapshotStatus . failure ( ExceptionsHelper . detailedMessage ( e ) ) ; if ( e instanceof IndexShardSnapshotFailedException ) { throw ( IndexShardSnapshotFailedException ) e ; } else { throw new IndexShardSnapshotFailedException ( shardId , e ) ; } } } @Override public void restore ( SnapshotId snapshotId , Version version , ShardId shardId , ShardId snapshotShardId , RecoveryState recoveryState ) { final RestoreContext snapshotContext = new RestoreContext ( snapshotId , version , shardId , snapshotShardId , recoveryState ) ; try { snapshotContext . restore ( ) ; } catch ( Throwable e ) { throw new IndexShardRestoreFailedException ( shardId , <str> + snapshotId . getSnapshot ( ) + <str> , e ) ; } } @Override public IndexShardSnapshotStatus snapshotStatus ( SnapshotId snapshotId , Version version , ShardId shardId ) { Context context = new Context ( snapshotId , version , shardId ) ; BlobStoreIndexShardSnapshot snapshot = context . loadSnapshot ( ) ; IndexShardSnapshotStatus status = new IndexShardSnapshotStatus ( ) ; status . updateStage ( IndexShardSnapshotStatus . Stage . DONE ) ; status . startTime ( snapshot . startTime ( ) ) ; status . files ( snapshot . numberOfFiles ( ) , snapshot . totalSize ( ) ) ; status . processedFiles ( snapshot . numberOfFiles ( ) , snapshot . totalSize ( ) ) ; status . time ( snapshot . time ( ) ) ; return status ; } @Override public void verify ( String seed ) { BlobContainer testBlobContainer = blobStore . blobContainer ( basePath . add ( testBlobPrefix ( seed ) ) ) ; DiscoveryNode localNode = clusterService . localNode ( ) ; if ( testBlobContainer . blobExists ( <str> ) ) { try { testBlobContainer . writeBlob ( <str> + localNode . getId ( ) + <str> , new BytesArray ( seed ) ) ; } catch ( IOException exp ) { throw new RepositoryVerificationException ( repositoryName , <str> + blobStore + <str> + localNode + <str> , exp ) ; } } else { throw new RepositoryVerificationException ( repositoryName , <str> + blobStore + <str> + localNode + <str> + <str> + blobStore + <str> + <str> ) ; } } public void delete ( SnapshotId snapshotId , Version version , ShardId shardId ) { Context context = new Context ( snapshotId , version , shardId , shardId ) ; context . delete ( ) ; } @Override public String toString ( ) { return <str> + <str> + repositoryName + <str> + blobStore + <str> + <str> ; } protected boolean isCompress ( ) { return compress ; } BlobStoreFormat < BlobStoreIndexShardSnapshot > indexShardSnapshotFormat ( Version version ) { if ( BlobStoreRepository . legacyMetaData ( version ) ) { return indexShardSnapshotLegacyFormat ; } else { return indexShardSnapshotFormat ; } } private class Context { protected final SnapshotId snapshotId ; protected final ShardId shardId ; protected final BlobContainer blobContainer ; protected final Version version ; public Context ( SnapshotId snapshotId , Version version , ShardId shardId ) { this ( snapshotId , version , shardId , shardId ) ; } public Context ( SnapshotId snapshotId , Version version , ShardId shardId , ShardId snapshotShardId ) { this . snapshotId = snapshotId ; this . version = version ; this . shardId = shardId ; blobContainer = blobStore . blobContainer ( basePath . add ( <str> ) . add ( snapshotShardId . getIndex ( ) ) . add ( Integer . toString ( snapshotShardId . getId ( ) ) ) ) ; } public void delete ( ) { final Map < String , BlobMetaData > blobs ; try { blobs = blobContainer . listBlobs ( ) ; } catch ( IOException e ) { throw new IndexShardSnapshotException ( shardId , <str> , e ) ; } Tuple < BlobStoreIndexShardSnapshots , Integer > tuple = buildBlobStoreIndexShardSnapshots ( blobs ) ; BlobStoreIndexShardSnapshots snapshots = tuple . v1 ( ) ; int fileListGeneration = tuple . v2 ( ) ; try { indexShardSnapshotFormat ( version ) . delete ( blobContainer , snapshotId . getSnapshot ( ) ) ; } catch ( IOException e ) { logger . debug ( <str> , shardId , snapshotId ) ; } List < SnapshotFiles > newSnapshotsList = new ArrayList < > ( ) ; for ( SnapshotFiles point : snapshots ) { if ( ! point . snapshot ( ) . equals ( snapshotId . getSnapshot ( ) ) ) { newSnapshotsList . add ( point ) ; } } finalize ( newSnapshotsList , fileListGeneration + <int> , blobs ) ; } public BlobStoreIndexShardSnapshot loadSnapshot ( ) { try { return indexShardSnapshotFormat ( version ) . read ( blobContainer , snapshotId . getSnapshot ( ) ) ; } catch ( IOException ex ) { throw new IndexShardRestoreFailedException ( shardId , <str> , ex ) ; } } protected void finalize ( List < SnapshotFiles > snapshots , int fileListGeneration , Map < String , BlobMetaData > blobs ) { BlobStoreIndexShardSnapshots newSnapshots = new BlobStoreIndexShardSnapshots ( snapshots ) ; List < String > blobsToDelete = new ArrayList < > ( ) ; for ( String blobName : blobs . keySet ( ) ) { if ( indexShardSnapshotsFormat . isTempBlobName ( blobName ) | | blobName . startsWith ( SNAPSHOT_INDEX_PREFIX ) ) { blobsToDelete . add ( blobName ) ; } } try { blobContainer . deleteBlobs ( blobsToDelete ) ; } catch ( IOException e ) { throw new IndexShardSnapshotFailedException ( shardId , <str> , e ) ; } blobsToDelete = new ArrayList < > ( ) ; for ( String blobName : blobs . keySet ( ) ) { if ( blobName . startsWith ( DATA_BLOB_PREFIX ) ) { if ( newSnapshots . findNameFile ( FileInfo . canonicalName ( blobName ) ) = = null ) { blobsToDelete . add ( blobName ) ; } } } try { blobContainer . deleteBlobs ( blobsToDelete ) ; } catch ( IOException e ) { logger . debug ( <str> , e , snapshotId , shardId , blobsToDelete ) ; } if ( snapshots . size ( ) > <int> ) { try { indexShardSnapshotsFormat . writeAtomic ( newSnapshots , blobContainer , Integer . toString ( fileListGeneration ) ) ; } catch ( IOException e ) { throw new IndexShardSnapshotFailedException ( shardId , <str> , e ) ; } } } protected String fileNameFromGeneration ( long generation ) { return DATA_BLOB_PREFIX + Long . toString ( generation , Character . MAX_RADIX ) ; } protected long findLatestFileNameGeneration ( Map < String , BlobMetaData > blobs ) { long generation = - <int> ; for ( String name : blobs . keySet ( ) ) { if ( ! name . startsWith ( DATA_BLOB_PREFIX ) ) { continue ; } name = FileInfo . canonicalName ( name ) ; try { long currentGen = Long . parseLong ( name . substring ( DATA_BLOB_PREFIX . length ( ) ) , Character . MAX_RADIX ) ; if ( currentGen > generation ) { generation = currentGen ; } } catch ( NumberFormatException e ) { logger . warn ( <str> , name , DATA_BLOB_PREFIX ) ; } } return generation ; } protected Tuple < BlobStoreIndexShardSnapshots , Integer > buildBlobStoreIndexShardSnapshots ( Map < String , BlobMetaData > blobs ) { int latest = - <int> ; for ( String name : blobs . keySet ( ) ) { if ( name . startsWith ( SNAPSHOT_INDEX_PREFIX ) ) { try { int gen = Integer . parseInt ( name . substring ( SNAPSHOT_INDEX_PREFIX . length ( ) ) ) ; if ( gen > latest ) { latest = gen ; } } catch ( NumberFormatException ex ) { logger . warn ( <str> , name ) ; } } } if ( latest > = <int> ) { try { return new Tuple < > ( indexShardSnapshotsFormat . read ( blobContainer , Integer . toString ( latest ) ) , latest ) ; } catch ( IOException e ) { logger . warn ( <str> , e , SNAPSHOT_INDEX_PREFIX + latest ) ; } } List < SnapshotFiles > snapshots = new ArrayList < > ( ) ; for ( String name : blobs . keySet ( ) ) { try { BlobStoreIndexShardSnapshot snapshot = null ; if ( name . startsWith ( SNAPSHOT_PREFIX ) ) { snapshot = indexShardSnapshotFormat . readBlob ( blobContainer , name ) ; } else if ( name . startsWith ( LEGACY_SNAPSHOT_PREFIX ) ) { snapshot = indexShardSnapshotLegacyFormat . readBlob ( blobContainer , name ) ; } if ( snapshot ! = null ) { snapshots . add ( new SnapshotFiles ( snapshot . snapshot ( ) , snapshot . indexFiles ( ) ) ) ; } } catch ( IOException e ) { logger . warn ( <str> , e , name ) ; } } return new Tuple < > ( new BlobStoreIndexShardSnapshots ( snapshots ) , - <int> ) ; } } private class SnapshotContext extends Context { private final Store store ; private final IndexShardSnapshotStatus snapshotStatus ; public SnapshotContext ( SnapshotId snapshotId , ShardId shardId , IndexShardSnapshotStatus snapshotStatus ) { super ( snapshotId , Version . CURRENT , shardId ) ; IndexService indexService = indicesService . indexServiceSafe ( shardId . getIndex ( ) ) ; store = indexService . getShardOrNull ( shardId . id ( ) ) . store ( ) ; this . snapshotStatus = snapshotStatus ; } public void snapshot ( IndexCommit snapshotIndexCommit ) { logger . debug ( <str> , shardId , snapshotId , repositoryName ) ; store . incRef ( ) ; try { final Map < String , BlobMetaData > blobs ; try { blobs = blobContainer . listBlobs ( ) ; } catch ( IOException e ) { throw new IndexShardSnapshotFailedException ( shardId , <str> , e ) ; } long generation = findLatestFileNameGeneration ( blobs ) ; Tuple < BlobStoreIndexShardSnapshots , Integer > tuple = buildBlobStoreIndexShardSnapshots ( blobs ) ; BlobStoreIndexShardSnapshots snapshots = tuple . v1 ( ) ; int fileListGeneration = tuple . v2 ( ) ; final List < BlobStoreIndexShardSnapshot . FileInfo > indexCommitPointFiles = new ArrayList < > ( ) ; int indexNumberOfFiles = <int> ; long indexTotalFilesSize = <int> ; ArrayList < FileInfo > filesToSnapshot = new ArrayList < > ( ) ; final Store . MetadataSnapshot metadata ; final Collection < String > fileNames ; try { metadata = store . getMetadata ( snapshotIndexCommit ) ; fileNames = snapshotIndexCommit . getFileNames ( ) ; } catch ( IOException e ) { throw new IndexShardSnapshotFailedException ( shardId , <str> , e ) ; } for ( String fileName : fileNames ) { if ( snapshotStatus . aborted ( ) ) { logger . debug ( <str> , shardId , snapshotId , fileName ) ; throw new IndexShardSnapshotFailedException ( shardId , <str> ) ; } logger . trace ( <str> , shardId , snapshotId , fileName ) ; final StoreFileMetaData md = metadata . get ( fileName ) ; FileInfo existingFileInfo = null ; List < FileInfo > filesInfo = snapshots . findPhysicalIndexFiles ( fileName ) ; if ( filesInfo ! = null ) { for ( FileInfo fileInfo : filesInfo ) { try { maybeRecalculateMetadataHash ( blobContainer , fileInfo , metadata ) ; } catch ( Throwable e ) { logger . warn ( <str> , e , shardId , fileInfo . physicalName ( ) , fileInfo . metadata ( ) ) ; } if ( fileInfo . isSame ( md ) & & snapshotFileExistsInBlobs ( fileInfo , blobs ) ) { existingFileInfo = fileInfo ; break ; } } } if ( existingFileInfo = = null ) { indexNumberOfFiles + + ; indexTotalFilesSize + = md . length ( ) ; BlobStoreIndexShardSnapshot . FileInfo snapshotFileInfo = new BlobStoreIndexShardSnapshot . FileInfo ( fileNameFromGeneration ( + + generation ) , md , chunkSize ) ; indexCommitPointFiles . add ( snapshotFileInfo ) ; filesToSnapshot . add ( snapshotFileInfo ) ; } else { indexCommitPointFiles . add ( existingFileInfo ) ; } } snapshotStatus . files ( indexNumberOfFiles , indexTotalFilesSize ) ; if ( snapshotStatus . aborted ( ) ) { logger . debug ( <str> , shardId , snapshotId ) ; throw new IndexShardSnapshotFailedException ( shardId , <str> ) ; } snapshotStatus . updateStage ( IndexShardSnapshotStatus . Stage . STARTED ) ; for ( FileInfo snapshotFileInfo : filesToSnapshot ) { try { snapshotFile ( snapshotFileInfo ) ; } catch ( IOException e ) { throw new IndexShardSnapshotFailedException ( shardId , <str> , e ) ; } } snapshotStatus . indexVersion ( snapshotIndexCommit . getGeneration ( ) ) ; snapshotStatus . updateStage ( IndexShardSnapshotStatus . Stage . FINALIZE ) ; BlobStoreIndexShardSnapshot snapshot = new BlobStoreIndexShardSnapshot ( snapshotId . getSnapshot ( ) , snapshotIndexCommit . getGeneration ( ) , indexCommitPointFiles , snapshotStatus . startTime ( ) , System . currentTimeMillis ( ) - snapshotStatus . startTime ( ) , indexNumberOfFiles , indexTotalFilesSize ) ; logger . trace ( <str> , shardId , snapshotId ) ; try { indexShardSnapshotFormat . write ( snapshot , blobContainer , snapshotId . getSnapshot ( ) ) ; } catch ( IOException e ) { throw new IndexShardSnapshotFailedException ( shardId , <str> , e ) ; } List < SnapshotFiles > newSnapshotsList = new ArrayList < > ( ) ; newSnapshotsList . add ( new SnapshotFiles ( snapshot . snapshot ( ) , snapshot . indexFiles ( ) ) ) ; for ( SnapshotFiles point : snapshots ) { newSnapshotsList . add ( point ) ; } finalize ( newSnapshotsList , fileListGeneration + <int> , blobs ) ; snapshotStatus . updateStage ( IndexShardSnapshotStatus . Stage . DONE ) ; } finally { store . decRef ( ) ; } } private void snapshotFile ( final BlobStoreIndexShardSnapshot . FileInfo fileInfo ) throws IOException { final String file = fileInfo . physicalName ( ) ; try ( IndexInput indexInput = store . openVerifyingInput ( file , IOContext . READONCE , fileInfo . metadata ( ) ) ) { for ( int i = <int> ; i < fileInfo . numberOfParts ( ) ; i + + ) { final long partBytes = fileInfo . partBytes ( i ) ; final InputStreamIndexInput inputStreamIndexInput = new InputStreamIndexInput ( indexInput , partBytes ) ; InputStream inputStream = snapshotRateLimiter = = null ? inputStreamIndexInput : new RateLimitingInputStream ( inputStreamIndexInput , snapshotRateLimiter , snapshotThrottleListener ) ; inputStream = new AbortableInputStream ( inputStream , fileInfo . physicalName ( ) ) ; blobContainer . writeBlob ( fileInfo . partName ( i ) , inputStream , partBytes ) ; } Store . verify ( indexInput ) ; snapshotStatus . addProcessedFile ( fileInfo . length ( ) ) ; } catch ( Throwable t ) { failStoreIfCorrupted ( t ) ; snapshotStatus . addProcessedFile ( <int> ) ; throw t ; } } private void failStoreIfCorrupted ( Throwable t ) { if ( t instanceof CorruptIndexException | | t instanceof IndexFormatTooOldException | | t instanceof IndexFormatTooNewException ) { try { store . markStoreCorrupted ( ( IOException ) t ) ; } catch ( IOException e ) { logger . warn ( <str> , e ) ; } } } private boolean snapshotFileExistsInBlobs ( BlobStoreIndexShardSnapshot . FileInfo fileInfo , Map < String , BlobMetaData > blobs ) { BlobMetaData blobMetaData = blobs . get ( fileInfo . name ( ) ) ; if ( blobMetaData ! = null ) { return blobMetaData . length ( ) = = fileInfo . length ( ) ; } else if ( blobs . containsKey ( fileInfo . partName ( <int> ) ) ) { int part = <int> ; long totalSize = <int> ; while ( true ) { blobMetaData = blobs . get ( fileInfo . partName ( part + + ) ) ; if ( blobMetaData = = null ) { break ; } totalSize + = blobMetaData . length ( ) ; } return totalSize = = fileInfo . length ( ) ; } return false ; } private class AbortableInputStream extends FilterInputStream { private final String fileName ; public AbortableInputStream ( InputStream delegate , String fileName ) { super ( delegate ) ; this . fileName = fileName ; } @Override public int read ( ) throws IOException { checkAborted ( ) ; return in . read ( ) ; } @Override public int read ( byte [ ] b , int off , int len ) throws IOException { checkAborted ( ) ; return in . read ( b , off , len ) ; } private void checkAborted ( ) { if ( snapshotStatus . aborted ( ) ) { logger . debug ( <str> , shardId , snapshotId , fileName ) ; throw new IndexShardSnapshotFailedException ( shardId , <str> ) ; } } } } private static void maybeRecalculateMetadataHash ( final BlobContainer blobContainer , final FileInfo fileInfo , Store . MetadataSnapshot snapshot ) throws Throwable { final StoreFileMetaData metadata ; if ( fileInfo ! = null & & ( metadata = snapshot . get ( fileInfo . physicalName ( ) ) ) ! = null ) { if ( metadata . hash ( ) . length > <int> & & fileInfo . metadata ( ) . hash ( ) . length = = <int> ) { try ( final InputStream stream = new PartSliceStream ( blobContainer , fileInfo ) ) { BytesRefBuilder builder = new BytesRefBuilder ( ) ; Store . MetadataSnapshot . hashFile ( builder , stream , fileInfo . length ( ) ) ; BytesRef hash = fileInfo . metadata ( ) . hash ( ) ; assert hash . length = = <int> ; hash . bytes = builder . bytes ( ) ; hash . offset = <int> ; hash . length = builder . length ( ) ; } } } } private static final class PartSliceStream extends SlicedInputStream { private final BlobContainer container ; private final FileInfo info ; public PartSliceStream ( BlobContainer container , FileInfo info ) { super ( info . numberOfParts ( ) ) ; this . info = info ; this . container = container ; } @Override protected InputStream openSlice ( long slice ) throws IOException { return container . readBlob ( info . partName ( slice ) ) ; } } private class RestoreContext extends Context { private final Store store ; private final RecoveryState recoveryState ; public RestoreContext ( SnapshotId snapshotId , Version version , ShardId shardId , ShardId snapshotShardId , RecoveryState recoveryState ) { super ( snapshotId , version , shardId , snapshotShardId ) ; store = indicesService . indexServiceSafe ( shardId . getIndex ( ) ) . getShardOrNull ( shardId . id ( ) ) . store ( ) ; this . recoveryState = recoveryState ; } public void restore ( ) throws IOException { store . incRef ( ) ; try { logger . debug ( <str> , snapshotId , repositoryName , shardId ) ; BlobStoreIndexShardSnapshot snapshot = loadSnapshot ( ) ; SnapshotFiles snapshotFiles = new SnapshotFiles ( snapshot . snapshot ( ) , snapshot . indexFiles ( ) ) ; final Store . MetadataSnapshot recoveryTargetMetadata ; try { recoveryTargetMetadata = store . getMetadataOrEmpty ( ) ; } catch ( CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException e ) { logger . warn ( <str> , e , shardId ) ; throw new IndexShardRestoreFailedException ( shardId , <str> , e ) ; } final List < FileInfo > filesToRecover = new ArrayList < > ( ) ; final Map < String , StoreFileMetaData > snapshotMetaData = new HashMap < > ( ) ; final Map < String , FileInfo > fileInfos = new HashMap < > ( ) ; for ( final FileInfo fileInfo : snapshot . indexFiles ( ) ) { try { maybeRecalculateMetadataHash ( blobContainer , fileInfo , recoveryTargetMetadata ) ; } catch ( Throwable e ) { logger . warn ( <str> , e , shardId , fileInfo . physicalName ( ) , fileInfo . metadata ( ) ) ; } snapshotMetaData . put ( fileInfo . metadata ( ) . name ( ) , fileInfo . metadata ( ) ) ; fileInfos . put ( fileInfo . metadata ( ) . name ( ) , fileInfo ) ; } final Store . MetadataSnapshot sourceMetaData = new Store . MetadataSnapshot ( unmodifiableMap ( snapshotMetaData ) , emptyMap ( ) , <int> ) ; final Store . RecoveryDiff diff = sourceMetaData . recoveryDiff ( recoveryTargetMetadata ) ; for ( StoreFileMetaData md : diff . identical ) { FileInfo fileInfo = fileInfos . get ( md . name ( ) ) ; recoveryState . getIndex ( ) . addFileDetail ( fileInfo . name ( ) , fileInfo . length ( ) , true ) ; if ( logger . isTraceEnabled ( ) ) { logger . trace ( <str> , shardId , snapshotId , fileInfo . physicalName ( ) , fileInfo . name ( ) ) ; } } for ( StoreFileMetaData md : Iterables . concat ( diff . different , diff . missing ) ) { FileInfo fileInfo = fileInfos . get ( md . name ( ) ) ; filesToRecover . add ( fileInfo ) ; recoveryState . getIndex ( ) . addFileDetail ( fileInfo . name ( ) , fileInfo . length ( ) , false ) ; if ( logger . isTraceEnabled ( ) ) { if ( md = = null ) { logger . trace ( <str> , shardId , snapshotId , fileInfo . physicalName ( ) , fileInfo . name ( ) ) ; } else { logger . trace ( <str> , shardId , snapshotId , fileInfo . physicalName ( ) , fileInfo . name ( ) ) ; } } } final RecoveryState . Index index = recoveryState . getIndex ( ) ; if ( filesToRecover . isEmpty ( ) ) { logger . trace ( <str> ) ; } if ( logger . isTraceEnabled ( ) ) { logger . trace ( <str> , shardId , snapshotId , index . totalRecoverFiles ( ) , new ByteSizeValue ( index . totalRecoverBytes ( ) ) , index . reusedFileCount ( ) , new ByteSizeValue ( index . reusedFileCount ( ) ) ) ; } try { for ( final FileInfo fileToRecover : filesToRecover ) { logger . trace ( <str> , shardId , snapshotId , fileToRecover . name ( ) ) ; restoreFile ( fileToRecover ) ; } } catch ( IOException ex ) { throw new IndexShardRestoreFailedException ( shardId , <str> , ex ) ; } final StoreFileMetaData restoredSegmentsFile = sourceMetaData . getSegmentsFile ( ) ; if ( recoveryTargetMetadata = = null ) { throw new IndexShardRestoreFailedException ( shardId , <str> ) ; } assert restoredSegmentsFile ! = null ; final SegmentInfos segmentCommitInfos ; try { segmentCommitInfos = Lucene . pruneUnreferencedFiles ( restoredSegmentsFile . name ( ) , store . directory ( ) ) ; } catch ( IOException e ) { throw new IndexShardRestoreFailedException ( shardId , <str> , e ) ; } recoveryState . getIndex ( ) . updateVersion ( segmentCommitInfos . getVersion ( ) ) ; try { for ( String storeFile : store . directory ( ) . listAll ( ) ) { if ( Store . isAutogenerated ( storeFile ) | | snapshotFiles . containPhysicalIndexFile ( storeFile ) ) { continue ; } try { store . deleteQuiet ( <str> , storeFile ) ; store . directory ( ) . deleteFile ( storeFile ) ; } catch ( IOException e ) { logger . warn ( <str> , snapshotId , storeFile ) ; } } } catch ( IOException e ) { logger . warn ( <str> , snapshotId ) ; } } finally { store . decRef ( ) ; } } private void restoreFile ( final FileInfo fileInfo ) throws IOException { boolean success = false ; try ( InputStream partSliceStream = new PartSliceStream ( blobContainer , fileInfo ) ) { final InputStream stream ; if ( restoreRateLimiter = = null ) { stream = partSliceStream ; } else { stream = new RateLimitingInputStream ( partSliceStream , restoreRateLimiter , restoreThrottleListener ) ; } try ( final IndexOutput indexOutput = store . createVerifyingOutput ( fileInfo . physicalName ( ) , fileInfo . metadata ( ) , IOContext . DEFAULT ) ) { final byte [ ] buffer = new byte [ BUFFER_SIZE ] ; int length ; while ( ( length = stream . read ( buffer ) ) > <int> ) { indexOutput . writeBytes ( buffer , <int> , length ) ; recoveryState . getIndex ( ) . addRecoveredBytesToFile ( fileInfo . name ( ) , length ) ; } Store . verify ( indexOutput ) ; indexOutput . close ( ) ; if ( fileInfo . metadata ( ) . hasLegacyChecksum ( ) ) { Store . LegacyChecksums legacyChecksums = new Store . LegacyChecksums ( ) ; legacyChecksums . add ( fileInfo . metadata ( ) ) ; legacyChecksums . write ( store ) ; } store . directory ( ) . sync ( Collections . singleton ( fileInfo . physicalName ( ) ) ) ; success = true ; } catch ( CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex ) { try { store . markStoreCorrupted ( ex ) ; } catch ( IOException e ) { logger . warn ( <str> , e ) ; } throw ex ; } finally { if ( success = = false ) { store . deleteQuiet ( fileInfo . physicalName ( ) ) ; } } } } } public interface RateLimiterListener { void onRestorePause ( long nanos ) ; void onSnapshotPause ( long nanos ) ; } } 
