package org . elasticsearch . index . engine ; import org . apache . lucene . index . * ; import org . apache . lucene . index . IndexWriter . IndexReaderWarmer ; import org . apache . lucene . search . IndexSearcher ; import org . apache . lucene . search . SearcherFactory ; import org . apache . lucene . search . SearcherManager ; import org . apache . lucene . store . AlreadyClosedException ; import org . apache . lucene . store . Directory ; import org . apache . lucene . store . LockObtainFailedException ; import org . apache . lucene . util . BytesRef ; import org . apache . lucene . util . IOUtils ; import org . apache . lucene . util . InfoStream ; import org . elasticsearch . ElasticsearchException ; import org . elasticsearch . ExceptionsHelper ; import org . elasticsearch . cluster . routing . Murmur3HashFunction ; import org . elasticsearch . common . Nullable ; import org . elasticsearch . common . lease . Releasable ; import org . elasticsearch . common . logging . ESLogger ; import org . elasticsearch . common . lucene . LoggerInfoStream ; import org . elasticsearch . common . lucene . Lucene ; import org . elasticsearch . common . lucene . index . ElasticsearchDirectoryReader ; import org . elasticsearch . common . lucene . index . ElasticsearchLeafReader ; import org . elasticsearch . common . lucene . uid . Versions ; import org . elasticsearch . common . math . MathUtils ; import org . elasticsearch . common . util . concurrent . AbstractRunnable ; import org . elasticsearch . common . util . concurrent . EsRejectedExecutionException ; import org . elasticsearch . common . util . concurrent . ReleasableLock ; import org . elasticsearch . index . IndexSettings ; import org . elasticsearch . index . indexing . ShardIndexingService ; import org . elasticsearch . index . mapper . Uid ; import org . elasticsearch . index . merge . MergeStats ; import org . elasticsearch . index . merge . OnGoingMerge ; import org . elasticsearch . index . shard . ElasticsearchMergePolicy ; import org . elasticsearch . index . shard . MergeSchedulerConfig ; import org . elasticsearch . index . shard . ShardId ; import org . elasticsearch . index . shard . TranslogRecoveryPerformer ; import org . elasticsearch . index . translog . Translog ; import org . elasticsearch . index . translog . TranslogConfig ; import org . elasticsearch . index . translog . TranslogCorruptedException ; import org . elasticsearch . rest . RestStatus ; import org . elasticsearch . threadpool . ThreadPool ; import java . io . IOException ; import java . util . * ; import java . util . concurrent . atomic . AtomicBoolean ; import java . util . concurrent . atomic . AtomicInteger ; import java . util . concurrent . locks . Lock ; import java . util . concurrent . locks . ReentrantLock ; import java . util . function . Function ; public class InternalEngine extends Engine { private volatile long lastDeleteVersionPruneTimeMSec ; private final ShardIndexingService indexingService ; private final Engine . Warmer warmer ; private final Translog translog ; private final ElasticsearchConcurrentMergeScheduler mergeScheduler ; private final IndexWriter indexWriter ; private final SearcherFactory searcherFactory ; private final SearcherManager searcherManager ; private final Lock flushLock = new ReentrantLock ( ) ; private final ReentrantLock optimizeLock = new ReentrantLock ( ) ; private final LiveVersionMap versionMap ; private final Object [ ] dirtyLocks ; private final AtomicBoolean versionMapRefreshPending = new AtomicBoolean ( ) ; private volatile SegmentInfos lastCommittedSegmentInfos ; private final IndexThrottle throttle ; public InternalEngine ( EngineConfig engineConfig , boolean skipInitialTranslogRecovery ) throws EngineException { super ( engineConfig ) ; this . versionMap = new LiveVersionMap ( ) ; store . incRef ( ) ; IndexWriter writer = null ; Translog translog = null ; SearcherManager manager = null ; EngineMergeScheduler scheduler = null ; boolean success = false ; try { this . lastDeleteVersionPruneTimeMSec = engineConfig . getThreadPool ( ) . estimatedTimeInMillis ( ) ; this . indexingService = engineConfig . getIndexingService ( ) ; this . warmer = engineConfig . getWarmer ( ) ; mergeScheduler = scheduler = new EngineMergeScheduler ( engineConfig . getShardId ( ) , engineConfig . getIndexSettings ( ) , engineConfig . getMergeSchedulerConfig ( ) ) ; this . dirtyLocks = new Object [ Runtime . getRuntime ( ) . availableProcessors ( ) * <int> ] ; for ( int i = <int> ; i < dirtyLocks . length ; i + + ) { dirtyLocks [ i ] = new Object ( ) ; } throttle = new IndexThrottle ( ) ; this . searcherFactory = new SearchFactory ( logger , isClosed , engineConfig ) ; final Translog . TranslogGeneration translogGeneration ; try { final boolean create = engineConfig . isCreate ( ) ; writer = createWriter ( create ) ; indexWriter = writer ; translog = openTranslog ( engineConfig , writer , create | | skipInitialTranslogRecovery | | engineConfig . forceNewTranslog ( ) ) ; translogGeneration = translog . getGeneration ( ) ; assert translogGeneration ! = null ; } catch ( IOException | TranslogCorruptedException e ) { throw new EngineCreationFailureException ( shardId , <str> , e ) ; } catch ( AssertionError e ) { if ( ExceptionsHelper . stackTrace ( e ) . contains ( <str> ) ) { throw new EngineCreationFailureException ( shardId , <str> , e ) ; } else { throw e ; } } this . translog = translog ; manager = createSearcherManager ( ) ; this . searcherManager = manager ; this . versionMap . setManager ( searcherManager ) ; try { if ( skipInitialTranslogRecovery ) { commitIndexWriter ( writer , translog , lastCommittedSegmentInfos . getUserData ( ) . get ( SYNC_COMMIT_ID ) ) ; } else { recoverFromTranslog ( engineConfig , translogGeneration ) ; } } catch ( IOException | EngineException ex ) { throw new EngineCreationFailureException ( shardId , <str> , ex ) ; } success = true ; } finally { if ( success = = false ) { IOUtils . closeWhileHandlingException ( writer , translog , manager , scheduler ) ; versionMap . clear ( ) ; if ( isClosed . get ( ) = = false ) { store . decRef ( ) ; } } } logger . trace ( <str> ) ; } private Translog openTranslog ( EngineConfig engineConfig , IndexWriter writer , boolean createNew ) throws IOException { final Translog . TranslogGeneration generation = loadTranslogIdFromCommit ( writer ) ; final TranslogConfig translogConfig = engineConfig . getTranslogConfig ( ) ; if ( createNew = = false ) { if ( generation = = null ) { throw new IllegalStateException ( <str> ) ; } translogConfig . setTranslogGeneration ( generation ) ; if ( generation ! = null & & generation . translogUUID = = null ) { throw new IndexFormatTooOldException ( <str> , <str> ) ; } } final Translog translog = new Translog ( translogConfig ) ; if ( generation = = null | | generation . translogUUID = = null ) { if ( generation = = null ) { logger . debug ( <str> ) ; } else if ( generation . translogUUID = = null ) { logger . debug ( <str> ) ; } boolean success = false ; try { commitIndexWriter ( writer , translog ) ; success = true ; } finally { if ( success = = false ) { IOUtils . closeWhileHandlingException ( translog ) ; } } } return translog ; } @Override public Translog getTranslog ( ) { ensureOpen ( ) ; return translog ; } protected void recoverFromTranslog ( EngineConfig engineConfig , Translog . TranslogGeneration translogGeneration ) throws IOException { int opsRecovered = <int> ; final TranslogRecoveryPerformer handler = engineConfig . getTranslogRecoveryPerformer ( ) ; try ( Translog . Snapshot snapshot = translog . newSnapshot ( ) ) { Translog . Operation operation ; while ( ( operation = snapshot . next ( ) ) ! = null ) { try { handler . performRecoveryOperation ( this , operation , true ) ; opsRecovered + + ; } catch ( ElasticsearchException e ) { if ( e . status ( ) = = RestStatus . BAD_REQUEST ) { logger . info ( <str> , e ) ; } else { throw e ; } } } } catch ( Throwable e ) { throw new EngineException ( shardId , <str> , e ) ; } if ( opsRecovered > <int> ) { logger . trace ( <str> , opsRecovered , translogGeneration = = null ? null : translogGeneration . translogFileGeneration , translog . currentFileGeneration ( ) ) ; flush ( true , true ) ; } else if ( translog . isCurrent ( translogGeneration ) = = false ) { commitIndexWriter ( indexWriter , translog , lastCommittedSegmentInfos . getUserData ( ) . get ( Engine . SYNC_COMMIT_ID ) ) ; } } @Nullable private Translog . TranslogGeneration loadTranslogIdFromCommit ( IndexWriter writer ) throws IOException { final Map < String , String > commitUserData = writer . getCommitData ( ) ; if ( commitUserData . containsKey ( <str> ) ) { assert commitUserData . containsKey ( Translog . TRANSLOG_UUID_KEY ) = = false : <str> ; return new Translog . TranslogGeneration ( null , Long . parseLong ( commitUserData . get ( <str> ) ) ) ; } else if ( commitUserData . containsKey ( Translog . TRANSLOG_GENERATION_KEY ) ) { if ( commitUserData . containsKey ( Translog . TRANSLOG_UUID_KEY ) = = false ) { throw new IllegalStateException ( <str> ) ; } final String translogUUID = commitUserData . get ( Translog . TRANSLOG_UUID_KEY ) ; final long translogGen = Long . parseLong ( commitUserData . get ( Translog . TRANSLOG_GENERATION_KEY ) ) ; return new Translog . TranslogGeneration ( translogUUID , translogGen ) ; } return null ; } private SearcherManager createSearcherManager ( ) throws EngineException { boolean success = false ; SearcherManager searcherManager = null ; try { try { final DirectoryReader directoryReader = ElasticsearchDirectoryReader . wrap ( DirectoryReader . open ( indexWriter , true ) , shardId ) ; searcherManager = new SearcherManager ( directoryReader , searcherFactory ) ; lastCommittedSegmentInfos = readLastCommittedSegmentInfos ( searcherManager , store ) ; success = true ; return searcherManager ; } catch ( IOException e ) { maybeFailEngine ( <str> , e ) ; try { indexWriter . rollback ( ) ; } catch ( IOException e1 ) { e . addSuppressed ( e1 ) ; } throw new EngineCreationFailureException ( shardId , <str> , e ) ; } } finally { if ( success = = false ) { IOUtils . closeWhileHandlingException ( searcherManager , indexWriter ) ; } } } private void updateIndexWriterSettings ( ) { try { final LiveIndexWriterConfig iwc = indexWriter . getConfig ( ) ; iwc . setRAMBufferSizeMB ( engineConfig . getIndexingBufferSize ( ) . mbFrac ( ) ) ; iwc . setUseCompoundFile ( engineConfig . isCompoundOnFlush ( ) ) ; } catch ( AlreadyClosedException ex ) { } } @Override public GetResult get ( Get get , Function < String , Searcher > searcherFactory ) throws EngineException { try ( ReleasableLock lock = readLock . acquire ( ) ) { ensureOpen ( ) ; if ( get . realtime ( ) ) { VersionValue versionValue = versionMap . getUnderLock ( get . uid ( ) . bytes ( ) ) ; if ( versionValue ! = null ) { if ( versionValue . delete ( ) ) { return GetResult . NOT_EXISTS ; } if ( get . versionType ( ) . isVersionConflictForReads ( versionValue . version ( ) , get . version ( ) ) ) { Uid uid = Uid . createUid ( get . uid ( ) . text ( ) ) ; throw new VersionConflictEngineException ( shardId , uid . type ( ) , uid . id ( ) , get . versionType ( ) . explainConflictForReads ( versionValue . version ( ) , get . version ( ) ) ) ; } Translog . Operation op = translog . read ( versionValue . translogLocation ( ) ) ; if ( op ! = null ) { return new GetResult ( true , versionValue . version ( ) , op . getSource ( ) ) ; } } } return getFromSearcher ( get , searcherFactory ) ; } } @Override public boolean index ( Index index ) { final boolean created ; try ( ReleasableLock lock = readLock . acquire ( ) ) { ensureOpen ( ) ; if ( index . origin ( ) = = Operation . Origin . RECOVERY ) { created = innerIndex ( index ) ; } else { try ( Releasable r = throttle . acquireThrottle ( ) ) { created = innerIndex ( index ) ; } } } catch ( OutOfMemoryError | IllegalStateException | IOException t ) { maybeFailEngine ( <str> , t ) ; throw new IndexFailedEngineException ( shardId , index . type ( ) , index . id ( ) , t ) ; } checkVersionMapRefresh ( ) ; return created ; } private boolean innerIndex ( Index index ) throws IOException { synchronized ( dirtyLock ( index . uid ( ) ) ) { lastWriteNanos = index . startTime ( ) ; final long currentVersion ; final boolean deleted ; VersionValue versionValue = versionMap . getUnderLock ( index . uid ( ) . bytes ( ) ) ; if ( versionValue = = null ) { currentVersion = loadCurrentVersionFromIndex ( index . uid ( ) ) ; deleted = currentVersion = = Versions . NOT_FOUND ; } else { deleted = versionValue . delete ( ) ; if ( engineConfig . isEnableGcDeletes ( ) & & versionValue . delete ( ) & & ( engineConfig . getThreadPool ( ) . estimatedTimeInMillis ( ) - versionValue . time ( ) ) > engineConfig . getGcDeletesInMillis ( ) ) { currentVersion = Versions . NOT_FOUND ; } else { currentVersion = versionValue . version ( ) ; } } long expectedVersion = index . version ( ) ; if ( index . versionType ( ) . isVersionConflictForWrites ( currentVersion , expectedVersion , deleted ) ) { if ( index . origin ( ) = = Operation . Origin . RECOVERY ) { return false ; } else { throw new VersionConflictEngineException ( shardId , index . type ( ) , index . id ( ) , index . versionType ( ) . explainConflictForWrites ( currentVersion , expectedVersion , deleted ) ) ; } } long updatedVersion = index . versionType ( ) . updateVersion ( currentVersion , expectedVersion ) ; final boolean created ; index . updateVersion ( updatedVersion ) ; if ( currentVersion = = Versions . NOT_FOUND ) { created = true ; if ( index . docs ( ) . size ( ) > <int> ) { indexWriter . addDocuments ( index . docs ( ) ) ; } else { indexWriter . addDocument ( index . docs ( ) . get ( <int> ) ) ; } } else { if ( versionValue ! = null ) { created = versionValue . delete ( ) ; } else { created = false ; } if ( index . docs ( ) . size ( ) > <int> ) { indexWriter . updateDocuments ( index . uid ( ) , index . docs ( ) ) ; } else { indexWriter . updateDocument ( index . uid ( ) , index . docs ( ) . get ( <int> ) ) ; } } Translog . Location translogLocation = translog . add ( new Translog . Index ( index ) ) ; versionMap . putUnderLock ( index . uid ( ) . bytes ( ) , new VersionValue ( updatedVersion , translogLocation ) ) ; index . setTranslogLocation ( translogLocation ) ; indexingService . postIndexUnderLock ( index ) ; return created ; } } private void checkVersionMapRefresh ( ) { if ( versionMap . ramBytesUsedForRefresh ( ) > config ( ) . getVersionMapSize ( ) . bytes ( ) & & versionMapRefreshPending . getAndSet ( true ) = = false ) { try { if ( isClosed . get ( ) ) { return ; } engineConfig . getThreadPool ( ) . executor ( ThreadPool . Names . REFRESH ) . execute ( new Runnable ( ) { @Override public void run ( ) { try { refresh ( <str> ) ; } catch ( EngineClosedException ex ) { } } } ) ; } catch ( EsRejectedExecutionException ex ) { } } } @Override public void delete ( Delete delete ) throws EngineException { try ( ReleasableLock lock = readLock . acquire ( ) ) { ensureOpen ( ) ; innerDelete ( delete ) ; } catch ( OutOfMemoryError | IllegalStateException | IOException t ) { maybeFailEngine ( <str> , t ) ; throw new DeleteFailedEngineException ( shardId , delete , t ) ; } maybePruneDeletedTombstones ( ) ; checkVersionMapRefresh ( ) ; } private void maybePruneDeletedTombstones ( ) { if ( engineConfig . isEnableGcDeletes ( ) & & engineConfig . getThreadPool ( ) . estimatedTimeInMillis ( ) - lastDeleteVersionPruneTimeMSec > engineConfig . getGcDeletesInMillis ( ) * <float> ) { pruneDeletedTombstones ( ) ; } } private void innerDelete ( Delete delete ) throws IOException { synchronized ( dirtyLock ( delete . uid ( ) ) ) { lastWriteNanos = delete . startTime ( ) ; final long currentVersion ; final boolean deleted ; VersionValue versionValue = versionMap . getUnderLock ( delete . uid ( ) . bytes ( ) ) ; if ( versionValue = = null ) { currentVersion = loadCurrentVersionFromIndex ( delete . uid ( ) ) ; deleted = currentVersion = = Versions . NOT_FOUND ; } else { deleted = versionValue . delete ( ) ; if ( engineConfig . isEnableGcDeletes ( ) & & versionValue . delete ( ) & & ( engineConfig . getThreadPool ( ) . estimatedTimeInMillis ( ) - versionValue . time ( ) ) > engineConfig . getGcDeletesInMillis ( ) ) { currentVersion = Versions . NOT_FOUND ; } else { currentVersion = versionValue . version ( ) ; } } long updatedVersion ; long expectedVersion = delete . version ( ) ; if ( delete . versionType ( ) . isVersionConflictForWrites ( currentVersion , expectedVersion , deleted ) ) { if ( delete . origin ( ) = = Operation . Origin . RECOVERY ) { return ; } else { throw new VersionConflictEngineException ( shardId , delete . type ( ) , delete . id ( ) , delete . versionType ( ) . explainConflictForWrites ( currentVersion , expectedVersion , deleted ) ) ; } } updatedVersion = delete . versionType ( ) . updateVersion ( currentVersion , expectedVersion ) ; final boolean found ; if ( currentVersion = = Versions . NOT_FOUND ) { found = false ; } else if ( versionValue ! = null & & versionValue . delete ( ) ) { found = false ; } else { indexWriter . deleteDocuments ( delete . uid ( ) ) ; found = true ; } delete . updateVersion ( updatedVersion , found ) ; Translog . Location translogLocation = translog . add ( new Translog . Delete ( delete ) ) ; versionMap . putUnderLock ( delete . uid ( ) . bytes ( ) , new DeleteVersionValue ( updatedVersion , engineConfig . getThreadPool ( ) . estimatedTimeInMillis ( ) , translogLocation ) ) ; delete . setTranslogLocation ( translogLocation ) ; indexingService . postDeleteUnderLock ( delete ) ; } } @Override public void refresh ( String source ) throws EngineException { try ( ReleasableLock lock = readLock . acquire ( ) ) { ensureOpen ( ) ; searcherManager . maybeRefreshBlocking ( ) ; } catch ( AlreadyClosedException e ) { ensureOpen ( ) ; maybeFailEngine ( <str> , e ) ; } catch ( EngineClosedException e ) { throw e ; } catch ( Throwable t ) { failEngine ( <str> , t ) ; throw new RefreshFailedEngineException ( shardId , t ) ; } maybePruneDeletedTombstones ( ) ; versionMapRefreshPending . set ( false ) ; mergeScheduler . refreshConfig ( ) ; } @Override public SyncedFlushResult syncFlush ( String syncId , CommitId expectedCommitId ) throws EngineException { ensureOpen ( ) ; if ( indexWriter . hasUncommittedChanges ( ) ) { logger . trace ( <str> , syncId ) ; return SyncedFlushResult . PENDING_OPERATIONS ; } if ( expectedCommitId . idsEqual ( lastCommittedSegmentInfos . getId ( ) ) = = false ) { logger . trace ( <str> , syncId ) ; return SyncedFlushResult . COMMIT_MISMATCH ; } try ( ReleasableLock lock = writeLock . acquire ( ) ) { ensureOpen ( ) ; if ( indexWriter . hasUncommittedChanges ( ) ) { logger . trace ( <str> , syncId ) ; return SyncedFlushResult . PENDING_OPERATIONS ; } if ( expectedCommitId . idsEqual ( lastCommittedSegmentInfos . getId ( ) ) = = false ) { logger . trace ( <str> , syncId ) ; return SyncedFlushResult . COMMIT_MISMATCH ; } logger . trace ( <str> , syncId ) ; commitIndexWriter ( indexWriter , translog , syncId ) ; logger . debug ( <str> , syncId ) ; lastCommittedSegmentInfos = store . readLastCommittedSegmentsInfo ( ) ; return SyncedFlushResult . SUCCESS ; } catch ( IOException ex ) { maybeFailEngine ( <str> , ex ) ; throw new EngineException ( shardId , <str> , ex ) ; } } final boolean tryRenewSyncCommit ( ) { boolean renewed = false ; try ( ReleasableLock lock = writeLock . acquire ( ) ) { ensureOpen ( ) ; String syncId = lastCommittedSegmentInfos . getUserData ( ) . get ( SYNC_COMMIT_ID ) ; if ( syncId ! = null & & translog . totalOperations ( ) = = <int> & & indexWriter . hasUncommittedChanges ( ) ) { logger . trace ( <str> , syncId ) ; commitIndexWriter ( indexWriter , translog , syncId ) ; logger . debug ( <str> , syncId ) ; lastCommittedSegmentInfos = store . readLastCommittedSegmentsInfo ( ) ; renewed = true ; } } catch ( IOException ex ) { maybeFailEngine ( <str> , ex ) ; throw new EngineException ( shardId , <str> , ex ) ; } if ( renewed ) { refresh ( <str> ) ; } return renewed ; } @Override public CommitId flush ( ) throws EngineException { return flush ( false , false ) ; } @Override public CommitId flush ( boolean force , boolean waitIfOngoing ) throws EngineException { ensureOpen ( ) ; final byte [ ] newCommitId ; try ( ReleasableLock lock = readLock . acquire ( ) ) { ensureOpen ( ) ; if ( flushLock . tryLock ( ) = = false ) { if ( waitIfOngoing ) { logger . trace ( <str> ) ; flushLock . lock ( ) ; logger . trace ( <str> ) ; } else { throw new FlushNotAllowedEngineException ( shardId , <str> ) ; } } else { logger . trace ( <str> ) ; } try { if ( indexWriter . hasUncommittedChanges ( ) | | force ) { try { translog . prepareCommit ( ) ; logger . trace ( <str> ) ; commitIndexWriter ( indexWriter , translog ) ; logger . trace ( <str> ) ; refresh ( <str> ) ; translog . commit ( ) ; } catch ( Throwable e ) { throw new FlushFailedEngineException ( shardId , e ) ; } } store . incRef ( ) ; try { lastCommittedSegmentInfos = store . readLastCommittedSegmentsInfo ( ) ; } catch ( Throwable e ) { if ( isClosed . get ( ) = = false ) { logger . warn ( <str> , e ) ; if ( Lucene . isCorruptionException ( e ) ) { throw new FlushFailedEngineException ( shardId , e ) ; } } } finally { store . decRef ( ) ; } newCommitId = lastCommittedSegmentInfos . getId ( ) ; } catch ( FlushFailedEngineException ex ) { maybeFailEngine ( <str> , ex ) ; throw ex ; } finally { flushLock . unlock ( ) ; } } if ( engineConfig . isEnableGcDeletes ( ) ) { pruneDeletedTombstones ( ) ; } return new CommitId ( newCommitId ) ; } private void pruneDeletedTombstones ( ) { long timeMSec = engineConfig . getThreadPool ( ) . estimatedTimeInMillis ( ) ; for ( Map . Entry < BytesRef , VersionValue > entry : versionMap . getAllTombstones ( ) ) { BytesRef uid = entry . getKey ( ) ; synchronized ( dirtyLock ( uid ) ) { VersionValue versionValue = versionMap . getTombstoneUnderLock ( uid ) ; if ( versionValue ! = null ) { if ( timeMSec - versionValue . time ( ) > engineConfig . getGcDeletesInMillis ( ) ) { versionMap . removeTombstoneUnderLock ( uid ) ; } } } } lastDeleteVersionPruneTimeMSec = timeMSec ; } @Override public void forceMerge ( final boolean flush , int maxNumSegments , boolean onlyExpungeDeletes , final boolean upgrade , final boolean upgradeOnlyAncientSegments ) throws EngineException , EngineClosedException , IOException { assert indexWriter . getConfig ( ) . getMergePolicy ( ) instanceof ElasticsearchMergePolicy : <str> + indexWriter . getConfig ( ) . getMergePolicy ( ) . getClass ( ) . getName ( ) ; ElasticsearchMergePolicy mp = ( ElasticsearchMergePolicy ) indexWriter . getConfig ( ) . getMergePolicy ( ) ; optimizeLock . lock ( ) ; try { ensureOpen ( ) ; if ( upgrade ) { logger . info ( <str> , upgradeOnlyAncientSegments ) ; mp . setUpgradeInProgress ( true , upgradeOnlyAncientSegments ) ; } store . incRef ( ) ; try { if ( onlyExpungeDeletes ) { assert upgrade = = false ; indexWriter . forceMergeDeletes ( true ) ; } else if ( maxNumSegments < = <int> ) { assert upgrade = = false ; indexWriter . maybeMerge ( ) ; } else { indexWriter . forceMerge ( maxNumSegments , true ) ; } if ( flush ) { flush ( true , true ) ; } if ( upgrade ) { logger . info ( <str> ) ; } } finally { store . decRef ( ) ; } } catch ( Throwable t ) { maybeFailEngine ( <str> , t ) ; throw t ; } finally { try { mp . setUpgradeInProgress ( false , false ) ; } finally { optimizeLock . unlock ( ) ; } } } @Override public IndexCommit snapshotIndex ( final boolean flushFirst ) throws EngineException { if ( flushFirst ) { logger . trace ( <str> ) ; flush ( false , true ) ; logger . trace ( <str> ) ; } try ( ReleasableLock lock = readLock . acquire ( ) ) { ensureOpen ( ) ; logger . trace ( <str> ) ; return deletionPolicy . snapshot ( ) ; } catch ( IOException e ) { throw new SnapshotFailedEngineException ( shardId , e ) ; } } @Override protected boolean maybeFailEngine ( String source , Throwable t ) { boolean shouldFail = super . maybeFailEngine ( source , t ) ; if ( shouldFail ) { return true ; } if ( t instanceof AlreadyClosedException ) { if ( indexWriter . isOpen ( ) = = false & & indexWriter . getTragicException ( ) ! = null ) { failEngine ( <str> , indexWriter . getTragicException ( ) ) ; } return true ; } else if ( t ! = null & & indexWriter . isOpen ( ) = = false & & indexWriter . getTragicException ( ) = = t ) { failEngine ( source , t ) ; return true ; } return false ; } @Override protected SegmentInfos getLastCommittedSegmentInfos ( ) { return lastCommittedSegmentInfos ; } @Override protected final void writerSegmentStats ( SegmentsStats stats ) { stats . addVersionMapMemoryInBytes ( versionMap . ramBytesUsed ( ) ) ; stats . addIndexWriterMemoryInBytes ( indexWriter . ramBytesUsed ( ) ) ; stats . addIndexWriterMaxMemoryInBytes ( ( long ) ( indexWriter . getConfig ( ) . getRAMBufferSizeMB ( ) * <int> * <int> ) ) ; } @Override public long indexWriterRAMBytesUsed ( ) { return indexWriter . ramBytesUsed ( ) ; } @Override public List < Segment > segments ( boolean verbose ) { try ( ReleasableLock lock = readLock . acquire ( ) ) { Segment [ ] segmentsArr = getSegmentInfo ( lastCommittedSegmentInfos , verbose ) ; Set < OnGoingMerge > onGoingMerges = mergeScheduler . onGoingMerges ( ) ; for ( OnGoingMerge onGoingMerge : onGoingMerges ) { for ( SegmentCommitInfo segmentInfoPerCommit : onGoingMerge . getMergedSegments ( ) ) { for ( Segment segment : segmentsArr ) { if ( segment . getName ( ) . equals ( segmentInfoPerCommit . info . name ) ) { segment . mergeId = onGoingMerge . getId ( ) ; break ; } } } } return Arrays . asList ( segmentsArr ) ; } } @Override protected final void closeNoLock ( String reason ) { if ( isClosed . compareAndSet ( false , true ) ) { assert rwl . isWriteLockedByCurrentThread ( ) | | failEngineLock . isHeldByCurrentThread ( ) : <str> ; try { this . versionMap . clear ( ) ; try { IOUtils . close ( searcherManager ) ; } catch ( Throwable t ) { logger . warn ( <str> , t ) ; } try { IOUtils . close ( translog ) ; } catch ( Throwable t ) { logger . warn ( <str> , t ) ; } logger . trace ( <str> ) ; try { indexWriter . rollback ( ) ; } catch ( AlreadyClosedException e ) { } logger . trace ( <str> ) ; } catch ( Throwable e ) { logger . warn ( <str> , e ) ; } finally { store . decRef ( ) ; logger . debug ( <str> , reason ) ; } } } @Override protected SearcherManager getSearcherManager ( ) { return searcherManager ; } private Object dirtyLock ( BytesRef uid ) { int hash = Murmur3HashFunction . hash ( uid . bytes , uid . offset , uid . length ) ; return dirtyLocks [ MathUtils . mod ( hash , dirtyLocks . length ) ] ; } private Object dirtyLock ( Term uid ) { return dirtyLock ( uid . bytes ( ) ) ; } private long loadCurrentVersionFromIndex ( Term uid ) throws IOException { try ( final Searcher searcher = acquireSearcher ( <str> ) ) { return Versions . loadVersion ( searcher . reader ( ) , uid ) ; } } private IndexWriter createWriter ( boolean create ) throws IOException { try { final IndexWriterConfig iwc = new IndexWriterConfig ( engineConfig . getAnalyzer ( ) ) ; iwc . setCommitOnClose ( false ) ; iwc . setOpenMode ( create ? IndexWriterConfig . OpenMode . CREATE : IndexWriterConfig . OpenMode . APPEND ) ; iwc . setIndexDeletionPolicy ( deletionPolicy ) ; boolean verbose = false ; try { verbose = Boolean . parseBoolean ( System . getProperty ( <str> ) ) ; } catch ( Throwable ignore ) { } iwc . setInfoStream ( verbose ? InfoStream . getDefault ( ) : new LoggerInfoStream ( logger ) ) ; iwc . setMergeScheduler ( mergeScheduler ) ; MergePolicy mergePolicy = config ( ) . getMergePolicy ( ) ; mergePolicy = new ElasticsearchMergePolicy ( mergePolicy ) ; iwc . setMergePolicy ( mergePolicy ) ; iwc . setSimilarity ( engineConfig . getSimilarity ( ) ) ; iwc . setRAMBufferSizeMB ( engineConfig . getIndexingBufferSize ( ) . mbFrac ( ) ) ; iwc . setCodec ( engineConfig . getCodec ( ) ) ; iwc . setWriteLockTimeout ( <int> ) ; iwc . setUseCompoundFile ( this . engineConfig . isCompoundOnFlush ( ) ) ; iwc . setMergedSegmentWarmer ( new IndexReaderWarmer ( ) { @Override public void warm ( LeafReader reader ) throws IOException { try { LeafReader esLeafReader = new ElasticsearchLeafReader ( reader , shardId ) ; assert isMergedSegment ( esLeafReader ) ; if ( warmer ! = null ) { final Engine . Searcher searcher = new Searcher ( <str> , searcherFactory . newSearcher ( esLeafReader , null ) ) ; warmer . warm ( searcher , false ) ; } } catch ( Throwable t ) { if ( isClosed . get ( ) = = false ) { logger . warn ( <str> , t ) ; } if ( t instanceof Error ) { throw ( Error ) t ; } } } } ) ; return new IndexWriter ( store . directory ( ) , iwc ) ; } catch ( LockObtainFailedException ex ) { boolean isLocked = IndexWriter . isLocked ( store . directory ( ) ) ; logger . warn ( <str> , ex , isLocked ) ; throw ex ; } } final static class SearchFactory extends EngineSearcherFactory { private final Engine . Warmer warmer ; private final ShardId shardId ; private final ESLogger logger ; private final AtomicBoolean isEngineClosed ; SearchFactory ( ESLogger logger , AtomicBoolean isEngineClosed , EngineConfig engineConfig ) { super ( engineConfig ) ; warmer = engineConfig . getWarmer ( ) ; shardId = engineConfig . getShardId ( ) ; this . logger = logger ; this . isEngineClosed = isEngineClosed ; } @Override public IndexSearcher newSearcher ( IndexReader reader , IndexReader previousReader ) throws IOException { IndexSearcher searcher = super . newSearcher ( reader , previousReader ) ; if ( reader instanceof LeafReader & & isMergedSegment ( ( LeafReader ) reader ) ) { return searcher ; } if ( warmer ! = null ) { IndexSearcher newSearcher = null ; boolean closeNewSearcher = false ; try { if ( previousReader = = null ) { newSearcher = searcher ; } else { List < IndexReader > readers = new ArrayList < > ( ) ; for ( LeafReaderContext newReaderContext : reader . leaves ( ) ) { if ( isMergedSegment ( newReaderContext . reader ( ) ) ) { continue ; } boolean found = false ; for ( LeafReaderContext currentReaderContext : previousReader . leaves ( ) ) { if ( currentReaderContext . reader ( ) . getCoreCacheKey ( ) . equals ( newReaderContext . reader ( ) . getCoreCacheKey ( ) ) ) { found = true ; break ; } } if ( ! found ) { readers . add ( newReaderContext . reader ( ) ) ; } } if ( ! readers . isEmpty ( ) ) { IndexReader newReader = new MultiReader ( readers . toArray ( new IndexReader [ readers . size ( ) ] ) , false ) ; newSearcher = super . newSearcher ( newReader , null ) ; closeNewSearcher = true ; } } if ( newSearcher ! = null ) { warmer . warm ( new Searcher ( <str> , newSearcher ) , false ) ; } assert searcher . getIndexReader ( ) instanceof ElasticsearchDirectoryReader : <str> + searcher . getIndexReader ( ) . getClass ( ) ; warmer . warm ( new Searcher ( <str> , searcher ) , true ) ; } catch ( Throwable e ) { if ( isEngineClosed . get ( ) = = false ) { logger . warn ( <str> , e ) ; } } finally { if ( newSearcher ! = null & & closeNewSearcher ) { IOUtils . closeWhileHandlingException ( newSearcher . getIndexReader ( ) ) ; } } } return searcher ; } } public void activateThrottling ( ) { throttle . activate ( ) ; } public void deactivateThrottling ( ) { throttle . deactivate ( ) ; } long getGcDeletesInMillis ( ) { return engineConfig . getGcDeletesInMillis ( ) ; } LiveIndexWriterConfig getCurrentIndexWriterConfig ( ) { return indexWriter . getConfig ( ) ; } private final class EngineMergeScheduler extends ElasticsearchConcurrentMergeScheduler { private final AtomicInteger numMergesInFlight = new AtomicInteger ( <int> ) ; private final AtomicBoolean isThrottling = new AtomicBoolean ( ) ; EngineMergeScheduler ( ShardId shardId , IndexSettings indexSettings , MergeSchedulerConfig config ) { super ( shardId , indexSettings , config ) ; } @Override public synchronized void beforeMerge ( OnGoingMerge merge ) { int maxNumMerges = mergeScheduler . getMaxMergeCount ( ) ; if ( numMergesInFlight . incrementAndGet ( ) > maxNumMerges ) { if ( isThrottling . getAndSet ( true ) = = false ) { logger . info ( <str> , numMergesInFlight , maxNumMerges ) ; indexingService . throttlingActivated ( ) ; activateThrottling ( ) ; } } } @Override public synchronized void afterMerge ( OnGoingMerge merge ) { int maxNumMerges = mergeScheduler . getMaxMergeCount ( ) ; if ( numMergesInFlight . decrementAndGet ( ) < maxNumMerges ) { if ( isThrottling . getAndSet ( false ) ) { logger . info ( <str> , numMergesInFlight , maxNumMerges ) ; indexingService . throttlingDeactivated ( ) ; deactivateThrottling ( ) ; } } if ( indexWriter . hasPendingMerges ( ) = = false & & System . nanoTime ( ) - lastWriteNanos > = engineConfig . getFlushMergesAfter ( ) . nanos ( ) ) { engineConfig . getThreadPool ( ) . executor ( ThreadPool . Names . FLUSH ) . execute ( new AbstractRunnable ( ) { @Override public void onFailure ( Throwable t ) { if ( isClosed . get ( ) = = false ) { logger . warn ( <str> ) ; } } @Override protected void doRun ( ) throws Exception { if ( tryRenewSyncCommit ( ) = = false ) { flush ( ) ; } } } ) ; } } @Override protected void handleMergeException ( final Directory dir , final Throwable exc ) { logger . error ( <str> , exc ) ; if ( config ( ) . getMergeSchedulerConfig ( ) . isNotifyOnMergeFailure ( ) ) { engineConfig . getThreadPool ( ) . generic ( ) . execute ( new AbstractRunnable ( ) { @Override public void onFailure ( Throwable t ) { logger . debug ( <str> , t ) ; } @Override protected void doRun ( ) throws Exception { MergePolicy . MergeException e = new MergePolicy . MergeException ( exc , dir ) ; failEngine ( <str> , e ) ; } } ) ; } } } private void commitIndexWriter ( IndexWriter writer , Translog translog , String syncId ) throws IOException { try { Translog . TranslogGeneration translogGeneration = translog . getGeneration ( ) ; logger . trace ( <str> , translogGeneration . translogFileGeneration , syncId ) ; Map < String , String > commitData = new HashMap < > ( <int> ) ; commitData . put ( Translog . TRANSLOG_GENERATION_KEY , Long . toString ( translogGeneration . translogFileGeneration ) ) ; commitData . put ( Translog . TRANSLOG_UUID_KEY , translogGeneration . translogUUID ) ; if ( syncId ! = null ) { commitData . put ( Engine . SYNC_COMMIT_ID , syncId ) ; } indexWriter . setCommitData ( commitData ) ; writer . commit ( ) ; } catch ( Throwable ex ) { failEngine ( <str> , ex ) ; throw ex ; } } private void commitIndexWriter ( IndexWriter writer , Translog translog ) throws IOException { commitIndexWriter ( writer , translog , null ) ; } public void onSettingsChanged ( ) { mergeScheduler . refreshConfig ( ) ; updateIndexWriterSettings ( ) ; checkVersionMapRefresh ( ) ; maybePruneDeletedTombstones ( ) ; } public MergeStats getMergeStats ( ) { return mergeScheduler . stats ( ) ; } } 
