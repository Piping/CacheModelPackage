package org . apache . cassandra . streaming . compress ; import java . io . IOException ; import java . nio . channels . WritableByteChannel ; import java . util . ArrayList ; import java . util . Collection ; import java . util . List ; import com . google . common . base . Function ; import org . apache . cassandra . io . compress . CompressionMetadata ; import org . apache . cassandra . io . sstable . format . SSTableReader ; import org . apache . cassandra . io . util . ChannelProxy ; import org . apache . cassandra . io . util . DataOutputStreamPlus ; import org . apache . cassandra . io . util . FileUtils ; import org . apache . cassandra . io . util . RandomAccessReader ; import org . apache . cassandra . streaming . ProgressInfo ; import org . apache . cassandra . streaming . StreamSession ; import org . apache . cassandra . streaming . StreamWriter ; import org . apache . cassandra . utils . Pair ; public class CompressedStreamWriter extends StreamWriter { public static final int CHUNK_SIZE = <int> * <int> * <int> ; private final CompressionInfo compressionInfo ; public CompressedStreamWriter ( SSTableReader sstable , Collection < Pair < Long , Long > > sections , CompressionInfo compressionInfo , StreamSession session ) { super ( sstable , sections , session ) ; this . compressionInfo = compressionInfo ; } @Override public void write ( DataOutputStreamPlus out ) throws IOException { long totalSize = totalSize ( ) ; try ( ChannelProxy fc = sstable . getDataChannel ( ) . sharedCopy ( ) ) { long progress = <int> L ; List < Pair < Long , Long > > sections = getTransferSections ( compressionInfo . chunks ) ; for ( final Pair < Long , Long > section : sections ) { long length = section . right - section . left ; long bytesTransferred = <int> ; while ( bytesTransferred < length ) { final long bytesTransferredFinal = bytesTransferred ; final int toTransfer = ( int ) Math . min ( CHUNK_SIZE , length - bytesTransferred ) ; limiter . acquire ( toTransfer ) ; long lastWrite = out . applyToChannel ( ( wbc ) - > fc . transferTo ( section . left + bytesTransferredFinal , toTransfer , wbc ) ) ; bytesTransferred + = lastWrite ; progress + = lastWrite ; session . progress ( sstable . descriptor , ProgressInfo . Direction . OUT , progress , totalSize ) ; } } } } @Override protected long totalSize ( ) { long size = <int> ; for ( CompressionMetadata . Chunk chunk : compressionInfo . chunks ) size + = chunk . length + <int> ; return size ; } private List < Pair < Long , Long > > getTransferSections ( CompressionMetadata . Chunk [ ] chunks ) { List < Pair < Long , Long > > transferSections = new ArrayList < > ( ) ; Pair < Long , Long > lastSection = null ; for ( CompressionMetadata . Chunk chunk : chunks ) { if ( lastSection ! = null ) { if ( chunk . offset = = lastSection . right ) { lastSection = Pair . create ( lastSection . left , chunk . offset + chunk . length + <int> ) ; } else { transferSections . add ( lastSection ) ; lastSection = Pair . create ( chunk . offset , chunk . offset + chunk . length + <int> ) ; } } else { lastSection = Pair . create ( chunk . offset , chunk . offset + chunk . length + <int> ) ; } } if ( lastSection ! = null ) transferSections . add ( lastSection ) ; return transferSections ; } } 
