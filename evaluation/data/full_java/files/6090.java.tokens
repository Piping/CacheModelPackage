package org . elasticsearch . common . util ; import com . carrotsearch . hppc . BitMixer ; import org . apache . lucene . util . BytesRef ; import org . elasticsearch . common . lease . Releasable ; import org . elasticsearch . common . lease . Releasables ; public final class BytesRefHash extends AbstractHash { private LongArray startOffsets ; private ByteArray bytes ; private IntArray hashes ; private final BytesRef spare ; public BytesRefHash ( long capacity , BigArrays bigArrays ) { this ( capacity , DEFAULT_MAX_LOAD_FACTOR , bigArrays ) ; } public BytesRefHash ( long capacity , float maxLoadFactor , BigArrays bigArrays ) { super ( capacity , maxLoadFactor , bigArrays ) ; startOffsets = bigArrays . newLongArray ( capacity + <int> , false ) ; startOffsets . set ( <int> , <int> ) ; bytes = bigArrays . newByteArray ( capacity * <int> , false ) ; hashes = bigArrays . newIntArray ( capacity , false ) ; spare = new BytesRef ( ) ; } private static int rehash ( int hash ) { return BitMixer . mix32 ( hash ) ; } public BytesRef get ( long id , BytesRef dest ) { final long startOffset = startOffsets . get ( id ) ; final int length = ( int ) ( startOffsets . get ( id + <int> ) - startOffset ) ; bytes . get ( startOffset , length , dest ) ; return dest ; } public long find ( BytesRef key , int code ) { final long slot = slot ( rehash ( code ) , mask ) ; for ( long index = slot ; ; index = nextSlot ( index , mask ) ) { final long id = id ( index ) ; if ( id = = - <int> | | key . bytesEquals ( get ( id , spare ) ) ) { return id ; } } } public long find ( BytesRef key ) { return find ( key , key . hashCode ( ) ) ; } private long set ( BytesRef key , int code , long id ) { assert rehash ( key . hashCode ( ) ) = = code ; assert size < maxSize ; final long slot = slot ( code , mask ) ; for ( long index = slot ; ; index = nextSlot ( index , mask ) ) { final long curId = id ( index ) ; if ( curId = = - <int> ) { id ( index , id ) ; append ( id , key , code ) ; + + size ; return id ; } else if ( key . bytesEquals ( get ( curId , spare ) ) ) { return - <int> - curId ; } } } private void append ( long id , BytesRef key , int code ) { assert size = = id ; final long startOffset = startOffsets . get ( size ) ; bytes = bigArrays . grow ( bytes , startOffset + key . length ) ; bytes . set ( startOffset , key . bytes , key . offset , key . length ) ; startOffsets = bigArrays . grow ( startOffsets , size + <int> ) ; startOffsets . set ( size + <int> , startOffset + key . length ) ; hashes = bigArrays . grow ( hashes , id + <int> ) ; hashes . set ( id , code ) ; } private boolean assertConsistent ( long id , int code ) { get ( id , spare ) ; return rehash ( spare . hashCode ( ) ) = = code ; } private void reset ( int code , long id ) { assert assertConsistent ( id , code ) ; final long slot = slot ( code , mask ) ; for ( long index = slot ; ; index = nextSlot ( index , mask ) ) { final long curId = id ( index ) ; if ( curId = = - <int> ) { id ( index , id ) ; break ; } } } public long add ( BytesRef key , int code ) { if ( size > = maxSize ) { assert size = = maxSize ; grow ( ) ; } assert size < maxSize ; return set ( key , rehash ( code ) , size ) ; } public long add ( BytesRef key ) { return add ( key , key . hashCode ( ) ) ; } @Override protected void removeAndAdd ( long index ) { final long id = id ( index , - <int> ) ; assert id > = <int> ; final int code = hashes . get ( id ) ; reset ( code , id ) ; } @Override public void close ( ) { try ( Releasable releasable = Releasables . wrap ( bytes , hashes , startOffsets ) ) { super . close ( ) ; } } } 
