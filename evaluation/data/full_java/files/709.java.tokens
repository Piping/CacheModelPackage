package org . apache . cassandra . io . sstable . format ; import java . io . * ; import java . nio . ByteBuffer ; import java . util . * ; import java . util . concurrent . * ; import java . util . concurrent . atomic . AtomicBoolean ; import java . util . concurrent . atomic . AtomicLong ; import com . google . common . annotations . VisibleForTesting ; import com . google . common . base . Predicate ; import com . google . common . collect . Iterables ; import com . google . common . collect . Ordering ; import com . google . common . primitives . Longs ; import com . google . common . util . concurrent . RateLimiter ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import com . clearspring . analytics . stream . cardinality . CardinalityMergeException ; import com . clearspring . analytics . stream . cardinality . HyperLogLogPlus ; import com . clearspring . analytics . stream . cardinality . ICardinality ; import org . apache . cassandra . cache . InstrumentingCache ; import org . apache . cassandra . cache . KeyCacheKey ; import org . apache . cassandra . concurrent . DebuggableThreadPoolExecutor ; import org . apache . cassandra . concurrent . ScheduledExecutors ; import org . apache . cassandra . config . CFMetaData ; import org . apache . cassandra . config . DatabaseDescriptor ; import org . apache . cassandra . config . Schema ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . commitlog . ReplayPosition ; import org . apache . cassandra . db . filter . ColumnFilter ; import org . apache . cassandra . db . rows . SliceableUnfilteredRowIterator ; import org . apache . cassandra . dht . AbstractBounds ; import org . apache . cassandra . dht . Range ; import org . apache . cassandra . dht . Token ; import org . apache . cassandra . index . internal . CassandraIndex ; import org . apache . cassandra . io . FSError ; import org . apache . cassandra . io . compress . CompressionMetadata ; import org . apache . cassandra . io . sstable . * ; import org . apache . cassandra . io . sstable . metadata . * ; import org . apache . cassandra . io . util . * ; import org . apache . cassandra . metrics . RestorableMeter ; import org . apache . cassandra . metrics . StorageMetrics ; import org . apache . cassandra . schema . CachingParams ; import org . apache . cassandra . schema . IndexMetadata ; import org . apache . cassandra . service . ActiveRepairService ; import org . apache . cassandra . service . CacheService ; import org . apache . cassandra . utils . * ; import org . apache . cassandra . utils . concurrent . OpOrder ; import org . apache . cassandra . utils . concurrent . Ref ; import org . apache . cassandra . utils . concurrent . SelfRefCounted ; import static org . apache . cassandra . db . Directories . SECONDARY_INDEX_NAME_SEPARATOR ; public abstract class SSTableReader extends SSTable implements SelfRefCounted < SSTableReader > { private static final Logger logger = LoggerFactory . getLogger ( SSTableReader . class ) ; private static final ScheduledThreadPoolExecutor syncExecutor = new ScheduledThreadPoolExecutor ( <int> ) ; static { syncExecutor . setRemoveOnCancelPolicy ( true ) ; } private static final RateLimiter meterSyncThrottle = RateLimiter . create ( <float> ) ; public static final Comparator < SSTableReader > maxTimestampComparator = new Comparator < SSTableReader > ( ) { public int compare ( SSTableReader o1 , SSTableReader o2 ) { long ts1 = o1 . getMaxTimestamp ( ) ; long ts2 = o2 . getMaxTimestamp ( ) ; return ( ts1 > ts2 ? - <int> : ( ts1 = = ts2 ? <int> : <int> ) ) ; } } ; public static final class UniqueIdentifier { } public static final Comparator < SSTableReader > sstableComparator = new Comparator < SSTableReader > ( ) { public int compare ( SSTableReader o1 , SSTableReader o2 ) { return o1 . first . compareTo ( o2 . first ) ; } } ; public static final Ordering < SSTableReader > sstableOrdering = Ordering . from ( sstableComparator ) ; public final long maxDataAge ; public enum OpenReason { NORMAL , EARLY , METADATA_CHANGE , MOVED_START } public final OpenReason openReason ; public final UniqueIdentifier instanceId = new UniqueIdentifier ( ) ; protected SegmentedFile ifile ; protected SegmentedFile dfile ; protected IndexSummary indexSummary ; protected IFilter bf ; protected final RowIndexEntry . IndexSerializer rowIndexEntrySerializer ; protected InstrumentingCache < KeyCacheKey , RowIndexEntry > keyCache ; protected final BloomFilterTracker bloomFilterTracker = new BloomFilterTracker ( ) ; protected final AtomicBoolean isSuspect = new AtomicBoolean ( false ) ; protected volatile StatsMetadata sstableMetadata ; public final SerializationHeader header ; protected final AtomicLong keyCacheHit = new AtomicLong ( <int> ) ; protected final AtomicLong keyCacheRequest = new AtomicLong ( <int> ) ; private final InstanceTidier tidy = new InstanceTidier ( descriptor , metadata ) ; private final Ref < SSTableReader > selfRef = new Ref < > ( this , tidy ) ; private RestorableMeter readMeter ; private volatile double crcCheckChance ; public static long getApproximateKeyCount ( Iterable < SSTableReader > sstables ) { long count = - <int> ; boolean cardinalityAvailable = ! Iterables . isEmpty ( sstables ) & & Iterables . all ( sstables , new Predicate < SSTableReader > ( ) { public boolean apply ( SSTableReader sstable ) { return sstable . descriptor . version . hasNewStatsFile ( ) ; } } ) ; if ( cardinalityAvailable ) { boolean failed = false ; ICardinality cardinality = null ; for ( SSTableReader sstable : sstables ) { if ( sstable . openReason = = OpenReason . EARLY ) continue ; try { CompactionMetadata metadata = ( CompactionMetadata ) sstable . descriptor . getMetadataSerializer ( ) . deserialize ( sstable . descriptor , MetadataType . COMPACTION ) ; assert metadata ! = null : sstable . getFilename ( ) ; if ( cardinality = = null ) cardinality = metadata . cardinalityEstimator ; else cardinality = cardinality . merge ( metadata . cardinalityEstimator ) ; } catch ( IOException e ) { logger . warn ( <str> , e ) ; failed = true ; break ; } catch ( CardinalityMergeException e ) { logger . warn ( <str> , e ) ; failed = true ; break ; } } if ( cardinality ! = null & & ! failed ) count = cardinality . cardinality ( ) ; } if ( count < <int> ) { for ( SSTableReader sstable : sstables ) count + = sstable . estimatedKeys ( ) ; } return count ; } public static double estimateCompactionGain ( Set < SSTableReader > overlapping ) { Set < ICardinality > cardinalities = new HashSet < > ( overlapping . size ( ) ) ; for ( SSTableReader sstable : overlapping ) { try { ICardinality cardinality = ( ( CompactionMetadata ) sstable . descriptor . getMetadataSerializer ( ) . deserialize ( sstable . descriptor , MetadataType . COMPACTION ) ) . cardinalityEstimator ; if ( cardinality ! = null ) cardinalities . add ( cardinality ) ; else logger . trace ( <str> , sstable . getFilename ( ) ) ; } catch ( IOException e ) { logger . warn ( <str> , sstable , e ) ; } } long totalKeyCountBefore = <int> ; for ( ICardinality cardinality : cardinalities ) { totalKeyCountBefore + = cardinality . cardinality ( ) ; } if ( totalKeyCountBefore = = <int> ) return <int> ; long totalKeyCountAfter = mergeCardinalities ( cardinalities ) . cardinality ( ) ; logger . trace ( <str> , totalKeyCountAfter , totalKeyCountBefore , ( ( double ) totalKeyCountAfter ) / totalKeyCountBefore ) ; return ( ( double ) totalKeyCountAfter ) / totalKeyCountBefore ; } private static ICardinality mergeCardinalities ( Collection < ICardinality > cardinalities ) { ICardinality base = new HyperLogLogPlus ( <int> , <int> ) ; try { base = base . merge ( cardinalities . toArray ( new ICardinality [ cardinalities . size ( ) ] ) ) ; } catch ( CardinalityMergeException e ) { logger . warn ( <str> , e ) ; } return base ; } public static SSTableReader open ( Descriptor descriptor ) throws IOException { CFMetaData metadata ; if ( descriptor . cfname . contains ( SECONDARY_INDEX_NAME_SEPARATOR ) ) { int i = descriptor . cfname . indexOf ( SECONDARY_INDEX_NAME_SEPARATOR ) ; String parentName = descriptor . cfname . substring ( <int> , i ) ; String indexName = descriptor . cfname . substring ( i + <int> ) ; CFMetaData parent = Schema . instance . getCFMetaData ( descriptor . ksname , parentName ) ; IndexMetadata def = parent . getIndexes ( ) . get ( indexName ) . orElseThrow ( ( ) - > new AssertionError ( <str> + i ) ) ; metadata = CassandraIndex . indexCfsMetadata ( parent , def ) ; } else { metadata = Schema . instance . getCFMetaData ( descriptor . ksname , descriptor . cfname ) ; } return open ( descriptor , metadata ) ; } public static SSTableReader open ( Descriptor desc , CFMetaData metadata ) throws IOException { return open ( desc , componentsFor ( desc ) , metadata ) ; } public static SSTableReader open ( Descriptor descriptor , Set < Component > components , CFMetaData metadata ) throws IOException { return open ( descriptor , components , metadata , true , true ) ; } public static SSTableReader openNoValidation ( Descriptor descriptor , Set < Component > components , ColumnFamilyStore cfs ) throws IOException { return open ( descriptor , components , cfs . metadata , false , false ) ; } public static SSTableReader openNoValidation ( Descriptor descriptor , CFMetaData metadata ) throws IOException { return open ( descriptor , componentsFor ( descriptor ) , metadata , false , false ) ; } public static SSTableReader openForBatch ( Descriptor descriptor , Set < Component > components , CFMetaData metadata ) throws IOException { assert components . contains ( Component . DATA ) : <str> + descriptor ; assert components . contains ( Component . PRIMARY_INDEX ) : <str> + descriptor ; EnumSet < MetadataType > types = EnumSet . of ( MetadataType . VALIDATION , MetadataType . STATS , MetadataType . HEADER ) ; Map < MetadataType , MetadataComponent > sstableMetadata = descriptor . getMetadataSerializer ( ) . deserialize ( descriptor , types ) ; ValidationMetadata validationMetadata = ( ValidationMetadata ) sstableMetadata . get ( MetadataType . VALIDATION ) ; StatsMetadata statsMetadata = ( StatsMetadata ) sstableMetadata . get ( MetadataType . STATS ) ; SerializationHeader . Component header = ( SerializationHeader . Component ) sstableMetadata . get ( MetadataType . HEADER ) ; String partitionerName = metadata . partitioner . getClass ( ) . getCanonicalName ( ) ; if ( validationMetadata ! = null & & ! partitionerName . equals ( validationMetadata . partitioner ) ) { logger . error ( String . format ( <str> , descriptor , validationMetadata . partitioner , partitionerName ) ) ; System . exit ( <int> ) ; } logger . debug ( <str> , descriptor , new File ( descriptor . filenameFor ( Component . DATA ) ) . length ( ) ) ; SSTableReader sstable = internalOpen ( descriptor , components , metadata , System . currentTimeMillis ( ) , statsMetadata , OpenReason . NORMAL , header . toHeader ( metadata ) ) ; try ( SegmentedFile . Builder ibuilder = new BufferedSegmentedFile . Builder ( ) ; SegmentedFile . Builder dbuilder = sstable . compression ? new CompressedSegmentedFile . Builder ( null ) : new BufferedSegmentedFile . Builder ( ) ) { if ( ! sstable . loadSummary ( ibuilder , dbuilder ) ) sstable . buildSummary ( false , ibuilder , dbuilder , false , Downsampling . BASE_SAMPLING_LEVEL ) ; sstable . ifile = ibuilder . buildIndex ( sstable . descriptor , sstable . indexSummary ) ; sstable . dfile = dbuilder . buildData ( sstable . descriptor , statsMetadata ) ; sstable . bf = FilterFactory . AlwaysPresent ; sstable . setup ( false ) ; return sstable ; } } public static SSTableReader open ( Descriptor descriptor , Set < Component > components , CFMetaData metadata , boolean validate , boolean trackHotness ) throws IOException { assert components . contains ( Component . DATA ) : <str> + descriptor ; assert ! validate | | components . contains ( Component . PRIMARY_INDEX ) : <str> + descriptor ; assert ! descriptor . version . storeRows ( ) | | components . contains ( Component . STATS ) : <str> + descriptor ; EnumSet < MetadataType > types = EnumSet . of ( MetadataType . VALIDATION , MetadataType . STATS , MetadataType . HEADER ) ; Map < MetadataType , MetadataComponent > sstableMetadata = descriptor . getMetadataSerializer ( ) . deserialize ( descriptor , types ) ; ValidationMetadata validationMetadata = ( ValidationMetadata ) sstableMetadata . get ( MetadataType . VALIDATION ) ; StatsMetadata statsMetadata = ( StatsMetadata ) sstableMetadata . get ( MetadataType . STATS ) ; SerializationHeader . Component header = ( SerializationHeader . Component ) sstableMetadata . get ( MetadataType . HEADER ) ; assert ! descriptor . version . storeRows ( ) | | header ! = null ; String partitionerName = metadata . partitioner . getClass ( ) . getCanonicalName ( ) ; if ( validationMetadata ! = null & & ! partitionerName . equals ( validationMetadata . partitioner ) ) { logger . error ( String . format ( <str> , descriptor , validationMetadata . partitioner , partitionerName ) ) ; System . exit ( <int> ) ; } logger . debug ( <str> , descriptor , new File ( descriptor . filenameFor ( Component . DATA ) ) . length ( ) ) ; SSTableReader sstable = internalOpen ( descriptor , components , metadata , System . currentTimeMillis ( ) , statsMetadata , OpenReason . NORMAL , header = = null ? null : header . toHeader ( metadata ) ) ; try { long start = System . nanoTime ( ) ; sstable . load ( validationMetadata ) ; logger . trace ( <str> , descriptor , TimeUnit . NANOSECONDS . toMillis ( System . nanoTime ( ) - start ) ) ; sstable . setup ( trackHotness ) ; if ( validate ) sstable . validate ( ) ; if ( sstable . getKeyCache ( ) ! = null ) logger . trace ( <str> , sstable . getKeyCache ( ) . size ( ) , sstable . getKeyCache ( ) . getCapacity ( ) ) ; return sstable ; } catch ( Throwable t ) { sstable . selfRef ( ) . release ( ) ; throw t ; } } public static void logOpenException ( Descriptor descriptor , IOException e ) { if ( e instanceof FileNotFoundException ) logger . error ( <str> , descriptor , e . getMessage ( ) ) ; else logger . error ( <str> , descriptor , e ) ; } public static Collection < SSTableReader > openAll ( Set < Map . Entry < Descriptor , Set < Component > > > entries , final CFMetaData metadata ) { final Collection < SSTableReader > sstables = new LinkedBlockingQueue < > ( ) ; ExecutorService executor = DebuggableThreadPoolExecutor . createWithFixedPoolSize ( <str> , FBUtilities . getAvailableProcessors ( ) ) ; for ( final Map . Entry < Descriptor , Set < Component > > entry : entries ) { Runnable runnable = new Runnable ( ) { public void run ( ) { SSTableReader sstable ; try { sstable = open ( entry . getKey ( ) , entry . getValue ( ) , metadata ) ; } catch ( CorruptSSTableException ex ) { FileUtils . handleCorruptSSTable ( ex ) ; logger . error ( <str> , entry , ex ) ; return ; } catch ( FSError ex ) { FileUtils . handleFSError ( ex ) ; logger . error ( <str> , entry , ex ) ; return ; } catch ( IOException ex ) { logger . error ( <str> , entry , ex ) ; return ; } sstables . add ( sstable ) ; } } ; executor . submit ( runnable ) ; } executor . shutdown ( ) ; try { executor . awaitTermination ( <int> , TimeUnit . DAYS ) ; } catch ( InterruptedException e ) { throw new AssertionError ( e ) ; } return sstables ; } public static SSTableReader internalOpen ( Descriptor desc , Set < Component > components , CFMetaData metadata , SegmentedFile ifile , SegmentedFile dfile , IndexSummary isummary , IFilter bf , long maxDataAge , StatsMetadata sstableMetadata , OpenReason openReason , SerializationHeader header ) { assert desc ! = null & & ifile ! = null & & dfile ! = null & & isummary ! = null & & bf ! = null & & sstableMetadata ! = null ; SSTableReader reader = internalOpen ( desc , components , metadata , maxDataAge , sstableMetadata , openReason , header ) ; reader . bf = bf ; reader . ifile = ifile ; reader . dfile = dfile ; reader . indexSummary = isummary ; reader . setup ( true ) ; return reader ; } private static SSTableReader internalOpen ( final Descriptor descriptor , Set < Component > components , CFMetaData metadata , Long maxDataAge , StatsMetadata sstableMetadata , OpenReason openReason , SerializationHeader header ) { Factory readerFactory = descriptor . getFormat ( ) . getReaderFactory ( ) ; return readerFactory . open ( descriptor , components , metadata , maxDataAge , sstableMetadata , openReason , header ) ; } protected SSTableReader ( final Descriptor desc , Set < Component > components , CFMetaData metadata , long maxDataAge , StatsMetadata sstableMetadata , OpenReason openReason , SerializationHeader header ) { super ( desc , components , metadata ) ; this . sstableMetadata = sstableMetadata ; this . header = header ; this . maxDataAge = maxDataAge ; this . openReason = openReason ; this . rowIndexEntrySerializer = descriptor . version . getSSTableFormat ( ) . getIndexSerializer ( metadata , desc . version , header ) ; } public static long getTotalBytes ( Iterable < SSTableReader > sstables ) { long sum = <int> ; for ( SSTableReader sstable : sstables ) sum + = sstable . onDiskLength ( ) ; return sum ; } public static long getTotalUncompressedBytes ( Iterable < SSTableReader > sstables ) { long sum = <int> ; for ( SSTableReader sstable : sstables ) sum + = sstable . uncompressedLength ( ) ; return sum ; } public boolean equals ( Object that ) { return that instanceof SSTableReader & & ( ( SSTableReader ) that ) . descriptor . equals ( this . descriptor ) ; } public int hashCode ( ) { return this . descriptor . hashCode ( ) ; } public String getFilename ( ) { return dfile . path ( ) ; } public void setupOnline ( ) { keyCache = CacheService . instance . keyCache ; final ColumnFamilyStore cfs = Schema . instance . getColumnFamilyStoreInstance ( metadata . cfId ) ; if ( cfs ! = null ) setCrcCheckChance ( cfs . getCrcCheckChance ( ) ) ; } public boolean isKeyCacheSetup ( ) { return keyCache ! = null ; } private void load ( ValidationMetadata validation ) throws IOException { if ( metadata . params . bloomFilterFpChance = = <float> ) { load ( false , true ) ; bf = FilterFactory . AlwaysPresent ; } else if ( ! components . contains ( Component . PRIMARY_INDEX ) ) { load ( false , false ) ; } else if ( ! components . contains ( Component . FILTER ) | | validation = = null ) { load ( true , true ) ; } else if ( validation . bloomFilterFPChance ! = metadata . params . bloomFilterFpChance ) { load ( true , true ) ; } else { load ( false , true ) ; loadBloomFilter ( descriptor . version . hasOldBfHashOrder ( ) ) ; } } private void loadBloomFilter ( boolean oldBfHashOrder ) throws IOException { try ( DataInputStream stream = new DataInputStream ( new BufferedInputStream ( new FileInputStream ( descriptor . filenameFor ( Component . FILTER ) ) ) ) ) { bf = FilterFactory . deserialize ( stream , true , oldBfHashOrder ) ; } } private void load ( boolean recreateBloomFilter , boolean saveSummaryIfCreated ) throws IOException { try ( SegmentedFile . Builder ibuilder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) , false ) ; SegmentedFile . Builder dbuilder = SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) , compression ) ) { boolean summaryLoaded = loadSummary ( ibuilder , dbuilder ) ; boolean builtSummary = false ; if ( recreateBloomFilter | | ! summaryLoaded ) { buildSummary ( recreateBloomFilter , ibuilder , dbuilder , summaryLoaded , Downsampling . BASE_SAMPLING_LEVEL ) ; builtSummary = true ; } if ( components . contains ( Component . PRIMARY_INDEX ) ) ifile = ibuilder . buildIndex ( descriptor , indexSummary ) ; dfile = dbuilder . buildData ( descriptor , sstableMetadata ) ; if ( ! descriptor . version . hasSamplingLevel ( ) & & ! builtSummary & & ! validateSummarySamplingLevel ( ) & & ifile ! = null ) { indexSummary . close ( ) ; ifile . close ( ) ; dfile . close ( ) ; logger . info ( <str> ) ; FileUtils . deleteWithConfirm ( new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ) ; try ( SegmentedFile . Builder ibuilderRebuild = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) , false ) ; SegmentedFile . Builder dbuilderRebuild = SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) , compression ) ) { buildSummary ( false , ibuilderRebuild , dbuilderRebuild , false , Downsampling . BASE_SAMPLING_LEVEL ) ; ifile = ibuilderRebuild . buildIndex ( descriptor , indexSummary ) ; dfile = dbuilderRebuild . buildData ( descriptor , sstableMetadata ) ; saveSummary ( ibuilderRebuild , dbuilderRebuild ) ; } } else if ( saveSummaryIfCreated & & builtSummary ) { saveSummary ( ibuilder , dbuilder ) ; } } catch ( Throwable t ) { if ( ifile ! = null ) { ifile . close ( ) ; ifile = null ; } if ( dfile ! = null ) { dfile . close ( ) ; dfile = null ; } if ( indexSummary ! = null ) { indexSummary . close ( ) ; indexSummary = null ; } throw t ; } } private void buildSummary ( boolean recreateBloomFilter , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , boolean summaryLoaded , int samplingLevel ) throws IOException { if ( ! components . contains ( Component . PRIMARY_INDEX ) ) return ; try ( RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY_INDEX ) ) ) ) { long indexSize = primaryIndex . length ( ) ; long histogramCount = sstableMetadata . estimatedPartitionSize . count ( ) ; long estimatedKeys = histogramCount > <int> & & ! sstableMetadata . estimatedPartitionSize . isOverflowed ( ) ? histogramCount : estimateRowsFromIndex ( primaryIndex ) ; if ( recreateBloomFilter ) bf = FilterFactory . getFilter ( estimatedKeys , metadata . params . bloomFilterFpChance , true , descriptor . version . hasOldBfHashOrder ( ) ) ; try ( IndexSummaryBuilder summaryBuilder = summaryLoaded ? null : new IndexSummaryBuilder ( estimatedKeys , metadata . params . minIndexInterval , samplingLevel ) ) { long indexPosition ; RowIndexEntry . IndexSerializer rowIndexSerializer = descriptor . getFormat ( ) . getIndexSerializer ( metadata , descriptor . version , header ) ; while ( ( indexPosition = primaryIndex . getFilePointer ( ) ) ! = indexSize ) { ByteBuffer key = ByteBufferUtil . readWithShortLength ( primaryIndex ) ; RowIndexEntry indexEntry = rowIndexSerializer . deserialize ( primaryIndex ) ; DecoratedKey decoratedKey = decorateKey ( key ) ; if ( first = = null ) first = decoratedKey ; last = decoratedKey ; if ( recreateBloomFilter ) bf . add ( decoratedKey ) ; if ( ! summaryLoaded ) { summaryBuilder . maybeAddEntry ( decoratedKey , indexPosition ) ; } } if ( ! summaryLoaded ) indexSummary = summaryBuilder . build ( getPartitioner ( ) ) ; } } first = getMinimalKey ( first ) ; last = getMinimalKey ( last ) ; } @SuppressWarnings ( <str> ) public boolean loadSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder ) { File summariesFile = new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ; if ( ! summariesFile . exists ( ) ) return false ; DataInputStream iStream = null ; try { iStream = new DataInputStream ( new FileInputStream ( summariesFile ) ) ; indexSummary = IndexSummary . serializer . deserialize ( iStream , getPartitioner ( ) , descriptor . version . hasSamplingLevel ( ) , metadata . params . minIndexInterval , metadata . params . maxIndexInterval ) ; first = decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; last = decorateKey ( ByteBufferUtil . readWithLength ( iStream ) ) ; ibuilder . deserializeBounds ( iStream , descriptor . version ) ; dbuilder . deserializeBounds ( iStream , descriptor . version ) ; } catch ( IOException e ) { if ( indexSummary ! = null ) indexSummary . close ( ) ; logger . trace ( <str> , summariesFile . getPath ( ) , e . getMessage ( ) ) ; FileUtils . closeQuietly ( iStream ) ; FileUtils . deleteWithConfirm ( summariesFile ) ; return false ; } finally { FileUtils . closeQuietly ( iStream ) ; } return true ; } private boolean validateSummarySamplingLevel ( ) { if ( ifile = = null ) return false ; int i = <int> ; int summaryEntriesChecked = <int> ; int expectedIndexInterval = getMinIndexInterval ( ) ; String path = null ; try ( FileDataInput in = ifile . createReader ( <int> ) ) { path = in . getPath ( ) ; while ( ! in . isEOF ( ) ) { ByteBuffer indexKey = ByteBufferUtil . readWithShortLength ( in ) ; if ( i % expectedIndexInterval = = <int> ) { ByteBuffer summaryKey = ByteBuffer . wrap ( indexSummary . getKey ( i / expectedIndexInterval ) ) ; if ( ! summaryKey . equals ( indexKey ) ) return false ; summaryEntriesChecked + + ; if ( summaryEntriesChecked = = Downsampling . BASE_SAMPLING_LEVEL ) return true ; } RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; i + + ; } } catch ( IOException e ) { markSuspect ( ) ; throw new CorruptSSTableException ( e , path ) ; } return true ; } public void saveSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder ) { saveSummary ( this . descriptor , this . first , this . last , ibuilder , dbuilder , indexSummary ) ; } private void saveSummary ( SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , IndexSummary newSummary ) { saveSummary ( this . descriptor , this . first , this . last , ibuilder , dbuilder , newSummary ) ; } public static void saveSummary ( Descriptor descriptor , DecoratedKey first , DecoratedKey last , SegmentedFile . Builder ibuilder , SegmentedFile . Builder dbuilder , IndexSummary summary ) { File summariesFile = new File ( descriptor . filenameFor ( Component . SUMMARY ) ) ; if ( summariesFile . exists ( ) ) FileUtils . deleteWithConfirm ( summariesFile ) ; try ( DataOutputStreamPlus oStream = new BufferedDataOutputStreamPlus ( new FileOutputStream ( summariesFile ) ) ; ) { IndexSummary . serializer . serialize ( summary , oStream , descriptor . version . hasSamplingLevel ( ) ) ; ByteBufferUtil . writeWithLength ( first . getKey ( ) , oStream ) ; ByteBufferUtil . writeWithLength ( last . getKey ( ) , oStream ) ; ibuilder . serializeBounds ( oStream , descriptor . version ) ; dbuilder . serializeBounds ( oStream , descriptor . version ) ; } catch ( IOException e ) { logger . trace ( <str> , e ) ; if ( summariesFile . exists ( ) ) FileUtils . deleteWithConfirm ( summariesFile ) ; } } public void setReplaced ( ) { synchronized ( tidy . global ) { assert ! tidy . isReplaced ; tidy . isReplaced = true ; } } public boolean isReplaced ( ) { synchronized ( tidy . global ) { return tidy . isReplaced ; } } public void runOnClose ( final Runnable runOnClose ) { synchronized ( tidy . global ) { final Runnable existing = tidy . runOnClose ; tidy . runOnClose = AndThen . get ( existing , runOnClose ) ; } } private static class AndThen implements Runnable { final Runnable runFirst ; final Runnable runSecond ; private AndThen ( Runnable runFirst , Runnable runSecond ) { this . runFirst = runFirst ; this . runSecond = runSecond ; } public void run ( ) { runFirst . run ( ) ; runSecond . run ( ) ; } static Runnable get ( Runnable runFirst , Runnable runSecond ) { if ( runFirst = = null ) return runSecond ; return new AndThen ( runFirst , runSecond ) ; } } private SSTableReader cloneAndReplace ( DecoratedKey newFirst , OpenReason reason ) { return cloneAndReplace ( newFirst , reason , indexSummary . sharedCopy ( ) ) ; } private SSTableReader cloneAndReplace ( DecoratedKey newFirst , OpenReason reason , IndexSummary newSummary ) { SSTableReader replacement = internalOpen ( descriptor , components , metadata , ifile ! = null ? ifile . sharedCopy ( ) : null , dfile . sharedCopy ( ) , newSummary , bf . sharedCopy ( ) , maxDataAge , sstableMetadata , reason , header ) ; replacement . first = newFirst ; replacement . last = last ; replacement . isSuspect . set ( isSuspect . get ( ) ) ; return replacement ; } public SSTableReader cloneWithRestoredStart ( DecoratedKey restoredStart ) { synchronized ( tidy . global ) { return cloneAndReplace ( restoredStart , OpenReason . NORMAL ) ; } } public SSTableReader cloneWithNewStart ( DecoratedKey newStart , final Runnable runOnClose ) { synchronized ( tidy . global ) { assert openReason ! = OpenReason . EARLY ; if ( newStart . compareTo ( first ) > <int> ) { final long dataStart = getPosition ( newStart , Operator . EQ ) . position ; final long indexStart = getIndexScanPosition ( newStart ) ; this . tidy . runOnClose = new DropPageCache ( dfile , dataStart , ifile , indexStart , runOnClose ) ; } return cloneAndReplace ( newStart , OpenReason . MOVED_START ) ; } } private static class DropPageCache implements Runnable { final SegmentedFile dfile ; final long dfilePosition ; final SegmentedFile ifile ; final long ifilePosition ; final Runnable andThen ; private DropPageCache ( SegmentedFile dfile , long dfilePosition , SegmentedFile ifile , long ifilePosition , Runnable andThen ) { this . dfile = dfile ; this . dfilePosition = dfilePosition ; this . ifile = ifile ; this . ifilePosition = ifilePosition ; this . andThen = andThen ; } public void run ( ) { dfile . dropPageCache ( dfilePosition ) ; if ( ifile ! = null ) ifile . dropPageCache ( ifilePosition ) ; andThen . run ( ) ; } } @SuppressWarnings ( <str> ) public SSTableReader cloneWithNewSummarySamplingLevel ( ColumnFamilyStore parent , int samplingLevel ) throws IOException { assert descriptor . version . hasSamplingLevel ( ) ; synchronized ( tidy . global ) { assert openReason ! = OpenReason . EARLY ; int minIndexInterval = metadata . params . minIndexInterval ; int maxIndexInterval = metadata . params . maxIndexInterval ; double effectiveInterval = indexSummary . getEffectiveIndexInterval ( ) ; IndexSummary newSummary ; long oldSize = bytesOnDisk ( ) ; if ( samplingLevel > indexSummary . getSamplingLevel ( ) | | indexSummary . getMinIndexInterval ( ) ! = minIndexInterval | | effectiveInterval > maxIndexInterval ) { newSummary = buildSummaryAtLevel ( samplingLevel ) ; } else if ( samplingLevel < indexSummary . getSamplingLevel ( ) ) { newSummary = IndexSummaryBuilder . downsample ( indexSummary , samplingLevel , minIndexInterval , getPartitioner ( ) ) ; try ( SegmentedFile . Builder ibuilder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) , false ) ; SegmentedFile . Builder dbuilder = SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) , compression ) ) { saveSummary ( ibuilder , dbuilder , newSummary ) ; } } else { throw new AssertionError ( <str> + <str> ) ; } long newSize = bytesOnDisk ( ) ; StorageMetrics . load . inc ( newSize - oldSize ) ; parent . metric . liveDiskSpaceUsed . inc ( newSize - oldSize ) ; parent . metric . totalDiskSpaceUsed . inc ( newSize - oldSize ) ; return cloneAndReplace ( first , OpenReason . METADATA_CHANGE , newSummary ) ; } } private IndexSummary buildSummaryAtLevel ( int newSamplingLevel ) throws IOException { RandomAccessReader primaryIndex = RandomAccessReader . open ( new File ( descriptor . filenameFor ( Component . PRIMARY_INDEX ) ) ) ; try { long indexSize = primaryIndex . length ( ) ; try ( IndexSummaryBuilder summaryBuilder = new IndexSummaryBuilder ( estimatedKeys ( ) , metadata . params . minIndexInterval , newSamplingLevel ) ) { long indexPosition ; while ( ( indexPosition = primaryIndex . getFilePointer ( ) ) ! = indexSize ) { summaryBuilder . maybeAddEntry ( decorateKey ( ByteBufferUtil . readWithShortLength ( primaryIndex ) ) , indexPosition ) ; RowIndexEntry . Serializer . skip ( primaryIndex , descriptor . version ) ; } return summaryBuilder . build ( getPartitioner ( ) ) ; } } finally { FileUtils . closeQuietly ( primaryIndex ) ; } } public RestorableMeter getReadMeter ( ) { return readMeter ; } public int getIndexSummarySamplingLevel ( ) { return indexSummary . getSamplingLevel ( ) ; } public long getIndexSummaryOffHeapSize ( ) { return indexSummary . getOffHeapSize ( ) ; } public int getMinIndexInterval ( ) { return indexSummary . getMinIndexInterval ( ) ; } public double getEffectiveIndexInterval ( ) { return indexSummary . getEffectiveIndexInterval ( ) ; } public void releaseSummary ( ) { tidy . releaseSummary ( ) ; indexSummary = null ; } private void validate ( ) { if ( this . first . compareTo ( this . last ) > <int> ) { throw new IllegalStateException ( String . format ( <str> , this . first , this . last ) ) ; } } public long getIndexScanPosition ( PartitionPosition key ) { if ( openReason = = OpenReason . MOVED_START & & key . compareTo ( first ) < <int> ) key = first ; return getIndexScanPositionFromBinarySearchResult ( indexSummary . binarySearch ( key ) , indexSummary ) ; } @VisibleForTesting public static long getIndexScanPositionFromBinarySearchResult ( int binarySearchResult , IndexSummary referencedIndexSummary ) { if ( binarySearchResult = = - <int> ) return <int> ; else return referencedIndexSummary . getPosition ( getIndexSummaryIndexFromBinarySearchResult ( binarySearchResult ) ) ; } public static int getIndexSummaryIndexFromBinarySearchResult ( int binarySearchResult ) { if ( binarySearchResult < <int> ) { int greaterThan = ( binarySearchResult + <int> ) * - <int> ; if ( greaterThan = = <int> ) return - <int> ; return greaterThan - <int> ; } else { return binarySearchResult ; } } public CompressionMetadata getCompressionMetadata ( ) { if ( ! compression ) throw new IllegalStateException ( this + <str> ) ; return ( ( ICompressedFile ) dfile ) . getMetadata ( ) ; } public long getCompressionMetadataOffHeapSize ( ) { if ( ! compression ) return <int> ; return getCompressionMetadata ( ) . offHeapSize ( ) ; } public void forceFilterFailures ( ) { bf = FilterFactory . AlwaysPresent ; } public IFilter getBloomFilter ( ) { return bf ; } public long getBloomFilterSerializedSize ( ) { return bf . serializedSize ( ) ; } public long getBloomFilterOffHeapSize ( ) { return bf . offHeapSize ( ) ; } public long estimatedKeys ( ) { return indexSummary . getEstimatedKeyCount ( ) ; } public long estimatedKeysForRanges ( Collection < Range < Token > > ranges ) { long sampleKeyCount = <int> ; List < Pair < Integer , Integer > > sampleIndexes = getSampleIndexesForRanges ( indexSummary , ranges ) ; for ( Pair < Integer , Integer > sampleIndexRange : sampleIndexes ) sampleKeyCount + = ( sampleIndexRange . right - sampleIndexRange . left + <int> ) ; long estimatedKeys = sampleKeyCount * ( ( long ) Downsampling . BASE_SAMPLING_LEVEL * indexSummary . getMinIndexInterval ( ) ) / indexSummary . getSamplingLevel ( ) ; return Math . max ( <int> , estimatedKeys ) ; } public int getIndexSummarySize ( ) { return indexSummary . size ( ) ; } public int getMaxIndexSummarySize ( ) { return indexSummary . getMaxNumberOfEntries ( ) ; } public byte [ ] getIndexSummaryKey ( int index ) { return indexSummary . getKey ( index ) ; } private static List < Pair < Integer , Integer > > getSampleIndexesForRanges ( IndexSummary summary , Collection < Range < Token > > ranges ) { List < Pair < Integer , Integer > > positions = new ArrayList < > ( ) ; for ( Range < Token > range : Range . normalize ( ranges ) ) { PartitionPosition leftPosition = range . left . maxKeyBound ( ) ; PartitionPosition rightPosition = range . right . maxKeyBound ( ) ; int left = summary . binarySearch ( leftPosition ) ; if ( left < <int> ) left = ( left + <int> ) * - <int> ; else left = left + <int> ; if ( left = = summary . size ( ) ) continue ; int right = Range . isWrapAround ( range . left , range . right ) ? summary . size ( ) - <int> : summary . binarySearch ( rightPosition ) ; if ( right < <int> ) { right = ( right + <int> ) * - <int> ; if ( right = = <int> ) continue ; right - - ; } if ( left > right ) continue ; positions . add ( Pair . create ( left , right ) ) ; } return positions ; } public Iterable < DecoratedKey > getKeySamples ( final Range < Token > range ) { final List < Pair < Integer , Integer > > indexRanges = getSampleIndexesForRanges ( indexSummary , Collections . singletonList ( range ) ) ; if ( indexRanges . isEmpty ( ) ) return Collections . emptyList ( ) ; return new Iterable < DecoratedKey > ( ) { public Iterator < DecoratedKey > iterator ( ) { return new Iterator < DecoratedKey > ( ) { private Iterator < Pair < Integer , Integer > > rangeIter = indexRanges . iterator ( ) ; private Pair < Integer , Integer > current ; private int idx ; public boolean hasNext ( ) { if ( current = = null | | idx > current . right ) { if ( rangeIter . hasNext ( ) ) { current = rangeIter . next ( ) ; idx = current . left ; return true ; } return false ; } return true ; } public DecoratedKey next ( ) { byte [ ] bytes = indexSummary . getKey ( idx + + ) ; return decorateKey ( ByteBuffer . wrap ( bytes ) ) ; } public void remove ( ) { throw new UnsupportedOperationException ( ) ; } } ; } } ; } public List < Pair < Long , Long > > getPositionsForRanges ( Collection < Range < Token > > ranges ) { List < Pair < Long , Long > > positions = new ArrayList < > ( ) ; for ( Range < Token > range : Range . normalize ( ranges ) ) { assert ! range . isWrapAround ( ) | | range . right . isMinimum ( ) ; AbstractBounds < PartitionPosition > bounds = Range . makeRowRange ( range ) ; PartitionPosition leftBound = bounds . left . compareTo ( first ) > <int> ? bounds . left : first . getToken ( ) . minKeyBound ( ) ; PartitionPosition rightBound = bounds . right . isMinimum ( ) ? last . getToken ( ) . maxKeyBound ( ) : bounds . right ; if ( leftBound . compareTo ( last ) > <int> | | rightBound . compareTo ( first ) < <int> ) continue ; long left = getPosition ( leftBound , Operator . GT ) . position ; long right = ( rightBound . compareTo ( last ) > <int> ) ? uncompressedLength ( ) : getPosition ( rightBound , Operator . GT ) . position ; if ( left = = right ) continue ; assert left < right : String . format ( <str> , range , openReason , first , last , left , right ) ; positions . add ( Pair . create ( left , right ) ) ; } return positions ; } public KeyCacheKey getCacheKey ( DecoratedKey key ) { return new KeyCacheKey ( metadata . ksAndCFName , descriptor , key . getKey ( ) ) ; } public void cacheKey ( DecoratedKey key , RowIndexEntry info ) { CachingParams caching = metadata . params . caching ; if ( ! caching . cacheKeys ( ) | | keyCache = = null | | keyCache . getCapacity ( ) = = <int> ) return ; KeyCacheKey cacheKey = new KeyCacheKey ( metadata . ksAndCFName , descriptor , key . getKey ( ) ) ; logger . trace ( <str> , cacheKey , info ) ; keyCache . put ( cacheKey , info ) ; } public RowIndexEntry getCachedPosition ( DecoratedKey key , boolean updateStats ) { return getCachedPosition ( new KeyCacheKey ( metadata . ksAndCFName , descriptor , key . getKey ( ) ) , updateStats ) ; } protected RowIndexEntry getCachedPosition ( KeyCacheKey unifiedKey , boolean updateStats ) { if ( keyCache ! = null & & keyCache . getCapacity ( ) > <int> & & metadata . params . caching . cacheKeys ( ) ) { if ( updateStats ) { RowIndexEntry cachedEntry = keyCache . get ( unifiedKey ) ; keyCacheRequest . incrementAndGet ( ) ; if ( cachedEntry ! = null ) { keyCacheHit . incrementAndGet ( ) ; bloomFilterTracker . addTruePositive ( ) ; } return cachedEntry ; } else { return keyCache . getInternal ( unifiedKey ) ; } } return null ; } public RowIndexEntry getPosition ( PartitionPosition key , Operator op ) { return getPosition ( key , op , true , false ) ; } public RowIndexEntry getPosition ( PartitionPosition key , Operator op , boolean updateCacheAndStats ) { return getPosition ( key , op , updateCacheAndStats , false ) ; } protected abstract RowIndexEntry getPosition ( PartitionPosition key , Operator op , boolean updateCacheAndStats , boolean permitMatchPastLast ) ; public abstract SliceableUnfilteredRowIterator iterator ( DecoratedKey key , ColumnFilter selectedColumns , boolean reversed , boolean isForThrift ) ; public abstract SliceableUnfilteredRowIterator iterator ( FileDataInput file , DecoratedKey key , RowIndexEntry indexEntry , ColumnFilter selectedColumns , boolean reversed , boolean isForThrift ) ; public DecoratedKey firstKeyBeyond ( PartitionPosition token ) { if ( token . compareTo ( first ) < <int> ) return first ; long sampledPosition = getIndexScanPosition ( token ) ; if ( ifile = = null ) return null ; String path = null ; try ( FileDataInput in = ifile . createReader ( sampledPosition ) ) { path = in . getPath ( ) ; while ( ! in . isEOF ( ) ) { ByteBuffer indexKey = ByteBufferUtil . readWithShortLength ( in ) ; DecoratedKey indexDecoratedKey = decorateKey ( indexKey ) ; if ( indexDecoratedKey . compareTo ( token ) > <int> ) return indexDecoratedKey ; RowIndexEntry . Serializer . skip ( in , descriptor . version ) ; } } catch ( IOException e ) { markSuspect ( ) ; throw new CorruptSSTableException ( e , path ) ; } return null ; } public long uncompressedLength ( ) { return dfile . length ; } public long onDiskLength ( ) { return dfile . onDiskLength ; } @VisibleForTesting public double getCrcCheckChance ( ) { return crcCheckChance ; } public void setCrcCheckChance ( double crcCheckChance ) { this . crcCheckChance = crcCheckChance ; if ( compression ) ( ( CompressedSegmentedFile ) dfile ) . metadata . parameters . setCrcCheckChance ( crcCheckChance ) ; } public void markObsolete ( Runnable tidier ) { if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , getFilename ( ) ) ; synchronized ( tidy . global ) { assert ! tidy . isReplaced ; assert tidy . global . obsoletion = = null : this + <str> ; tidy . global . obsoletion = tidier ; tidy . global . stopReadMeterPersistence ( ) ; } } public boolean isMarkedCompacted ( ) { return tidy . global . obsoletion ! = null ; } public void markSuspect ( ) { if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , getFilename ( ) ) ; isSuspect . getAndSet ( true ) ; } public boolean isMarkedSuspect ( ) { return isSuspect . get ( ) ; } public ISSTableScanner getScanner ( ) { return getScanner ( ( RateLimiter ) null ) ; } public ISSTableScanner getScanner ( ColumnFilter columns , DataRange dataRange , boolean isForThrift ) { return getScanner ( columns , dataRange , null , isForThrift ) ; } public ISSTableScanner getScanner ( Range < Token > range , RateLimiter limiter ) { if ( range = = null ) return getScanner ( limiter ) ; return getScanner ( Collections . singletonList ( range ) , limiter ) ; } public abstract ISSTableScanner getScanner ( RateLimiter limiter ) ; public abstract ISSTableScanner getScanner ( Collection < Range < Token > > ranges , RateLimiter limiter ) ; public abstract ISSTableScanner getScanner ( ColumnFilter columns , DataRange dataRange , RateLimiter limiter , boolean isForThrift ) ; public FileDataInput getFileDataInput ( long position ) { return dfile . createReader ( position ) ; } public boolean newSince ( long age ) { return maxDataAge > age ; } public void createLinks ( String snapshotDirectoryPath ) { for ( Component component : components ) { File sourceFile = new File ( descriptor . filenameFor ( component ) ) ; if ( ! sourceFile . exists ( ) ) continue ; File targetLink = new File ( snapshotDirectoryPath , sourceFile . getName ( ) ) ; FileUtils . createHardLink ( sourceFile , targetLink ) ; } } public boolean isRepaired ( ) { return sstableMetadata . repairedAt ! = ActiveRepairService . UNREPAIRED_SSTABLE ; } public abstract static class Operator { public static final Operator EQ = new Equals ( ) ; public static final Operator GE = new GreaterThanOrEqualTo ( ) ; public static final Operator GT = new GreaterThan ( ) ; public abstract int apply ( int comparison ) ; final static class Equals extends Operator { public int apply ( int comparison ) { return - comparison ; } } final static class GreaterThanOrEqualTo extends Operator { public int apply ( int comparison ) { return comparison > = <int> ? <int> : <int> ; } } final static class GreaterThan extends Operator { public int apply ( int comparison ) { return comparison > <int> ? <int> : <int> ; } } } public long getBloomFilterFalsePositiveCount ( ) { return bloomFilterTracker . getFalsePositiveCount ( ) ; } public long getRecentBloomFilterFalsePositiveCount ( ) { return bloomFilterTracker . getRecentFalsePositiveCount ( ) ; } public long getBloomFilterTruePositiveCount ( ) { return bloomFilterTracker . getTruePositiveCount ( ) ; } public long getRecentBloomFilterTruePositiveCount ( ) { return bloomFilterTracker . getRecentTruePositiveCount ( ) ; } public InstrumentingCache < KeyCacheKey , RowIndexEntry > getKeyCache ( ) { return keyCache ; } public EstimatedHistogram getEstimatedPartitionSize ( ) { return sstableMetadata . estimatedPartitionSize ; } public EstimatedHistogram getEstimatedColumnCount ( ) { return sstableMetadata . estimatedColumnCount ; } public double getEstimatedDroppableTombstoneRatio ( int gcBefore ) { return sstableMetadata . getEstimatedDroppableTombstoneRatio ( gcBefore ) ; } public double getDroppableTombstonesBefore ( int gcBefore ) { return sstableMetadata . getDroppableTombstonesBefore ( gcBefore ) ; } public double getCompressionRatio ( ) { return sstableMetadata . compressionRatio ; } public ReplayPosition getReplayPosition ( ) { return sstableMetadata . replayPosition ; } public long getMinTimestamp ( ) { return sstableMetadata . minTimestamp ; } public long getMaxTimestamp ( ) { return sstableMetadata . maxTimestamp ; } public int getMinLocalDeletionTime ( ) { return sstableMetadata . minLocalDeletionTime ; } public int getMaxLocalDeletionTime ( ) { return sstableMetadata . maxLocalDeletionTime ; } public int getMinTTL ( ) { return sstableMetadata . minTTL ; } public int getMaxTTL ( ) { return sstableMetadata . maxTTL ; } public long getTotalColumnsSet ( ) { return sstableMetadata . totalColumnsSet ; } public long getTotalRows ( ) { return sstableMetadata . totalRows ; } public int getAvgColumnSetPerRow ( ) { return sstableMetadata . totalRows < <int> ? - <int> : ( sstableMetadata . totalRows = = <int> ? <int> : ( int ) ( sstableMetadata . totalColumnsSet / sstableMetadata . totalRows ) ) ; } public int getSSTableLevel ( ) { return sstableMetadata . sstableLevel ; } public void reloadSSTableMetadata ( ) throws IOException { this . sstableMetadata = ( StatsMetadata ) descriptor . getMetadataSerializer ( ) . deserialize ( descriptor , MetadataType . STATS ) ; } public StatsMetadata getSSTableMetadata ( ) { return sstableMetadata ; } public RandomAccessReader openDataReader ( RateLimiter limiter ) { assert limiter ! = null ; return dfile . createReader ( limiter ) ; } public RandomAccessReader openDataReader ( ) { return dfile . createReader ( ) ; } public RandomAccessReader openIndexReader ( ) { if ( ifile ! = null ) return ifile . createReader ( ) ; return null ; } public ChannelProxy getDataChannel ( ) { return dfile . channel ; } public ChannelProxy getIndexChannel ( ) { return ifile . channel ; } public long getCreationTimeFor ( Component component ) { return new File ( descriptor . filenameFor ( component ) ) . lastModified ( ) ; } public long getKeyCacheHit ( ) { return keyCacheHit . get ( ) ; } public long getKeyCacheRequest ( ) { return keyCacheRequest . get ( ) ; } public void incrementReadCount ( ) { if ( readMeter ! = null ) readMeter . mark ( ) ; } public boolean mayOverlapsWith ( SSTableReader other ) { StatsMetadata m1 = getSSTableMetadata ( ) ; StatsMetadata m2 = other . getSSTableMetadata ( ) ; if ( m1 . minClusteringValues . isEmpty ( ) | | m1 . maxClusteringValues . isEmpty ( ) | | m2 . minClusteringValues . isEmpty ( ) | | m2 . maxClusteringValues . isEmpty ( ) ) return true ; return ! ( compare ( m1 . maxClusteringValues , m2 . minClusteringValues ) < <int> | | compare ( m1 . minClusteringValues , m2 . maxClusteringValues ) > <int> ) ; } private int compare ( List < ByteBuffer > values1 , List < ByteBuffer > values2 ) { ClusteringComparator comparator = metadata . comparator ; for ( int i = <int> ; i < Math . min ( values1 . size ( ) , values2 . size ( ) ) ; i + + ) { int cmp = comparator . subtype ( i ) . compare ( values1 . get ( i ) , values2 . get ( i ) ) ; if ( cmp ! = <int> ) return cmp ; } return <int> ; } public static class SizeComparator implements Comparator < SSTableReader > { public int compare ( SSTableReader o1 , SSTableReader o2 ) { return Longs . compare ( o1 . onDiskLength ( ) , o2 . onDiskLength ( ) ) ; } } public Ref < SSTableReader > tryRef ( ) { return selfRef . tryRef ( ) ; } public Ref < SSTableReader > selfRef ( ) { return selfRef ; } public Ref < SSTableReader > ref ( ) { return selfRef . ref ( ) ; } void setup ( boolean trackHotness ) { tidy . setup ( this , trackHotness ) ; this . readMeter = tidy . global . readMeter ; } @VisibleForTesting public void overrideReadMeter ( RestorableMeter readMeter ) { this . readMeter = tidy . global . readMeter = readMeter ; } public void addTo ( Ref . IdentityCollection identities ) { identities . add ( this ) ; identities . add ( tidy . globalRef ) ; dfile . addTo ( identities ) ; ifile . addTo ( identities ) ; bf . addTo ( identities ) ; indexSummary . addTo ( identities ) ; } private static final class InstanceTidier implements Tidy { private final Descriptor descriptor ; private final CFMetaData metadata ; private IFilter bf ; private IndexSummary summary ; private SegmentedFile dfile ; private SegmentedFile ifile ; private Runnable runOnClose ; private boolean isReplaced = false ; private Ref < GlobalTidy > globalRef ; private GlobalTidy global ; private boolean setup ; void setup ( SSTableReader reader , boolean trackHotness ) { this . setup = true ; this . bf = reader . bf ; this . summary = reader . indexSummary ; this . dfile = reader . dfile ; this . ifile = reader . ifile ; this . globalRef = GlobalTidy . get ( reader ) ; this . global = globalRef . get ( ) ; if ( trackHotness ) global . ensureReadMeter ( ) ; } InstanceTidier ( Descriptor descriptor , CFMetaData metadata ) { this . descriptor = descriptor ; this . metadata = metadata ; } public void tidy ( ) { if ( ! setup ) return ; final ColumnFamilyStore cfs = Schema . instance . getColumnFamilyStoreInstance ( metadata . cfId ) ; final OpOrder . Barrier barrier ; if ( cfs ! = null ) { barrier = cfs . readOrdering . newBarrier ( ) ; barrier . issue ( ) ; } else barrier = null ; ScheduledExecutors . nonPeriodicTasks . execute ( new Runnable ( ) { public void run ( ) { if ( barrier ! = null ) barrier . await ( ) ; if ( bf ! = null ) bf . close ( ) ; if ( summary ! = null ) summary . close ( ) ; if ( runOnClose ! = null ) runOnClose . run ( ) ; if ( dfile ! = null ) dfile . close ( ) ; if ( ifile ! = null ) ifile . close ( ) ; globalRef . release ( ) ; } } ) ; } public String name ( ) { return descriptor . toString ( ) ; } void releaseSummary ( ) { summary . close ( ) ; assert summary . isCleanedUp ( ) ; summary = null ; } } static final class GlobalTidy implements Tidy { static final ConcurrentMap < Descriptor , Ref < GlobalTidy > > lookup = new ConcurrentHashMap < > ( ) ; private final Descriptor desc ; private RestorableMeter readMeter ; private ScheduledFuture readMeterSyncFuture ; private volatile Runnable obsoletion ; GlobalTidy ( final SSTableReader reader ) { this . desc = reader . descriptor ; } void ensureReadMeter ( ) { if ( readMeter ! = null ) return ; if ( Schema . isSystemKeyspace ( desc . ksname ) ) { readMeter = null ; readMeterSyncFuture = null ; return ; } readMeter = SystemKeyspace . getSSTableReadMeter ( desc . ksname , desc . cfname , desc . generation ) ; readMeterSyncFuture = syncExecutor . scheduleAtFixedRate ( new Runnable ( ) { public void run ( ) { if ( obsoletion = = null ) { meterSyncThrottle . acquire ( ) ; SystemKeyspace . persistSSTableReadMeter ( desc . ksname , desc . cfname , desc . generation , readMeter ) ; } } } , <int> , <int> , TimeUnit . MINUTES ) ; } private void stopReadMeterPersistence ( ) { if ( readMeterSyncFuture ! = null ) { readMeterSyncFuture . cancel ( true ) ; readMeterSyncFuture = null ; } } public void tidy ( ) { lookup . remove ( desc ) ; if ( obsoletion ! = null ) obsoletion . run ( ) ; CLibrary . trySkipCache ( desc . filenameFor ( Component . DATA ) , <int> , <int> ) ; CLibrary . trySkipCache ( desc . filenameFor ( Component . PRIMARY_INDEX ) , <int> , <int> ) ; } public String name ( ) { return desc . toString ( ) ; } @SuppressWarnings ( <str> ) public static Ref < GlobalTidy > get ( SSTableReader sstable ) { Descriptor descriptor = sstable . descriptor ; Ref < GlobalTidy > refc = lookup . get ( descriptor ) ; if ( refc ! = null ) return refc . ref ( ) ; final GlobalTidy tidy = new GlobalTidy ( sstable ) ; refc = new Ref < > ( tidy , tidy ) ; Ref < ? > ex = lookup . putIfAbsent ( descriptor , refc ) ; if ( ex ! = null ) { refc . close ( ) ; throw new AssertionError ( ) ; } return refc ; } } @VisibleForTesting public static void resetTidying ( ) { GlobalTidy . lookup . clear ( ) ; } public static abstract class Factory { public abstract SSTableReader open ( final Descriptor descriptor , Set < Component > components , CFMetaData metadata , Long maxDataAge , StatsMetadata sstableMetadata , OpenReason openReason , SerializationHeader header ) ; } } 
