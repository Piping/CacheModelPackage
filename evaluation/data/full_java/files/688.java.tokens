package org . apache . cassandra . io . sstable ; import java . io . DataInputStream ; import java . io . IOException ; import java . nio . ByteBuffer ; import java . nio . ByteOrder ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . db . DecoratedKey ; import org . apache . cassandra . db . PartitionPosition ; import org . apache . cassandra . dht . IPartitioner ; import org . apache . cassandra . io . util . * ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . concurrent . Ref ; import org . apache . cassandra . utils . concurrent . WrappedSharedCloseable ; import org . apache . cassandra . utils . memory . MemoryUtil ; import static org . apache . cassandra . io . sstable . Downsampling . BASE_SAMPLING_LEVEL ; public class IndexSummary extends WrappedSharedCloseable { private static final Logger logger = LoggerFactory . getLogger ( IndexSummary . class ) ; public static final IndexSummarySerializer serializer = new IndexSummarySerializer ( ) ; private final int minIndexInterval ; private final IPartitioner partitioner ; private final int sizeAtFullSampling ; private final Memory offsets ; private final int offsetCount ; private final Memory entries ; private final long entriesLength ; private final int samplingLevel ; public IndexSummary ( IPartitioner partitioner , Memory offsets , int offsetCount , Memory entries , long entriesLength , int sizeAtFullSampling , int minIndexInterval , int samplingLevel ) { super ( new Memory [ ] { offsets , entries } ) ; assert offsets . getInt ( <int> ) = = <int> ; this . partitioner = partitioner ; this . minIndexInterval = minIndexInterval ; this . offsetCount = offsetCount ; this . entriesLength = entriesLength ; this . sizeAtFullSampling = sizeAtFullSampling ; this . offsets = offsets ; this . entries = entries ; this . samplingLevel = samplingLevel ; assert samplingLevel > <int> ; } private IndexSummary ( IndexSummary copy ) { super ( copy ) ; this . partitioner = copy . partitioner ; this . minIndexInterval = copy . minIndexInterval ; this . offsetCount = copy . offsetCount ; this . entriesLength = copy . entriesLength ; this . sizeAtFullSampling = copy . sizeAtFullSampling ; this . offsets = copy . offsets ; this . entries = copy . entries ; this . samplingLevel = copy . samplingLevel ; } public int binarySearch ( PartitionPosition key ) { ByteBuffer hollow = MemoryUtil . getHollowDirectByteBuffer ( ) . order ( ByteOrder . BIG_ENDIAN ) ; int low = <int> , mid = offsetCount , high = mid - <int> , result = - <int> ; while ( low < = high ) { mid = ( low + high ) > > <int> ; fillTemporaryKey ( mid , hollow ) ; result = - DecoratedKey . compareTo ( partitioner , hollow , key ) ; if ( result > <int> ) { low = mid + <int> ; } else if ( result = = <int> ) { return mid ; } else { high = mid - <int> ; } } return - mid - ( result < <int> ? <int> : <int> ) ; } public int getPositionInSummary ( int index ) { return offsets . getInt ( index < < <int> ) ; } public byte [ ] getKey ( int index ) { long start = getPositionInSummary ( index ) ; int keySize = ( int ) ( calculateEnd ( index ) - start - <int> ) ; byte [ ] key = new byte [ keySize ] ; entries . getBytes ( start , key , <int> , keySize ) ; return key ; } private void fillTemporaryKey ( int index , ByteBuffer buffer ) { long start = getPositionInSummary ( index ) ; int keySize = ( int ) ( calculateEnd ( index ) - start - <int> ) ; entries . setByteBuffer ( buffer , start , keySize ) ; } public void addTo ( Ref . IdentityCollection identities ) { super . addTo ( identities ) ; identities . add ( offsets ) ; identities . add ( entries ) ; } public long getPosition ( int index ) { return entries . getLong ( calculateEnd ( index ) - <int> ) ; } public long getEndInSummary ( int index ) { return calculateEnd ( index ) ; } private long calculateEnd ( int index ) { return index = = ( offsetCount - <int> ) ? entriesLength : getPositionInSummary ( index + <int> ) ; } public int getMinIndexInterval ( ) { return minIndexInterval ; } public double getEffectiveIndexInterval ( ) { return ( BASE_SAMPLING_LEVEL / ( double ) samplingLevel ) * minIndexInterval ; } public long getEstimatedKeyCount ( ) { return ( ( long ) getMaxNumberOfEntries ( ) + <int> ) * minIndexInterval ; } public int size ( ) { return offsetCount ; } public int getSamplingLevel ( ) { return samplingLevel ; } public int getMaxNumberOfEntries ( ) { return sizeAtFullSampling ; } long getEntriesLength ( ) { return entriesLength ; } Memory getOffsets ( ) { return offsets ; } Memory getEntries ( ) { return entries ; } public long getOffHeapSize ( ) { return offsetCount * <int> + entriesLength ; } public int getEffectiveIndexIntervalAfterIndex ( int index ) { return Downsampling . getEffectiveIndexIntervalAfterIndex ( index , samplingLevel , minIndexInterval ) ; } public IndexSummary sharedCopy ( ) { return new IndexSummary ( this ) ; } public static class IndexSummarySerializer { public void serialize ( IndexSummary t , DataOutputPlus out , boolean withSamplingLevel ) throws IOException { out . writeInt ( t . minIndexInterval ) ; out . writeInt ( t . offsetCount ) ; out . writeLong ( t . getOffHeapSize ( ) ) ; if ( withSamplingLevel ) { out . writeInt ( t . samplingLevel ) ; out . writeInt ( t . sizeAtFullSampling ) ; } int baseOffset = t . offsetCount * <int> ; for ( int i = <int> ; i < t . offsetCount ; i + + ) { int offset = t . offsets . getInt ( i * <int> ) + baseOffset ; if ( ByteOrder . nativeOrder ( ) ! = ByteOrder . BIG_ENDIAN ) offset = Integer . reverseBytes ( offset ) ; out . writeInt ( offset ) ; } out . write ( t . entries , <int> , t . entriesLength ) ; } @SuppressWarnings ( <str> ) public IndexSummary deserialize ( DataInputStream in , IPartitioner partitioner , boolean haveSamplingLevel , int expectedMinIndexInterval , int maxIndexInterval ) throws IOException { int minIndexInterval = in . readInt ( ) ; if ( minIndexInterval ! = expectedMinIndexInterval ) { throw new IOException ( String . format ( <str> , minIndexInterval , expectedMinIndexInterval ) ) ; } int offsetCount = in . readInt ( ) ; long offheapSize = in . readLong ( ) ; int samplingLevel , fullSamplingSummarySize ; if ( haveSamplingLevel ) { samplingLevel = in . readInt ( ) ; fullSamplingSummarySize = in . readInt ( ) ; } else { samplingLevel = BASE_SAMPLING_LEVEL ; fullSamplingSummarySize = offsetCount ; } int effectiveIndexInterval = ( int ) Math . ceil ( ( BASE_SAMPLING_LEVEL / ( double ) samplingLevel ) * minIndexInterval ) ; if ( effectiveIndexInterval > maxIndexInterval ) { throw new IOException ( String . format ( <str> + <str> , effectiveIndexInterval , maxIndexInterval ) ) ; } Memory offsets = Memory . allocate ( offsetCount * <int> ) ; Memory entries = Memory . allocate ( offheapSize - offsets . size ( ) ) ; try { FBUtilities . copy ( in , new MemoryOutputStream ( offsets ) , offsets . size ( ) ) ; FBUtilities . copy ( in , new MemoryOutputStream ( entries ) , entries . size ( ) ) ; } catch ( IOException ioe ) { offsets . free ( ) ; entries . free ( ) ; throw ioe ; } for ( int i = <int> ; i < offsets . size ( ) ; i + = <int> ) offsets . setInt ( i , ( int ) ( offsets . getInt ( i ) - offsets . size ( ) ) ) ; return new IndexSummary ( partitioner , offsets , offsetCount , entries , entries . size ( ) , fullSamplingSummarySize , minIndexInterval , samplingLevel ) ; } } } 
