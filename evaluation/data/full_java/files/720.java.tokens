package org . apache . cassandra . io . sstable . metadata ; import java . io . File ; import java . nio . ByteBuffer ; import java . util . ArrayList ; import java . util . Collections ; import java . util . HashSet ; import java . util . List ; import java . util . Map ; import java . util . Set ; import com . google . common . collect . Maps ; import com . clearspring . analytics . stream . cardinality . HyperLogLogPlus ; import com . clearspring . analytics . stream . cardinality . ICardinality ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . commitlog . ReplayPosition ; import org . apache . cassandra . db . marshal . AbstractType ; import org . apache . cassandra . db . partitions . PartitionStatisticsCollector ; import org . apache . cassandra . db . rows . Cell ; import org . apache . cassandra . io . sstable . Component ; import org . apache . cassandra . io . sstable . SSTable ; import org . apache . cassandra . io . sstable . format . SSTableReader ; import org . apache . cassandra . service . ActiveRepairService ; import org . apache . cassandra . utils . EstimatedHistogram ; import org . apache . cassandra . utils . MurmurHash ; import org . apache . cassandra . utils . StreamingHistogram ; public class MetadataCollector implements PartitionStatisticsCollector { public static final double NO_COMPRESSION_RATIO = - <float> ; static EstimatedHistogram defaultCellPerPartitionCountHistogram ( ) { return new EstimatedHistogram ( <int> ) ; } static EstimatedHistogram defaultPartitionSizeHistogram ( ) { return new EstimatedHistogram ( <int> ) ; } static StreamingHistogram defaultTombstoneDropTimeHistogram ( ) { return new StreamingHistogram ( SSTable . TOMBSTONE_HISTOGRAM_BIN_SIZE ) ; } public static StatsMetadata defaultStatsMetadata ( ) { return new StatsMetadata ( defaultPartitionSizeHistogram ( ) , defaultCellPerPartitionCountHistogram ( ) , ReplayPosition . NONE , Long . MIN_VALUE , Long . MAX_VALUE , Integer . MAX_VALUE , Integer . MAX_VALUE , <int> , Integer . MAX_VALUE , NO_COMPRESSION_RATIO , defaultTombstoneDropTimeHistogram ( ) , <int> , Collections . < ByteBuffer > emptyList ( ) , Collections . < ByteBuffer > emptyList ( ) , true , ActiveRepairService . UNREPAIRED_SSTABLE , - <int> , - <int> ) ; } protected EstimatedHistogram estimatedPartitionSize = defaultPartitionSizeHistogram ( ) ; protected EstimatedHistogram estimatedCellPerPartitionCount = defaultCellPerPartitionCountHistogram ( ) ; protected ReplayPosition replayPosition = ReplayPosition . NONE ; protected final MinMaxLongTracker timestampTracker = new MinMaxLongTracker ( ) ; protected final MinMaxIntTracker localDeletionTimeTracker = new MinMaxIntTracker ( Cell . NO_DELETION_TIME , Cell . NO_DELETION_TIME ) ; protected final MinMaxIntTracker ttlTracker = new MinMaxIntTracker ( Cell . NO_TTL , Cell . NO_TTL ) ; protected double compressionRatio = NO_COMPRESSION_RATIO ; protected StreamingHistogram estimatedTombstoneDropTime = defaultTombstoneDropTimeHistogram ( ) ; protected int sstableLevel ; protected ByteBuffer [ ] minClusteringValues ; protected ByteBuffer [ ] maxClusteringValues ; protected boolean hasLegacyCounterShards = false ; protected long totalColumnsSet ; protected long totalRows ; protected ICardinality cardinality = new HyperLogLogPlus ( <int> , <int> ) ; private final ClusteringComparator comparator ; public MetadataCollector ( ClusteringComparator comparator ) { this . comparator = comparator ; this . minClusteringValues = new ByteBuffer [ comparator . size ( ) ] ; this . maxClusteringValues = new ByteBuffer [ comparator . size ( ) ] ; } public MetadataCollector ( Iterable < SSTableReader > sstables , ClusteringComparator comparator , int level ) { this ( comparator ) ; replayPosition ( ReplayPosition . getReplayPosition ( sstables ) ) ; sstableLevel ( level ) ; } public MetadataCollector addKey ( ByteBuffer key ) { long hashed = MurmurHash . hash2_64 ( key , key . position ( ) , key . remaining ( ) , <int> ) ; cardinality . offerHashed ( hashed ) ; return this ; } public MetadataCollector addPartitionSizeInBytes ( long partitionSize ) { estimatedPartitionSize . add ( partitionSize ) ; return this ; } public MetadataCollector addCellPerPartitionCount ( long cellCount ) { estimatedCellPerPartitionCount . add ( cellCount ) ; return this ; } public MetadataCollector mergeTombstoneHistogram ( StreamingHistogram histogram ) { estimatedTombstoneDropTime . merge ( histogram ) ; return this ; } public MetadataCollector addCompressionRatio ( long compressed , long uncompressed ) { compressionRatio = ( double ) compressed / uncompressed ; return this ; } public void update ( LivenessInfo newInfo ) { if ( newInfo . isEmpty ( ) ) return ; updateTimestamp ( newInfo . timestamp ( ) ) ; if ( newInfo . isExpiring ( ) ) { updateTTL ( newInfo . ttl ( ) ) ; updateLocalDeletionTime ( newInfo . localExpirationTime ( ) ) ; } } public void update ( Cell cell ) { updateTimestamp ( cell . timestamp ( ) ) ; updateTTL ( cell . ttl ( ) ) ; updateLocalDeletionTime ( cell . localDeletionTime ( ) ) ; } public void update ( DeletionTime dt ) { if ( ! dt . isLive ( ) ) { updateTimestamp ( dt . markedForDeleteAt ( ) ) ; updateLocalDeletionTime ( dt . localDeletionTime ( ) ) ; } } public void updateColumnSetPerRow ( long columnSetInRow ) { totalColumnsSet + = columnSetInRow ; + + totalRows ; } private void updateTimestamp ( long newTimestamp ) { timestampTracker . update ( newTimestamp ) ; } private void updateLocalDeletionTime ( int newLocalDeletionTime ) { localDeletionTimeTracker . update ( newLocalDeletionTime ) ; estimatedTombstoneDropTime . update ( newLocalDeletionTime ) ; } private void updateTTL ( int newTTL ) { ttlTracker . update ( newTTL ) ; } public MetadataCollector replayPosition ( ReplayPosition replayPosition ) { this . replayPosition = replayPosition ; return this ; } public MetadataCollector sstableLevel ( int sstableLevel ) { this . sstableLevel = sstableLevel ; return this ; } public MetadataCollector updateClusteringValues ( ClusteringPrefix clustering ) { int size = clustering . size ( ) ; for ( int i = <int> ; i < size ; i + + ) { AbstractType < ? > type = comparator . subtype ( i ) ; ByteBuffer newValue = clustering . get ( i ) ; minClusteringValues [ i ] = min ( minClusteringValues [ i ] , newValue , type ) ; maxClusteringValues [ i ] = max ( maxClusteringValues [ i ] , newValue , type ) ; } return this ; } private static ByteBuffer min ( ByteBuffer b1 , ByteBuffer b2 , AbstractType < ? > comparator ) { if ( b1 = = null ) return b2 ; if ( b2 = = null ) return b1 ; if ( comparator . compare ( b1 , b2 ) > = <int> ) return b2 ; return b1 ; } private static ByteBuffer max ( ByteBuffer b1 , ByteBuffer b2 , AbstractType < ? > comparator ) { if ( b1 = = null ) return b2 ; if ( b2 = = null ) return b1 ; if ( comparator . compare ( b1 , b2 ) > = <int> ) return b1 ; return b2 ; } public void updateHasLegacyCounterShards ( boolean hasLegacyCounterShards ) { this . hasLegacyCounterShards = this . hasLegacyCounterShards | | hasLegacyCounterShards ; } public Map < MetadataType , MetadataComponent > finalizeMetadata ( String partitioner , double bloomFilterFPChance , long repairedAt , SerializationHeader header ) { Map < MetadataType , MetadataComponent > components = Maps . newHashMap ( ) ; components . put ( MetadataType . VALIDATION , new ValidationMetadata ( partitioner , bloomFilterFPChance ) ) ; components . put ( MetadataType . STATS , new StatsMetadata ( estimatedPartitionSize , estimatedCellPerPartitionCount , replayPosition , timestampTracker . min ( ) , timestampTracker . max ( ) , localDeletionTimeTracker . min ( ) , localDeletionTimeTracker . max ( ) , ttlTracker . min ( ) , ttlTracker . max ( ) , compressionRatio , estimatedTombstoneDropTime , sstableLevel , makeList ( minClusteringValues ) , makeList ( maxClusteringValues ) , hasLegacyCounterShards , repairedAt , totalColumnsSet , totalRows ) ) ; components . put ( MetadataType . COMPACTION , new CompactionMetadata ( cardinality ) ) ; components . put ( MetadataType . HEADER , header . toComponent ( ) ) ; return components ; } private static List < ByteBuffer > makeList ( ByteBuffer [ ] values ) { List < ByteBuffer > l = new ArrayList < ByteBuffer > ( values . length ) ; for ( int i = <int> ; i < values . length ; i + + ) if ( values [ i ] = = null ) break ; else l . add ( values [ i ] ) ; return l ; } public static class MinMaxLongTracker { private final long defaultMin ; private final long defaultMax ; private boolean isSet = false ; private long min ; private long max ; public MinMaxLongTracker ( ) { this ( Long . MIN_VALUE , Long . MAX_VALUE ) ; } public MinMaxLongTracker ( long defaultMin , long defaultMax ) { this . defaultMin = defaultMin ; this . defaultMax = defaultMax ; } public void update ( long value ) { if ( ! isSet ) { min = max = value ; isSet = true ; } else { if ( value < min ) min = value ; if ( value > max ) max = value ; } } public long min ( ) { return isSet ? min : defaultMin ; } public long max ( ) { return isSet ? max : defaultMax ; } } public static class MinMaxIntTracker { private final int defaultMin ; private final int defaultMax ; private boolean isSet = false ; private int min ; private int max ; public MinMaxIntTracker ( ) { this ( Integer . MIN_VALUE , Integer . MAX_VALUE ) ; } public MinMaxIntTracker ( int defaultMin , int defaultMax ) { this . defaultMin = defaultMin ; this . defaultMax = defaultMax ; } public void update ( int value ) { if ( ! isSet ) { min = max = value ; isSet = true ; } else { if ( value < min ) min = value ; if ( value > max ) max = value ; } } public int min ( ) { return isSet ? min : defaultMin ; } public int max ( ) { return isSet ? max : defaultMax ; } } } 
