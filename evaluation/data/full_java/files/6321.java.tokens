package org . elasticsearch . index . analysis ; import org . apache . lucene . analysis . Tokenizer ; import org . apache . lucene . analysis . ngram . Lucene43NGramTokenizer ; import org . apache . lucene . analysis . ngram . NGramTokenizer ; import org . apache . lucene . util . Version ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . env . Environment ; import org . elasticsearch . index . IndexSettings ; import java . lang . reflect . Field ; import java . lang . reflect . Modifier ; import java . util . HashMap ; import java . util . Locale ; import java . util . Map ; import static java . util . Collections . unmodifiableMap ; public class NGramTokenizerFactory extends AbstractTokenizerFactory { private final int minGram ; private final int maxGram ; private final CharMatcher matcher ; private org . elasticsearch . Version esVersion ; static final Map < String , CharMatcher > MATCHERS ; static { Map < String , CharMatcher > matchers = new HashMap < > ( ) ; matchers . put ( <str> , CharMatcher . Basic . LETTER ) ; matchers . put ( <str> , CharMatcher . Basic . DIGIT ) ; matchers . put ( <str> , CharMatcher . Basic . WHITESPACE ) ; matchers . put ( <str> , CharMatcher . Basic . PUNCTUATION ) ; matchers . put ( <str> , CharMatcher . Basic . SYMBOL ) ; for ( Field field : Character . class . getFields ( ) ) { if ( ! field . getName ( ) . startsWith ( <str> ) & & Modifier . isPublic ( field . getModifiers ( ) ) & & Modifier . isStatic ( field . getModifiers ( ) ) & & field . getType ( ) = = byte . class ) { try { matchers . put ( field . getName ( ) . toLowerCase ( Locale . ROOT ) , CharMatcher . ByUnicodeCategory . of ( field . getByte ( null ) ) ) ; } catch ( Exception e ) { continue ; } } } MATCHERS = unmodifiableMap ( matchers ) ; } static CharMatcher parseTokenChars ( String [ ] characterClasses ) { if ( characterClasses = = null | | characterClasses . length = = <int> ) { return null ; } CharMatcher . Builder builder = new CharMatcher . Builder ( ) ; for ( String characterClass : characterClasses ) { characterClass = characterClass . toLowerCase ( Locale . ROOT ) . trim ( ) ; CharMatcher matcher = MATCHERS . get ( characterClass ) ; if ( matcher = = null ) { throw new IllegalArgumentException ( <str> + characterClass + <str> + MATCHERS . keySet ( ) ) ; } builder . or ( matcher ) ; } return builder . build ( ) ; } public NGramTokenizerFactory ( IndexSettings indexSettings , Environment environment , String name , Settings settings ) { super ( indexSettings , name , settings ) ; this . minGram = settings . getAsInt ( <str> , NGramTokenizer . DEFAULT_MIN_NGRAM_SIZE ) ; this . maxGram = settings . getAsInt ( <str> , NGramTokenizer . DEFAULT_MAX_NGRAM_SIZE ) ; this . matcher = parseTokenChars ( settings . getAsArray ( <str> ) ) ; this . esVersion = indexSettings . getIndexVersionCreated ( ) ; } @SuppressWarnings ( <str> ) @Override public Tokenizer create ( ) { if ( version . onOrAfter ( Version . LUCENE_4_3 ) & & esVersion . onOrAfter ( org . elasticsearch . Version . V_0_90_2 ) ) { final Version version = this . version = = Version . LUCENE_4_3 ? Version . LUCENE_4_4 : this . version ; if ( matcher = = null ) { return new NGramTokenizer ( minGram , maxGram ) ; } else { return new NGramTokenizer ( minGram , maxGram ) { @Override protected boolean isTokenChar ( int chr ) { return matcher . isTokenChar ( chr ) ; } } ; } } else { return new Lucene43NGramTokenizer ( minGram , maxGram ) ; } } } 
