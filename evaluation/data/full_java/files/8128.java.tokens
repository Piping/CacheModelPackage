package org . elasticsearch . index . fielddata ; import org . apache . lucene . document . Document ; import org . apache . lucene . document . Field ; import org . apache . lucene . document . SortedDocValuesField ; import org . apache . lucene . document . StringField ; import org . apache . lucene . index . DirectoryReader ; import org . apache . lucene . index . LeafReaderContext ; import org . apache . lucene . index . SortedDocValues ; import org . apache . lucene . search . FieldDoc ; import org . apache . lucene . search . IndexSearcher ; import org . apache . lucene . search . MatchAllDocsQuery ; import org . apache . lucene . search . Sort ; import org . apache . lucene . search . SortField ; import org . apache . lucene . search . TopFieldDocs ; import org . apache . lucene . util . BytesRef ; import org . elasticsearch . action . admin . indices . mapping . put . PutMappingRequest ; import org . elasticsearch . common . compress . CompressedXContent ; import org . elasticsearch . index . fielddata . plain . ParentChildIndexFieldData ; import org . elasticsearch . index . mapper . Uid ; import org . elasticsearch . index . mapper . internal . ParentFieldMapper ; import org . elasticsearch . index . mapper . internal . UidFieldMapper ; import org . elasticsearch . search . MultiValueMode ; import org . junit . Before ; import java . util . HashMap ; import java . util . Map ; import java . util . concurrent . CountDownLatch ; import java . util . concurrent . atomic . AtomicReference ; import static org . hamcrest . Matchers . equalTo ; import static org . hamcrest . Matchers . nullValue ; public class ParentChildFieldDataTests extends AbstractFieldDataTestCase { private final String parentType = <str> ; private final String childType = <str> ; private final String grandChildType = <str> ; @Before public void before ( ) throws Exception { mapperService . merge ( childType , new CompressedXContent ( PutMappingRequest . buildFromSimplifiedDef ( childType , <str> , <str> + parentType ) . string ( ) ) , true , false ) ; mapperService . merge ( grandChildType , new CompressedXContent ( PutMappingRequest . buildFromSimplifiedDef ( grandChildType , <str> , <str> + childType ) . string ( ) ) , true , false ) ; Document d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( parentType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( parentType , <str> ) ) ; writer . addDocument ( d ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( childType , <str> ) , Field . Store . NO ) ) ; d . add ( new StringField ( ParentFieldMapper . NAME , Uid . createUid ( parentType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( parentType , <str> ) ) ; d . add ( createJoinField ( childType , <str> ) ) ; writer . addDocument ( d ) ; writer . commit ( ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( childType , <str> ) , Field . Store . NO ) ) ; d . add ( new StringField ( ParentFieldMapper . NAME , Uid . createUid ( parentType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( parentType , <str> ) ) ; d . add ( createJoinField ( childType , <str> ) ) ; writer . addDocument ( d ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( parentType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( parentType , <str> ) ) ; writer . addDocument ( d ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( childType , <str> ) , Field . Store . NO ) ) ; d . add ( new StringField ( ParentFieldMapper . NAME , Uid . createUid ( parentType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( parentType , <str> ) ) ; d . add ( createJoinField ( childType , <str> ) ) ; writer . addDocument ( d ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( childType , <str> ) , Field . Store . NO ) ) ; d . add ( new StringField ( ParentFieldMapper . NAME , Uid . createUid ( parentType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( parentType , <str> ) ) ; d . add ( createJoinField ( childType , <str> ) ) ; writer . addDocument ( d ) ; writer . commit ( ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( grandChildType , <str> ) , Field . Store . NO ) ) ; d . add ( new StringField ( ParentFieldMapper . NAME , Uid . createUid ( childType , <str> ) , Field . Store . NO ) ) ; d . add ( createJoinField ( childType , <str> ) ) ; writer . addDocument ( d ) ; d = new Document ( ) ; d . add ( new StringField ( UidFieldMapper . NAME , Uid . createUid ( <str> , <str> ) , Field . Store . NO ) ) ; writer . addDocument ( d ) ; } private SortedDocValuesField createJoinField ( String parentType , String id ) { return new SortedDocValuesField ( ParentFieldMapper . joinField ( parentType ) , new BytesRef ( id ) ) ; } public void testGetBytesValues ( ) throws Exception { IndexFieldData indexFieldData = getForField ( childType ) ; AtomicFieldData fieldData = indexFieldData . load ( refreshReader ( ) ) ; SortedBinaryDocValues bytesValues = fieldData . getBytesValues ( ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; assertThat ( bytesValues . valueAt ( <int> ) . utf8ToString ( ) , equalTo ( <str> ) ) ; bytesValues . setDocument ( <int> ) ; assertThat ( bytesValues . count ( ) , equalTo ( <int> ) ) ; } public void testSorting ( ) throws Exception { IndexFieldData indexFieldData = getForField ( childType ) ; IndexSearcher searcher = new IndexSearcher ( DirectoryReader . open ( writer , true ) ) ; IndexFieldData . XFieldComparatorSource comparator = indexFieldData . comparatorSource ( <str> , MultiValueMode . MIN , null ) ; TopFieldDocs topDocs = searcher . search ( new MatchAllDocsQuery ( ) , <int> , new Sort ( new SortField ( ParentFieldMapper . NAME , comparator , false ) ) ) ; assertThat ( topDocs . totalHits , equalTo ( <int> ) ) ; assertThat ( topDocs . scoreDocs . length , equalTo ( <int> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) , equalTo ( null ) ) ; topDocs = searcher . search ( new MatchAllDocsQuery ( ) , <int> , new Sort ( new SortField ( ParentFieldMapper . NAME , comparator , true ) ) ) ; assertThat ( topDocs . totalHits , equalTo ( <int> ) ) ; assertThat ( topDocs . scoreDocs . length , equalTo ( <int> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( BytesRef ) ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] ) . utf8ToString ( ) , equalTo ( <str> ) ) ; assertThat ( topDocs . scoreDocs [ <int> ] . doc , equalTo ( <int> ) ) ; assertThat ( ( ( FieldDoc ) topDocs . scoreDocs [ <int> ] ) . fields [ <int> ] , nullValue ( ) ) ; } public void testThreads ( ) throws Exception { final ParentChildIndexFieldData indexFieldData = getForField ( childType ) ; final DirectoryReader reader = DirectoryReader . open ( writer , true ) ; final IndexParentChildFieldData global = indexFieldData . loadGlobal ( reader ) ; final AtomicReference < Exception > error = new AtomicReference < > ( ) ; final int numThreads = scaledRandomIntBetween ( <int> , <int> ) ; final Thread [ ] threads = new Thread [ numThreads ] ; final CountDownLatch latch = new CountDownLatch ( <int> ) ; final Map < Object , BytesRef [ ] > expected = new HashMap < > ( ) ; for ( LeafReaderContext context : reader . leaves ( ) ) { AtomicParentChildFieldData leafData = global . load ( context ) ; SortedDocValues parentIds = leafData . getOrdinalsValues ( parentType ) ; final BytesRef [ ] ids = new BytesRef [ parentIds . getValueCount ( ) ] ; for ( int j = <int> ; j < parentIds . getValueCount ( ) ; + + j ) { final BytesRef id = parentIds . lookupOrd ( j ) ; if ( id ! = null ) { ids [ j ] = BytesRef . deepCopyOf ( id ) ; } } expected . put ( context . reader ( ) . getCoreCacheKey ( ) , ids ) ; } for ( int i = <int> ; i < numThreads ; + + i ) { threads [ i ] = new Thread ( ) { @Override public void run ( ) { try { latch . await ( ) ; for ( int i = <int> ; i < <int> ; + + i ) { for ( LeafReaderContext context : reader . leaves ( ) ) { AtomicParentChildFieldData leafData = global . load ( context ) ; SortedDocValues parentIds = leafData . getOrdinalsValues ( parentType ) ; final BytesRef [ ] expectedIds = expected . get ( context . reader ( ) . getCoreCacheKey ( ) ) ; for ( int j = <int> ; j < parentIds . getValueCount ( ) ; + + j ) { final BytesRef id = parentIds . lookupOrd ( j ) ; assertEquals ( expectedIds [ j ] , id ) ; } } } } catch ( Exception e ) { error . compareAndSet ( null , e ) ; } } } ; threads [ i ] . start ( ) ; } latch . countDown ( ) ; for ( Thread thread : threads ) { thread . join ( ) ; } if ( error . get ( ) ! = null ) { throw error . get ( ) ; } } @Override protected FieldDataType getFieldDataType ( ) { return new FieldDataType ( <str> ) ; } } 
