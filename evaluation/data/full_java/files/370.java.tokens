package org . apache . cassandra . db . commitlog ; import java . io . DataOutputStream ; import java . io . EOFException ; import java . io . File ; import java . io . FileOutputStream ; import java . io . IOException ; import java . nio . ByteBuffer ; import java . util . * ; import java . util . concurrent . Future ; import java . util . concurrent . atomic . AtomicInteger ; import java . util . zip . CRC32 ; import com . google . common . annotations . VisibleForTesting ; import com . google . common . base . Predicate ; import com . google . common . collect . HashMultimap ; import com . google . common . collect . Iterables ; import com . google . common . collect . Multimap ; import com . google . common . collect . Ordering ; import org . apache . commons . lang3 . StringUtils ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . concurrent . Stage ; import org . apache . cassandra . concurrent . StageManager ; import org . apache . cassandra . config . CFMetaData ; import org . apache . cassandra . config . Schema ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . rows . SerializationHelper ; import org . apache . cassandra . db . partitions . PartitionUpdate ; import org . apache . cassandra . db . lifecycle . SSTableSet ; import org . apache . cassandra . exceptions . ConfigurationException ; import org . apache . cassandra . io . util . FileSegmentInputStream ; import org . apache . cassandra . io . util . RebufferingInputStream ; import org . apache . cassandra . schema . CompressionParams ; import org . apache . cassandra . io . compress . ICompressor ; import org . apache . cassandra . io . util . ChannelProxy ; import org . apache . cassandra . io . util . DataInputBuffer ; import org . apache . cassandra . io . util . FileDataInput ; import org . apache . cassandra . io . util . RandomAccessReader ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . JVMStabilityInspector ; import org . apache . cassandra . utils . WrappedRunnable ; import org . cliffc . high_scale_lib . NonBlockingHashSet ; import static org . apache . cassandra . utils . FBUtilities . updateChecksumInt ; public class CommitLogReplayer { @VisibleForTesting public static long MAX_OUTSTANDING_REPLAY_BYTES = Long . getLong ( <str> , <int> * <int> * <int> ) ; @VisibleForTesting public static MutationInitiator mutationInitiator = new MutationInitiator ( ) ; static final String IGNORE_REPLAY_ERRORS_PROPERTY = <str> ; private static final Logger logger = LoggerFactory . getLogger ( CommitLogReplayer . class ) ; private static final int MAX_OUTSTANDING_REPLAY_COUNT = Integer . getInteger ( <str> , <int> ) ; private static final int LEGACY_END_OF_SEGMENT_MARKER = <int> ; private final Set < Keyspace > keyspacesRecovered ; private final Queue < Future < Integer > > futures ; private final Map < UUID , AtomicInteger > invalidMutations ; private final AtomicInteger replayedCount ; private final Map < UUID , ReplayPosition > cfPositions ; private final ReplayPosition globalPosition ; private final CRC32 checksum ; private byte [ ] buffer ; private byte [ ] uncompressedBuffer ; private long pendingMutationBytes = <int> ; private final ReplayFilter replayFilter ; private final CommitLogArchiver archiver ; @VisibleForTesting public static class MutationInitiator { protected Future < Integer > initiateMutation ( final Mutation mutation , final long segmentId , final int serializedSize , final long entryLocation , final CommitLogReplayer clr ) { Runnable runnable = new WrappedRunnable ( ) { public void runMayThrow ( ) throws IOException { if ( Schema . instance . getKSMetaData ( mutation . getKeyspaceName ( ) ) = = null ) return ; if ( clr . pointInTimeExceeded ( mutation ) ) return ; final Keyspace keyspace = Keyspace . open ( mutation . getKeyspaceName ( ) ) ; Mutation newMutation = null ; for ( PartitionUpdate update : clr . replayFilter . filter ( mutation ) ) { if ( Schema . instance . getCF ( update . metadata ( ) . cfId ) = = null ) continue ; ReplayPosition rp = clr . cfPositions . get ( update . metadata ( ) . cfId ) ; if ( segmentId > rp . segment | | ( segmentId = = rp . segment & & entryLocation > rp . position ) ) { if ( newMutation = = null ) newMutation = new Mutation ( mutation . getKeyspaceName ( ) , mutation . key ( ) ) ; newMutation . add ( update ) ; clr . replayedCount . incrementAndGet ( ) ; } } if ( newMutation ! = null ) { assert ! newMutation . isEmpty ( ) ; Keyspace . open ( newMutation . getKeyspaceName ( ) ) . applyFromCommitLog ( newMutation ) ; clr . keyspacesRecovered . add ( keyspace ) ; } } } ; return StageManager . getStage ( Stage . MUTATION ) . submit ( runnable , serializedSize ) ; } } CommitLogReplayer ( CommitLog commitLog , ReplayPosition globalPosition , Map < UUID , ReplayPosition > cfPositions , ReplayFilter replayFilter ) { this . keyspacesRecovered = new NonBlockingHashSet < Keyspace > ( ) ; this . futures = new ArrayDeque < Future < Integer > > ( ) ; this . buffer = new byte [ <int> ] ; this . uncompressedBuffer = new byte [ <int> ] ; this . invalidMutations = new HashMap < UUID , AtomicInteger > ( ) ; this . replayedCount = new AtomicInteger ( ) ; this . checksum = new CRC32 ( ) ; this . cfPositions = cfPositions ; this . globalPosition = globalPosition ; this . replayFilter = replayFilter ; this . archiver = commitLog . archiver ; } public static CommitLogReplayer construct ( CommitLog commitLog ) { Map < UUID , ReplayPosition > cfPositions = new HashMap < UUID , ReplayPosition > ( ) ; Ordering < ReplayPosition > replayPositionOrdering = Ordering . from ( ReplayPosition . comparator ) ; ReplayFilter replayFilter = ReplayFilter . create ( ) ; for ( ColumnFamilyStore cfs : ColumnFamilyStore . all ( ) ) { ReplayPosition rp = ReplayPosition . getReplayPosition ( cfs . getSSTables ( SSTableSet . CANONICAL ) ) ; ReplayPosition truncatedAt = SystemKeyspace . getTruncatedPosition ( cfs . metadata . cfId ) ; if ( truncatedAt ! = null ) { long restoreTime = commitLog . archiver . restorePointInTime ; long truncatedTime = SystemKeyspace . getTruncatedAt ( cfs . metadata . cfId ) ; if ( truncatedTime > restoreTime ) { if ( replayFilter . includes ( cfs . metadata ) ) { logger . info ( <str> , cfs . metadata . ksName , cfs . metadata . cfName ) ; SystemKeyspace . removeTruncationRecord ( cfs . metadata . cfId ) ; } } else { rp = replayPositionOrdering . max ( Arrays . asList ( rp , truncatedAt ) ) ; } } cfPositions . put ( cfs . metadata . cfId , rp ) ; } ReplayPosition globalPosition = replayPositionOrdering . min ( cfPositions . values ( ) ) ; logger . trace ( <str> , globalPosition , FBUtilities . toString ( cfPositions ) ) ; return new CommitLogReplayer ( commitLog , globalPosition , cfPositions , replayFilter ) ; } public void recover ( File [ ] clogs ) throws IOException { int i ; for ( i = <int> ; i < clogs . length ; + + i ) recover ( clogs [ i ] , i + <int> = = clogs . length ) ; } public int blockForWrites ( ) { for ( Map . Entry < UUID , AtomicInteger > entry : invalidMutations . entrySet ( ) ) logger . warn ( String . format ( <str> , entry . getValue ( ) . intValue ( ) , entry . getKey ( ) ) ) ; FBUtilities . waitOnFutures ( futures ) ; logger . trace ( <str> ) ; futures . clear ( ) ; boolean flushingSystem = false ; List < Future < ? > > futures = new ArrayList < Future < ? > > ( ) ; for ( Keyspace keyspace : keyspacesRecovered ) { if ( keyspace . getName ( ) . equals ( SystemKeyspace . NAME ) ) flushingSystem = true ; futures . addAll ( keyspace . flush ( ) ) ; } if ( ! flushingSystem ) futures . add ( Keyspace . open ( SystemKeyspace . NAME ) . getColumnFamilyStore ( SystemKeyspace . BATCHES ) . forceFlush ( ) ) ; FBUtilities . waitOnFutures ( futures ) ; return replayedCount . get ( ) ; } private int readSyncMarker ( CommitLogDescriptor descriptor , int offset , RandomAccessReader reader , boolean tolerateTruncation ) throws IOException { if ( offset > reader . length ( ) - CommitLogSegment . SYNC_MARKER_SIZE ) { return - <int> ; } reader . seek ( offset ) ; CRC32 crc = new CRC32 ( ) ; updateChecksumInt ( crc , ( int ) ( descriptor . id & <hex> ) ) ; updateChecksumInt ( crc , ( int ) ( descriptor . id > > > <int> ) ) ; updateChecksumInt ( crc , ( int ) reader . getPosition ( ) ) ; int end = reader . readInt ( ) ; long filecrc = reader . readInt ( ) & <hex> ; if ( crc . getValue ( ) ! = filecrc ) { if ( end ! = <int> | | filecrc ! = <int> ) { handleReplayError ( false , <str> + <str> , offset , reader . getPath ( ) ) ; } return - <int> ; } else if ( end < offset | | end > reader . length ( ) ) { handleReplayError ( tolerateTruncation , <str> , offset , reader . getPath ( ) ) ; return - <int> ; } return end ; } abstract static class ReplayFilter { public abstract Iterable < PartitionUpdate > filter ( Mutation mutation ) ; public abstract boolean includes ( CFMetaData metadata ) ; public static ReplayFilter create ( ) { if ( System . getProperty ( <str> ) = = null ) return new AlwaysReplayFilter ( ) ; Multimap < String , String > toReplay = HashMultimap . create ( ) ; for ( String rawPair : System . getProperty ( <str> ) . split ( <str> ) ) { String [ ] pair = rawPair . trim ( ) . split ( <str> ) ; if ( pair . length ! = <int> ) throw new IllegalArgumentException ( <str> ) ; Keyspace ks = Schema . instance . getKeyspaceInstance ( pair [ <int> ] ) ; if ( ks = = null ) throw new IllegalArgumentException ( <str> + pair [ <int> ] ) ; ColumnFamilyStore cfs = ks . getColumnFamilyStore ( pair [ <int> ] ) ; if ( cfs = = null ) throw new IllegalArgumentException ( String . format ( <str> , pair [ <int> ] , pair [ <int> ] ) ) ; toReplay . put ( pair [ <int> ] , pair [ <int> ] ) ; } return new CustomReplayFilter ( toReplay ) ; } } private static class AlwaysReplayFilter extends ReplayFilter { public Iterable < PartitionUpdate > filter ( Mutation mutation ) { return mutation . getPartitionUpdates ( ) ; } public boolean includes ( CFMetaData metadata ) { return true ; } } private static class CustomReplayFilter extends ReplayFilter { private Multimap < String , String > toReplay ; public CustomReplayFilter ( Multimap < String , String > toReplay ) { this . toReplay = toReplay ; } public Iterable < PartitionUpdate > filter ( Mutation mutation ) { final Collection < String > cfNames = toReplay . get ( mutation . getKeyspaceName ( ) ) ; if ( cfNames = = null ) return Collections . emptySet ( ) ; return Iterables . filter ( mutation . getPartitionUpdates ( ) , new Predicate < PartitionUpdate > ( ) { public boolean apply ( PartitionUpdate upd ) { return cfNames . contains ( upd . metadata ( ) . cfName ) ; } } ) ; } public boolean includes ( CFMetaData metadata ) { return toReplay . containsEntry ( metadata . ksName , metadata . cfName ) ; } } public void recover ( File file , boolean tolerateTruncation ) throws IOException { CommitLogDescriptor desc = CommitLogDescriptor . fromFileName ( file . getName ( ) ) ; try ( ChannelProxy channel = new ChannelProxy ( file ) ; RandomAccessReader reader = RandomAccessReader . open ( channel ) ) { if ( desc . version < CommitLogDescriptor . VERSION_21 ) { if ( logAndCheckIfShouldSkip ( file , desc ) ) return ; if ( globalPosition . segment = = desc . id ) reader . seek ( globalPosition . position ) ; replaySyncSection ( reader , ( int ) reader . length ( ) , desc , desc . fileName ( ) , tolerateTruncation ) ; return ; } final long segmentId = desc . id ; try { desc = CommitLogDescriptor . readHeader ( reader ) ; } catch ( IOException e ) { desc = null ; } if ( desc = = null ) { handleReplayError ( false , <str> , file ) ; return ; } if ( segmentId ! = desc . id ) { handleReplayError ( false , <str> , segmentId , desc . id , file ) ; } if ( logAndCheckIfShouldSkip ( file , desc ) ) return ; ICompressor compressor = null ; if ( desc . compression ! = null ) { try { compressor = CompressionParams . createCompressor ( desc . compression ) ; } catch ( ConfigurationException e ) { handleReplayError ( false , <str> , e . getMessage ( ) ) ; return ; } } assert reader . length ( ) < = Integer . MAX_VALUE ; int end = ( int ) reader . getFilePointer ( ) ; int replayEnd = end ; while ( ( end = readSyncMarker ( desc , end , reader , tolerateTruncation ) ) > = <int> ) { int replayPos = replayEnd + CommitLogSegment . SYNC_MARKER_SIZE ; if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , file , reader . getFilePointer ( ) , end ) ; if ( compressor ! = null ) { int uncompressedLength = reader . readInt ( ) ; replayEnd = replayPos + uncompressedLength ; } else { replayEnd = end ; } if ( segmentId = = globalPosition . segment & & replayEnd < globalPosition . position ) continue ; FileDataInput sectionReader = reader ; String errorContext = desc . fileName ( ) ; boolean tolerateErrorsInSection = tolerateTruncation ; if ( compressor ! = null ) { tolerateErrorsInSection & = end = = reader . length ( ) | | end < <int> ; int start = ( int ) reader . getFilePointer ( ) ; try { int compressedLength = end - start ; if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , file , replayPos , replayEnd ) ; if ( compressedLength > buffer . length ) buffer = new byte [ ( int ) ( <float> * compressedLength ) ] ; reader . readFully ( buffer , <int> , compressedLength ) ; int uncompressedLength = replayEnd - replayPos ; if ( uncompressedLength > uncompressedBuffer . length ) uncompressedBuffer = new byte [ ( int ) ( <float> * uncompressedLength ) ] ; compressedLength = compressor . uncompress ( buffer , <int> , compressedLength , uncompressedBuffer , <int> ) ; sectionReader = new FileSegmentInputStream ( ByteBuffer . wrap ( uncompressedBuffer ) , reader . getPath ( ) , replayPos ) ; errorContext = <str> + start + <str> + errorContext ; } catch ( IOException | ArrayIndexOutOfBoundsException e ) { handleReplayError ( tolerateErrorsInSection , <str> , start , e ) ; continue ; } } if ( ! replaySyncSection ( sectionReader , replayEnd , desc , errorContext , tolerateErrorsInSection ) ) break ; } logger . debug ( <str> , file ) ; } } public boolean logAndCheckIfShouldSkip ( File file , CommitLogDescriptor desc ) { logger . debug ( <str> , file . getPath ( ) , desc . version , desc . getMessagingVersion ( ) , desc . compression ) ; if ( globalPosition . segment > desc . id ) { logger . trace ( <str> , file ) ; return true ; } return false ; } private boolean replaySyncSection ( FileDataInput reader , int end , CommitLogDescriptor desc , String errorContext , boolean tolerateErrors ) throws IOException { while ( reader . getFilePointer ( ) < end & & ! reader . isEOF ( ) ) { long mutationStart = reader . getFilePointer ( ) ; if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , mutationStart ) ; long claimedCRC32 ; int serializedSize ; try { serializedSize = reader . readInt ( ) ; if ( serializedSize = = LEGACY_END_OF_SEGMENT_MARKER ) { logger . trace ( <str> , reader . getFilePointer ( ) ) ; return false ; } if ( serializedSize < <int> ) { handleReplayError ( tolerateErrors , <str> , serializedSize , mutationStart , errorContext ) ; return false ; } long claimedSizeChecksum ; if ( desc . version < CommitLogDescriptor . VERSION_21 ) claimedSizeChecksum = reader . readLong ( ) ; else claimedSizeChecksum = reader . readInt ( ) & <hex> ; checksum . reset ( ) ; if ( desc . version < CommitLogDescriptor . VERSION_20 ) checksum . update ( serializedSize ) ; else updateChecksumInt ( checksum , serializedSize ) ; if ( checksum . getValue ( ) ! = claimedSizeChecksum ) { handleReplayError ( tolerateErrors , <str> , mutationStart , errorContext ) ; return false ; } if ( serializedSize > buffer . length ) buffer = new byte [ ( int ) ( <float> * serializedSize ) ] ; reader . readFully ( buffer , <int> , serializedSize ) ; if ( desc . version < CommitLogDescriptor . VERSION_21 ) claimedCRC32 = reader . readLong ( ) ; else claimedCRC32 = reader . readInt ( ) & <hex> ; } catch ( EOFException eof ) { handleReplayError ( tolerateErrors , <str> , mutationStart , errorContext ) ; return false ; } checksum . update ( buffer , <int> , serializedSize ) ; if ( claimedCRC32 ! = checksum . getValue ( ) ) { handleReplayError ( tolerateErrors , <str> , mutationStart , errorContext ) ; continue ; } replayMutation ( buffer , serializedSize , reader . getFilePointer ( ) , desc ) ; } return true ; } void replayMutation ( byte [ ] inputBuffer , int size , final long entryLocation , final CommitLogDescriptor desc ) throws IOException { final Mutation mutation ; try ( RebufferingInputStream bufIn = new DataInputBuffer ( inputBuffer , <int> , size ) ) { mutation = Mutation . serializer . deserialize ( bufIn , desc . getMessagingVersion ( ) , SerializationHelper . Flag . LOCAL ) ; for ( PartitionUpdate upd : mutation . getPartitionUpdates ( ) ) upd . validate ( ) ; } catch ( UnknownColumnFamilyException ex ) { if ( ex . cfId = = null ) return ; AtomicInteger i = invalidMutations . get ( ex . cfId ) ; if ( i = = null ) { i = new AtomicInteger ( <int> ) ; invalidMutations . put ( ex . cfId , i ) ; } else i . incrementAndGet ( ) ; return ; } catch ( Throwable t ) { JVMStabilityInspector . inspectThrowable ( t ) ; File f = File . createTempFile ( <str> , <str> ) ; try ( DataOutputStream out = new DataOutputStream ( new FileOutputStream ( f ) ) ) { out . write ( inputBuffer , <int> , size ) ; } handleReplayError ( false , <str> + <str> + <str> , f . getAbsolutePath ( ) , t ) ; return ; } if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , mutation . getKeyspaceName ( ) , mutation . key ( ) , <str> + StringUtils . join ( mutation . getPartitionUpdates ( ) . iterator ( ) , <str> ) + <str> ) ; pendingMutationBytes + = size ; futures . offer ( mutationInitiator . initiateMutation ( mutation , desc . id , size , entryLocation , this ) ) ; while ( futures . size ( ) > MAX_OUTSTANDING_REPLAY_COUNT | | pendingMutationBytes > MAX_OUTSTANDING_REPLAY_BYTES | | ( ! futures . isEmpty ( ) & & futures . peek ( ) . isDone ( ) ) ) { pendingMutationBytes - = FBUtilities . waitOnFuture ( futures . poll ( ) ) ; } } protected boolean pointInTimeExceeded ( Mutation fm ) { long restoreTarget = archiver . restorePointInTime ; for ( PartitionUpdate upd : fm . getPartitionUpdates ( ) ) { if ( archiver . precision . toMillis ( upd . maxTimestamp ( ) ) > restoreTarget ) return true ; } return false ; } static void handleReplayError ( boolean permissible , String message , Object . . . messageArgs ) throws IOException { String msg = String . format ( message , messageArgs ) ; IOException e = new CommitLogReplayException ( msg ) ; if ( permissible ) logger . error ( <str> , e ) ; else if ( Boolean . getBoolean ( IGNORE_REPLAY_ERRORS_PROPERTY ) ) logger . error ( <str> , e ) ; else if ( ! CommitLog . handleCommitError ( <str> , e ) ) { logger . error ( <str> + <str> + IGNORE_REPLAY_ERRORS_PROPERTY + <str> + <str> ) ; throw e ; } } @SuppressWarnings ( <str> ) public static class CommitLogReplayException extends IOException { public CommitLogReplayException ( String message , Throwable cause ) { super ( message , cause ) ; } public CommitLogReplayException ( String message ) { super ( message ) ; } } } 
