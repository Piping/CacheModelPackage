package org . elasticsearch . action . termvectors ; import com . carrotsearch . hppc . ObjectLongHashMap ; import com . carrotsearch . hppc . cursors . ObjectLongCursor ; import org . apache . lucene . index . Fields ; import org . apache . lucene . index . PostingsEnum ; import org . apache . lucene . index . Terms ; import org . apache . lucene . index . TermsEnum ; import org . apache . lucene . search . BoostAttribute ; import org . apache . lucene . util . * ; import org . elasticsearch . common . bytes . BytesReference ; import org . elasticsearch . common . io . stream . StreamInput ; import java . io . IOException ; import java . util . Arrays ; import java . util . Iterator ; import static org . apache . lucene . util . ArrayUtil . grow ; public final class TermVectorsFields extends Fields { private final ObjectLongHashMap < String > fieldMap ; private final BytesReference termVectors ; final boolean hasTermStatistic ; final boolean hasFieldStatistic ; public final boolean hasScores ; public TermVectorsFields ( BytesReference headerRef , BytesReference termVectors ) throws IOException { StreamInput header = StreamInput . wrap ( headerRef . toBytesArray ( ) ) ; fieldMap = new ObjectLongHashMap < > ( ) ; String headerString = header . readString ( ) ; assert headerString . equals ( <str> ) ; int version = header . readInt ( ) ; assert version = = - <int> ; hasTermStatistic = header . readBoolean ( ) ; hasFieldStatistic = header . readBoolean ( ) ; hasScores = header . readBoolean ( ) ; final int numFields = header . readVInt ( ) ; for ( int i = <int> ; i < numFields ; i + + ) { fieldMap . put ( ( header . readString ( ) ) , header . readVLong ( ) ) ; } header . close ( ) ; this . termVectors = termVectors ; } @Override public Iterator < String > iterator ( ) { final Iterator < ObjectLongCursor < String > > iterator = fieldMap . iterator ( ) ; return new Iterator < String > ( ) { @Override public boolean hasNext ( ) { return iterator . hasNext ( ) ; } @Override public String next ( ) { return iterator . next ( ) . key ; } @Override public void remove ( ) { throw new UnsupportedOperationException ( ) ; } } ; } @Override public Terms terms ( String field ) throws IOException { final int keySlot = fieldMap . indexOf ( field ) ; if ( keySlot < <int> ) { return null ; } long readOffset = fieldMap . indexGet ( keySlot ) ; return new TermVector ( termVectors , readOffset ) ; } @Override public int size ( ) { return fieldMap . size ( ) ; } private final class TermVector extends Terms { private final StreamInput perFieldTermVectorInput ; private final long readOffset ; private long numTerms ; private boolean hasPositions ; private boolean hasOffsets ; private boolean hasPayloads ; private long sumTotalTermFreq ; private long sumDocFreq ; private int docCount ; public TermVector ( BytesReference termVectors , long readOffset ) throws IOException { this . perFieldTermVectorInput = StreamInput . wrap ( termVectors . toBytesArray ( ) ) ; this . readOffset = readOffset ; reset ( ) ; } private void reset ( ) throws IOException { this . perFieldTermVectorInput . reset ( ) ; this . perFieldTermVectorInput . skip ( readOffset ) ; this . numTerms = perFieldTermVectorInput . readVLong ( ) ; this . hasPositions = perFieldTermVectorInput . readBoolean ( ) ; this . hasOffsets = perFieldTermVectorInput . readBoolean ( ) ; this . hasPayloads = perFieldTermVectorInput . readBoolean ( ) ; this . sumTotalTermFreq = hasFieldStatistic ? readPotentiallyNegativeVLong ( perFieldTermVectorInput ) : - <int> ; this . sumDocFreq = hasFieldStatistic ? readPotentiallyNegativeVLong ( perFieldTermVectorInput ) : - <int> ; this . docCount = hasFieldStatistic ? readPotentiallyNegativeVInt ( perFieldTermVectorInput ) : - <int> ; } @Override public TermsEnum iterator ( ) throws IOException { reset ( ) ; return new TermsEnum ( ) { int currentTerm = <int> ; int freq = <int> ; int docFreq = - <int> ; long totalTermFrequency = - <int> ; int [ ] positions = new int [ <int> ] ; int [ ] startOffsets = new int [ <int> ] ; int [ ] endOffsets = new int [ <int> ] ; BytesRefBuilder [ ] payloads = new BytesRefBuilder [ <int> ] ; final BytesRefBuilder spare = new BytesRefBuilder ( ) ; BoostAttribute boostAtt = this . attributes ( ) . addAttribute ( BoostAttribute . class ) ; @Override public BytesRef next ( ) throws IOException { if ( currentTerm + + < numTerms ) { int termVectorSize = perFieldTermVectorInput . readVInt ( ) ; spare . grow ( termVectorSize ) ; perFieldTermVectorInput . readBytes ( spare . bytes ( ) , <int> , termVectorSize ) ; spare . setLength ( termVectorSize ) ; if ( hasTermStatistic ) { docFreq = readPotentiallyNegativeVInt ( perFieldTermVectorInput ) ; totalTermFrequency = readPotentiallyNegativeVLong ( perFieldTermVectorInput ) ; } freq = readPotentiallyNegativeVInt ( perFieldTermVectorInput ) ; growBuffers ( ) ; writeInfos ( perFieldTermVectorInput ) ; if ( hasScores ) { boostAtt . setBoost ( perFieldTermVectorInput . readFloat ( ) ) ; } return spare . get ( ) ; } else { return null ; } } private void writeInfos ( final StreamInput input ) throws IOException { for ( int i = <int> ; i < freq ; i + + ) { if ( hasPositions ) { positions [ i ] = input . readVInt ( ) ; } if ( hasOffsets ) { startOffsets [ i ] = input . readVInt ( ) ; endOffsets [ i ] = input . readVInt ( ) ; } if ( hasPayloads ) { int payloadLength = input . readVInt ( ) ; if ( payloads [ i ] = = null ) { payloads [ i ] = new BytesRefBuilder ( ) ; } payloads [ i ] . grow ( payloadLength ) ; input . readBytes ( payloads [ i ] . bytes ( ) , <int> , payloadLength ) ; payloads [ i ] . setLength ( payloadLength ) ; } } } private void growBuffers ( ) { if ( hasPositions ) { positions = grow ( positions , freq ) ; } if ( hasOffsets ) { startOffsets = grow ( startOffsets , freq ) ; endOffsets = grow ( endOffsets , freq ) ; } if ( hasPayloads ) { if ( payloads . length < freq ) { payloads = Arrays . copyOf ( payloads , ArrayUtil . oversize ( freq , RamUsageEstimator . NUM_BYTES_OBJECT_REF ) ) ; } } } @Override public SeekStatus seekCeil ( BytesRef text ) throws IOException { throw new UnsupportedOperationException ( ) ; } @Override public void seekExact ( long ord ) throws IOException { throw new UnsupportedOperationException ( <str> ) ; } @Override public BytesRef term ( ) throws IOException { return spare . get ( ) ; } @Override public long ord ( ) throws IOException { throw new UnsupportedOperationException ( <str> ) ; } @Override public int docFreq ( ) throws IOException { return docFreq ; } @Override public long totalTermFreq ( ) throws IOException { return totalTermFrequency ; } @Override public PostingsEnum postings ( PostingsEnum reuse , int flags ) throws IOException { final TermVectorPostingsEnum retVal = ( reuse instanceof TermVectorPostingsEnum ? ( TermVectorPostingsEnum ) reuse : new TermVectorPostingsEnum ( ) ) ; return retVal . reset ( hasPositions ? positions : null , hasOffsets ? startOffsets : null , hasOffsets ? endOffsets : null , hasPayloads ? payloads : null , freq ) ; } } ; } @Override public long size ( ) throws IOException { return numTerms ; } @Override public long getSumTotalTermFreq ( ) throws IOException { return sumTotalTermFreq ; } @Override public long getSumDocFreq ( ) throws IOException { return sumDocFreq ; } @Override public int getDocCount ( ) throws IOException { return docCount ; } @Override public boolean hasFreqs ( ) { return true ; } @Override public boolean hasOffsets ( ) { return hasOffsets ; } @Override public boolean hasPositions ( ) { return hasPositions ; } @Override public boolean hasPayloads ( ) { return hasPayloads ; } } private final class TermVectorPostingsEnum extends PostingsEnum { private boolean hasPositions ; private boolean hasOffsets ; private boolean hasPayloads ; int curPos = - <int> ; int doc = - <int> ; private int freq ; private int [ ] startOffsets ; private int [ ] positions ; private BytesRefBuilder [ ] payloads ; private int [ ] endOffsets ; private PostingsEnum reset ( int [ ] positions , int [ ] startOffsets , int [ ] endOffsets , BytesRefBuilder [ ] payloads , int freq ) { curPos = - <int> ; doc = - <int> ; this . hasPositions = positions ! = null ; this . hasOffsets = startOffsets ! = null ; this . hasPayloads = payloads ! = null ; this . freq = freq ; this . startOffsets = startOffsets ; this . endOffsets = endOffsets ; this . payloads = payloads ; this . positions = positions ; return this ; } @Override public int nextDoc ( ) throws IOException { return doc = ( doc = = - <int> ? <int> : NO_MORE_DOCS ) ; } @Override public int docID ( ) { return doc ; } @Override public int advance ( int target ) throws IOException { while ( nextDoc ( ) < target & & doc ! = NO_MORE_DOCS ) { } return doc ; } @Override public int freq ( ) throws IOException { return freq ; } @Override public int startOffset ( ) throws IOException { assert curPos < freq & & curPos > = <int> ; return hasOffsets ? startOffsets [ curPos ] : - <int> ; } @Override public int nextPosition ( ) throws IOException { assert curPos + <int> < freq ; + + curPos ; return hasPositions ? positions [ curPos ] : - <int> ; } @Override public BytesRef getPayload ( ) throws IOException { assert curPos < freq & & curPos > = <int> ; if ( hasPayloads ) { final BytesRefBuilder payload = payloads [ curPos ] ; if ( payload ! = null ) { return payload . get ( ) ; } } return null ; } @Override public int endOffset ( ) throws IOException { assert curPos < freq & & curPos > = <int> ; return hasOffsets ? endOffsets [ curPos ] : - <int> ; } @Override public long cost ( ) { return <int> ; } } int readPotentiallyNegativeVInt ( StreamInput stream ) throws IOException { return stream . readVInt ( ) - <int> ; } long readPotentiallyNegativeVLong ( StreamInput stream ) throws IOException { return stream . readVLong ( ) - <int> ; } } 
