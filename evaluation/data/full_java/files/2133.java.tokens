package org . nd4j . linalg . api . parallel . tasks . cpu . accumulation ; import org . nd4j . linalg . api . ndarray . INDArray ; import org . nd4j . linalg . api . ops . Accumulation ; import org . nd4j . linalg . api . ops . executioner . OpExecutionerUtil ; import org . nd4j . linalg . api . parallel . tasks . Task ; import org . nd4j . linalg . api . parallel . tasks . cpu . BaseCPUTask ; import java . util . ArrayList ; import java . util . List ; import java . util . concurrent . RecursiveTask ; public class CPUAccumulationViaTensorTask extends BaseCPUTask < Double > { protected final Accumulation op ; protected final boolean outerTask ; protected List < Task < Double > > subTasks ; public CPUAccumulationViaTensorTask ( Accumulation op , int threshold , boolean outerTask ) { super ( op , threshold ) ; this . op = op ; this . outerTask = outerTask ; } @Override public Double blockUntilComplete ( ) { if ( future = = null ) { invokeAsync ( ) ; } Double accum ; try { accum = future . get ( ) ; } catch ( Exception e ) { throw new RuntimeException ( e ) ; } if ( subTasks ! = null ) { accum = op . zeroDouble ( ) ; for ( Task < Double > task : subTasks ) { double subAccum = task . blockUntilComplete ( ) ; accum = op . combineSubResults ( accum , subAccum ) ; } } if ( outerTask & & subTasks ! = null ) { return op . getAndSetFinalResult ( accum ) ; } return accum ; } @Override public Double call ( ) { return execute ( false ) ; } @Override protected Double compute ( ) { double out = execute ( true ) ; if ( outerTask ) { return op . getAndSetFinalResult ( out ) ; } else { return out ; } } private Double execute ( final boolean forkJoin ) { INDArray x = op . x ( ) ; INDArray y = op . y ( ) ; int tensorDim ; if ( y = = null ) tensorDim = OpExecutionerUtil . chooseElementWiseTensorDimension ( x ) ; else tensorDim = OpExecutionerUtil . chooseElementWiseTensorDimension ( x , y ) ; int nTensors = x . tensorssAlongDimension ( tensorDim ) ; List < RecursiveTask < Double > > fjTasks = null ; if ( forkJoin ) fjTasks = new ArrayList < > ( nTensors ) ; else subTasks = new ArrayList < > ( nTensors ) ; if ( nTensors = = <int> ) { CPUAccumulationTask task = new CPUAccumulationTask ( op , threshold , false ) ; if ( forkJoin ) { return task . invoke ( ) ; } else { task . invokeAsync ( ) ; subTasks . add ( task ) ; return null ; } } else { if ( x . rank ( ) = = <int> ) { OpExecutionerUtil . Tensor1DStats tsx = OpExecutionerUtil . get1DTensorStats ( x , tensorDim ) ; int n = tsx . getTensorLength ( ) ; int incrX = tsx . getElementWiseStride ( ) ; if ( y = = null ) { for ( int i = <int> ; i < nTensors ; i + + ) { int offsetX = tsx . getFirstTensorOffset ( ) + i * tsx . getTensorStartSeparation ( ) ; CPUAccumulationTask task = new CPUAccumulationTask ( op , threshold , n , offsetX , <int> , incrX , <int> , false ) ; if ( forkJoin ) { task . fork ( ) ; fjTasks . add ( task ) ; } else { task . invokeAsync ( ) ; subTasks . add ( task ) ; } } } else { OpExecutionerUtil . Tensor1DStats tsy = OpExecutionerUtil . get1DTensorStats ( y , tensorDim ) ; int incrY = tsy . getElementWiseStride ( ) ; for ( int i = <int> ; i < nTensors ; i + + ) { int offsetX = tsx . getFirstTensorOffset ( ) + i * tsx . getTensorStartSeparation ( ) ; int offsetY = tsy . getFirstTensorOffset ( ) + i * tsy . getTensorStartSeparation ( ) ; CPUAccumulationTask task = new CPUAccumulationTask ( op , threshold , n , offsetX , offsetY , incrX , incrY , false ) ; if ( forkJoin ) { task . fork ( ) ; fjTasks . add ( task ) ; } else { task . invokeAsync ( ) ; subTasks . add ( task ) ; } } } } else { for ( int i = <int> ; i < nTensors ; i + + ) { CPUAccumulationTask task = new CPUAccumulationTask ( op , threshold , i , tensorDim , false ) ; if ( forkJoin ) { task . fork ( ) ; fjTasks . add ( task ) ; } else { task . invokeAsync ( ) ; subTasks . add ( task ) ; } } } } if ( forkJoin ) { double accum = op . zeroDouble ( ) ; for ( RecursiveTask < Double > task : fjTasks ) { accum = op . combineSubResults ( accum , task . join ( ) ) ; } return accum ; } else return null ; } } 
