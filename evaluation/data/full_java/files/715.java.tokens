package org . apache . cassandra . io . sstable . format . big ; import java . io . * ; import java . util . Collection ; import java . util . Map ; import org . apache . cassandra . db . * ; import org . apache . cassandra . db . lifecycle . LifecycleTransaction ; import org . apache . cassandra . db . transform . Transformation ; import org . apache . cassandra . io . sstable . * ; import org . apache . cassandra . io . sstable . format . SSTableFlushObserver ; import org . apache . cassandra . io . sstable . format . SSTableReader ; import org . apache . cassandra . io . sstable . format . SSTableWriter ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . config . CFMetaData ; import org . apache . cassandra . config . DatabaseDescriptor ; import org . apache . cassandra . db . rows . * ; import org . apache . cassandra . io . FSWriteError ; import org . apache . cassandra . io . compress . CompressedSequentialWriter ; import org . apache . cassandra . io . sstable . metadata . MetadataCollector ; import org . apache . cassandra . io . sstable . metadata . MetadataComponent ; import org . apache . cassandra . io . sstable . metadata . MetadataType ; import org . apache . cassandra . io . sstable . metadata . StatsMetadata ; import org . apache . cassandra . io . util . * ; import org . apache . cassandra . utils . ByteBufferUtil ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . FilterFactory ; import org . apache . cassandra . utils . IFilter ; import org . apache . cassandra . utils . concurrent . Transactional ; import org . apache . cassandra . utils . SyncUtil ; public class BigTableWriter extends SSTableWriter { private static final Logger logger = LoggerFactory . getLogger ( BigTableWriter . class ) ; private final IndexWriter iwriter ; private final SegmentedFile . Builder dbuilder ; protected final SequentialWriter dataFile ; private DecoratedKey lastWrittenKey ; private FileMark dataMark ; public BigTableWriter ( Descriptor descriptor , Long keyCount , Long repairedAt , CFMetaData metadata , MetadataCollector metadataCollector , SerializationHeader header , Collection < SSTableFlushObserver > observers , LifecycleTransaction txn ) { super ( descriptor , keyCount , repairedAt , metadata , metadataCollector , header , observers ) ; txn . trackNew ( this ) ; if ( compression ) { dataFile = SequentialWriter . open ( getFilename ( ) , descriptor . filenameFor ( Component . COMPRESSION_INFO ) , metadata . params . compression , metadataCollector ) ; dbuilder = SegmentedFile . getCompressedBuilder ( ( CompressedSequentialWriter ) dataFile ) ; } else { dataFile = SequentialWriter . open ( new File ( getFilename ( ) ) , new File ( descriptor . filenameFor ( Component . CRC ) ) ) ; dbuilder = SegmentedFile . getBuilder ( DatabaseDescriptor . getDiskAccessMode ( ) , false ) ; } iwriter = new IndexWriter ( keyCount , dataFile ) ; } public void mark ( ) { dataMark = dataFile . mark ( ) ; iwriter . mark ( ) ; } public void resetAndTruncate ( ) { dataFile . resetAndTruncate ( dataMark ) ; iwriter . resetAndTruncate ( ) ; } protected long beforeAppend ( DecoratedKey decoratedKey ) { assert decoratedKey ! = null : <str> ; if ( lastWrittenKey ! = null & & lastWrittenKey . compareTo ( decoratedKey ) > = <int> ) throw new RuntimeException ( <str> + lastWrittenKey + <str> + decoratedKey + <str> + getFilename ( ) ) ; return ( lastWrittenKey = = null ) ? <int> : dataFile . position ( ) ; } private void afterAppend ( DecoratedKey decoratedKey , long dataEnd , RowIndexEntry index ) throws IOException { metadataCollector . addKey ( decoratedKey . getKey ( ) ) ; lastWrittenKey = decoratedKey ; last = lastWrittenKey ; if ( first = = null ) first = lastWrittenKey ; if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , decoratedKey , dataEnd ) ; iwriter . append ( decoratedKey , index , dataEnd ) ; } public RowIndexEntry append ( UnfilteredRowIterator iterator ) { DecoratedKey key = iterator . partitionKey ( ) ; if ( key . getKey ( ) . remaining ( ) > FBUtilities . MAX_UNSIGNED_SHORT ) { logger . error ( <str> , key . getKey ( ) . remaining ( ) , FBUtilities . MAX_UNSIGNED_SHORT ) ; return null ; } if ( iterator . isEmpty ( ) ) return null ; long startPosition = beforeAppend ( key ) ; observers . forEach ( ( o ) - > o . startPartition ( key , iwriter . indexFile . position ( ) ) ) ; try ( UnfilteredRowIterator collecting = Transformation . apply ( iterator , new StatsCollector ( metadataCollector ) ) ) { ColumnIndex index = ColumnIndex . writeAndBuildIndex ( collecting , dataFile , header , observers , descriptor . version ) ; RowIndexEntry entry = RowIndexEntry . create ( startPosition , collecting . partitionLevelDeletion ( ) , index ) ; long endPosition = dataFile . position ( ) ; long rowSize = endPosition - startPosition ; maybeLogLargePartitionWarning ( key , rowSize ) ; metadataCollector . addPartitionSizeInBytes ( rowSize ) ; afterAppend ( key , endPosition , entry ) ; return entry ; } catch ( IOException e ) { throw new FSWriteError ( e , dataFile . getPath ( ) ) ; } } private void maybeLogLargePartitionWarning ( DecoratedKey key , long rowSize ) { if ( rowSize > DatabaseDescriptor . getCompactionLargePartitionWarningThreshold ( ) ) { String keyString = metadata . getKeyValidator ( ) . getString ( key . getKey ( ) ) ; logger . warn ( <str> , metadata . ksName , metadata . cfName , keyString , rowSize ) ; } } private static class StatsCollector extends Transformation { private final MetadataCollector collector ; private int cellCount ; StatsCollector ( MetadataCollector collector ) { this . collector = collector ; } @Override public Row applyToStatic ( Row row ) { if ( ! row . isEmpty ( ) ) cellCount + = Rows . collectStats ( row , collector ) ; return row ; } @Override public Row applyToRow ( Row row ) { collector . updateClusteringValues ( row . clustering ( ) ) ; cellCount + = Rows . collectStats ( row , collector ) ; return row ; } @Override public RangeTombstoneMarker applyToMarker ( RangeTombstoneMarker marker ) { collector . updateClusteringValues ( marker . clustering ( ) ) ; if ( marker . isBoundary ( ) ) { RangeTombstoneBoundaryMarker bm = ( RangeTombstoneBoundaryMarker ) marker ; collector . update ( bm . endDeletionTime ( ) ) ; collector . update ( bm . startDeletionTime ( ) ) ; } else { collector . update ( ( ( RangeTombstoneBoundMarker ) marker ) . deletionTime ( ) ) ; } return marker ; } @Override public void onPartitionClose ( ) { collector . addCellPerPartitionCount ( cellCount ) ; } @Override public DeletionTime applyToDeletion ( DeletionTime deletionTime ) { collector . update ( deletionTime ) ; return deletionTime ; } } @SuppressWarnings ( <str> ) public SSTableReader openEarly ( ) { IndexSummaryBuilder . ReadableBoundary boundary = iwriter . getMaxReadable ( ) ; if ( boundary = = null ) return null ; StatsMetadata stats = statsMetadata ( ) ; assert boundary . indexLength > <int> & & boundary . dataLength > <int> ; IndexSummary indexSummary = iwriter . summary . build ( metadata . partitioner , boundary ) ; SegmentedFile ifile = iwriter . builder . buildIndex ( descriptor , indexSummary , boundary ) ; SegmentedFile dfile = dbuilder . buildData ( descriptor , stats , boundary ) ; SSTableReader sstable = SSTableReader . internalOpen ( descriptor , components , metadata , ifile , dfile , indexSummary , iwriter . bf . sharedCopy ( ) , maxDataAge , stats , SSTableReader . OpenReason . EARLY , header ) ; sstable . first = getMinimalKey ( first ) ; sstable . last = getMinimalKey ( boundary . lastKey ) ; return sstable ; } public SSTableReader openFinalEarly ( ) { dataFile . sync ( ) ; iwriter . indexFile . sync ( ) ; return openFinal ( descriptor , SSTableReader . OpenReason . EARLY ) ; } @SuppressWarnings ( <str> ) private SSTableReader openFinal ( Descriptor desc , SSTableReader . OpenReason openReason ) { if ( maxDataAge < <int> ) maxDataAge = System . currentTimeMillis ( ) ; StatsMetadata stats = statsMetadata ( ) ; IndexSummary indexSummary = iwriter . summary . build ( this . metadata . partitioner ) ; SegmentedFile ifile = iwriter . builder . buildIndex ( desc , indexSummary ) ; SegmentedFile dfile = dbuilder . buildData ( desc , stats ) ; SSTableReader sstable = SSTableReader . internalOpen ( desc , components , this . metadata , ifile , dfile , indexSummary , iwriter . bf . sharedCopy ( ) , maxDataAge , stats , openReason , header ) ; sstable . first = getMinimalKey ( first ) ; sstable . last = getMinimalKey ( last ) ; return sstable ; } protected SSTableWriter . TransactionalProxy txnProxy ( ) { return new TransactionalProxy ( ) ; } class TransactionalProxy extends SSTableWriter . TransactionalProxy { protected void doPrepare ( ) { iwriter . prepareToCommit ( ) ; dataFile . setDescriptor ( descriptor ) . prepareToCommit ( ) ; writeMetadata ( descriptor , finalizeMetadata ( ) ) ; SSTable . appendTOC ( descriptor , components ) ; if ( openResult ) finalReader = openFinal ( descriptor , SSTableReader . OpenReason . NORMAL ) ; } protected Throwable doCommit ( Throwable accumulate ) { accumulate = dataFile . commit ( accumulate ) ; accumulate = iwriter . commit ( accumulate ) ; return accumulate ; } @Override protected Throwable doPostCleanup ( Throwable accumulate ) { accumulate = dbuilder . close ( accumulate ) ; return accumulate ; } protected Throwable doAbort ( Throwable accumulate ) { accumulate = iwriter . abort ( accumulate ) ; accumulate = dataFile . abort ( accumulate ) ; return accumulate ; } } private static void writeMetadata ( Descriptor desc , Map < MetadataType , MetadataComponent > components ) { File file = new File ( desc . filenameFor ( Component . STATS ) ) ; try ( SequentialWriter out = SequentialWriter . open ( file ) ) { desc . getMetadataSerializer ( ) . serialize ( components , out , desc . version ) ; out . setDescriptor ( desc ) . finish ( ) ; } catch ( IOException e ) { throw new FSWriteError ( e , file . getPath ( ) ) ; } } public long getFilePointer ( ) { return dataFile . position ( ) ; } public long getOnDiskFilePointer ( ) { return dataFile . getOnDiskFilePointer ( ) ; } class IndexWriter extends AbstractTransactional implements Transactional { private final SequentialWriter indexFile ; public final SegmentedFile . Builder builder ; public final IndexSummaryBuilder summary ; public final IFilter bf ; private FileMark mark ; IndexWriter ( long keyCount , final SequentialWriter dataFile ) { indexFile = SequentialWriter . open ( new File ( descriptor . filenameFor ( Component . PRIMARY_INDEX ) ) ) ; builder = SegmentedFile . getBuilder ( DatabaseDescriptor . getIndexAccessMode ( ) , false ) ; summary = new IndexSummaryBuilder ( keyCount , metadata . params . minIndexInterval , Downsampling . BASE_SAMPLING_LEVEL ) ; bf = FilterFactory . getFilter ( keyCount , metadata . params . bloomFilterFpChance , true , descriptor . version . hasOldBfHashOrder ( ) ) ; indexFile . setPostFlushListener ( new Runnable ( ) { public void run ( ) { summary . markIndexSynced ( indexFile . getLastFlushOffset ( ) ) ; } } ) ; dataFile . setPostFlushListener ( new Runnable ( ) { public void run ( ) { summary . markDataSynced ( dataFile . getLastFlushOffset ( ) ) ; } } ) ; } IndexSummaryBuilder . ReadableBoundary getMaxReadable ( ) { return summary . getLastReadableBoundary ( ) ; } public void append ( DecoratedKey key , RowIndexEntry indexEntry , long dataEnd ) throws IOException { bf . add ( key ) ; long indexStart = indexFile . position ( ) ; try { ByteBufferUtil . writeWithShortLength ( key . getKey ( ) , indexFile ) ; rowIndexEntrySerializer . serialize ( indexEntry , indexFile ) ; } catch ( IOException e ) { throw new FSWriteError ( e , indexFile . getPath ( ) ) ; } long indexEnd = indexFile . position ( ) ; if ( logger . isTraceEnabled ( ) ) logger . trace ( <str> , indexEntry , indexStart ) ; summary . maybeAddEntry ( key , indexStart , indexEnd , dataEnd ) ; } void flushBf ( ) { if ( components . contains ( Component . FILTER ) ) { String path = descriptor . filenameFor ( Component . FILTER ) ; try ( FileOutputStream fos = new FileOutputStream ( path ) ; DataOutputStreamPlus stream = new BufferedDataOutputStreamPlus ( fos ) ) { FilterFactory . serialize ( bf , stream ) ; stream . flush ( ) ; SyncUtil . sync ( fos ) ; } catch ( IOException e ) { throw new FSWriteError ( e , path ) ; } } } public void mark ( ) { mark = indexFile . mark ( ) ; } public void resetAndTruncate ( ) { indexFile . resetAndTruncate ( mark ) ; } protected void doPrepare ( ) { flushBf ( ) ; long position = iwriter . indexFile . position ( ) ; iwriter . indexFile . setDescriptor ( descriptor ) . prepareToCommit ( ) ; FileUtils . truncate ( iwriter . indexFile . getPath ( ) , position ) ; summary . prepareToCommit ( ) ; try ( IndexSummary summary = iwriter . summary . build ( getPartitioner ( ) ) ) { SSTableReader . saveSummary ( descriptor , first , last , iwriter . builder , dbuilder , summary ) ; } } protected Throwable doCommit ( Throwable accumulate ) { return indexFile . commit ( accumulate ) ; } protected Throwable doAbort ( Throwable accumulate ) { return indexFile . abort ( accumulate ) ; } @Override protected Throwable doPostCleanup ( Throwable accumulate ) { accumulate = summary . close ( accumulate ) ; accumulate = bf . close ( accumulate ) ; accumulate = builder . close ( accumulate ) ; return accumulate ; } } } 
