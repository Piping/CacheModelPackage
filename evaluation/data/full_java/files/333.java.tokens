package org . apache . cassandra . db ; import java . io . IOException ; import java . nio . ByteBuffer ; import java . util . * ; import com . google . common . collect . Lists ; import org . slf4j . Logger ; import org . slf4j . LoggerFactory ; import org . apache . cassandra . config . * ; import org . apache . cassandra . cql3 . Operator ; import org . apache . cassandra . db . filter . * ; import org . apache . cassandra . db . monitoring . MonitorableImpl ; import org . apache . cassandra . db . partitions . * ; import org . apache . cassandra . db . rows . * ; import org . apache . cassandra . db . transform . StoppingTransformation ; import org . apache . cassandra . db . transform . Transformation ; import org . apache . cassandra . dht . AbstractBounds ; import org . apache . cassandra . index . Index ; import org . apache . cassandra . index . IndexNotAvailableException ; import org . apache . cassandra . io . IVersionedSerializer ; import org . apache . cassandra . io . util . DataInputPlus ; import org . apache . cassandra . io . util . DataOutputPlus ; import org . apache . cassandra . metrics . TableMetrics ; import org . apache . cassandra . net . MessageOut ; import org . apache . cassandra . net . MessagingService ; import org . apache . cassandra . schema . IndexMetadata ; import org . apache . cassandra . schema . UnknownIndexException ; import org . apache . cassandra . service . ClientWarn ; import org . apache . cassandra . tracing . Tracing ; import org . apache . cassandra . utils . ByteBufferUtil ; import org . apache . cassandra . utils . FBUtilities ; import org . apache . cassandra . utils . Pair ; public abstract class ReadCommand extends MonitorableImpl implements ReadQuery { private static final int TEST_ITERATION_DELAY_MILLIS = Integer . valueOf ( System . getProperty ( <str> , <str> ) ) ; protected static final Logger logger = LoggerFactory . getLogger ( ReadCommand . class ) ; public static final IVersionedSerializer < ReadCommand > serializer = new Serializer ( ) ; public static final IVersionedSerializer < ReadCommand > rangeSliceSerializer = new RangeSliceSerializer ( ) ; public static final IVersionedSerializer < ReadCommand > legacyRangeSliceCommandSerializer = new LegacyRangeSliceCommandSerializer ( ) ; public static final IVersionedSerializer < ReadCommand > legacyPagedRangeCommandSerializer = new LegacyPagedRangeCommandSerializer ( ) ; public static final IVersionedSerializer < ReadCommand > legacyReadCommandSerializer = new LegacyReadCommandSerializer ( ) ; private final Kind kind ; private final CFMetaData metadata ; private final int nowInSec ; private final ColumnFilter columnFilter ; private final RowFilter rowFilter ; private final DataLimits limits ; protected Optional < IndexMetadata > index = Optional . empty ( ) ; private boolean indexManagerQueried = false ; private boolean isDigestQuery ; private int digestVersion ; private final boolean isForThrift ; protected static abstract class SelectionDeserializer { public abstract ReadCommand deserialize ( DataInputPlus in , int version , boolean isDigest , int digestVersion , boolean isForThrift , CFMetaData metadata , int nowInSec , ColumnFilter columnFilter , RowFilter rowFilter , DataLimits limits , Optional < IndexMetadata > index ) throws IOException ; } protected enum Kind { SINGLE_PARTITION ( SinglePartitionReadCommand . selectionDeserializer ) , PARTITION_RANGE ( PartitionRangeReadCommand . selectionDeserializer ) ; private final SelectionDeserializer selectionDeserializer ; Kind ( SelectionDeserializer selectionDeserializer ) { this . selectionDeserializer = selectionDeserializer ; } } protected ReadCommand ( Kind kind , boolean isDigestQuery , int digestVersion , boolean isForThrift , CFMetaData metadata , int nowInSec , ColumnFilter columnFilter , RowFilter rowFilter , DataLimits limits ) { this . kind = kind ; this . isDigestQuery = isDigestQuery ; this . digestVersion = digestVersion ; this . isForThrift = isForThrift ; this . metadata = metadata ; this . nowInSec = nowInSec ; this . columnFilter = columnFilter ; this . rowFilter = rowFilter ; this . limits = limits ; } protected abstract void serializeSelection ( DataOutputPlus out , int version ) throws IOException ; protected abstract long selectionSerializedSize ( int version ) ; public CFMetaData metadata ( ) { return metadata ; } public int nowInSec ( ) { return nowInSec ; } public abstract long getTimeout ( ) ; public ColumnFilter columnFilter ( ) { return columnFilter ; } public RowFilter rowFilter ( ) { return rowFilter ; } public DataLimits limits ( ) { return limits ; } public boolean isDigestQuery ( ) { return isDigestQuery ; } public int digestVersion ( ) { return digestVersion ; } public ReadCommand setIsDigestQuery ( boolean isDigestQuery ) { this . isDigestQuery = isDigestQuery ; return this ; } public ReadCommand setDigestVersion ( int digestVersion ) { this . digestVersion = digestVersion ; return this ; } public boolean isForThrift ( ) { return isForThrift ; } public abstract ClusteringIndexFilter clusteringIndexFilter ( DecoratedKey key ) ; public abstract ReadCommand copy ( ) ; protected abstract UnfilteredPartitionIterator queryStorage ( ColumnFamilyStore cfs , ReadExecutionController executionController ) ; protected abstract int oldestUnrepairedTombstone ( ) ; public ReadResponse createResponse ( UnfilteredPartitionIterator iterator , ColumnFilter selection ) { return isDigestQuery ( ) ? ReadResponse . createDigestResponse ( iterator , digestVersion ) : ReadResponse . createDataResponse ( iterator , selection ) ; } public long indexSerializedSize ( int version ) { if ( index . isPresent ( ) ) return IndexMetadata . serializer . serializedSize ( index . get ( ) , version ) ; else return <int> ; } public Index getIndex ( ColumnFamilyStore cfs ) { if ( index . isPresent ( ) ) return cfs . indexManager . getIndex ( index . get ( ) ) ; if ( indexManagerQueried ) return null ; Index selected = cfs . indexManager . getBestIndexFor ( this ) ; indexManagerQueried = true ; if ( selected = = null ) return null ; index = Optional . of ( selected . getIndexMetadata ( ) ) ; return selected ; } @SuppressWarnings ( <str> ) public UnfilteredPartitionIterator executeLocally ( ReadExecutionController executionController ) { long startTimeNanos = System . nanoTime ( ) ; ColumnFamilyStore cfs = Keyspace . openAndGetStore ( metadata ( ) ) ; Index index = getIndex ( cfs ) ; Index . Searcher searcher = null ; if ( index ! = null ) { if ( ! cfs . indexManager . isIndexQueryable ( index ) ) throw new IndexNotAvailableException ( index ) ; searcher = index . searcherFor ( this ) ; Tracing . trace ( <str> , cfs . metadata . ksName , cfs . metadata . cfName , index . getIndexMetadata ( ) . name ) ; } UnfilteredPartitionIterator resultIterator = searcher = = null ? queryStorage ( cfs , executionController ) : searcher . search ( executionController ) ; try { resultIterator = withStateTracking ( resultIterator ) ; resultIterator = withMetricsRecording ( withoutPurgeableTombstones ( resultIterator , cfs ) , cfs . metric , startTimeNanos ) ; RowFilter updatedFilter = searcher = = null ? rowFilter ( ) : index . getPostIndexQueryFilter ( rowFilter ( ) ) ; return limits ( ) . filter ( updatedFilter . filter ( resultIterator , nowInSec ( ) ) , nowInSec ( ) ) ; } catch ( RuntimeException | Error e ) { resultIterator . close ( ) ; throw e ; } } protected abstract void recordLatency ( TableMetrics metric , long latencyNanos ) ; public PartitionIterator executeInternal ( ReadExecutionController controller ) { return UnfilteredPartitionIterators . filter ( executeLocally ( controller ) , nowInSec ( ) ) ; } public ReadExecutionController executionController ( ) { return ReadExecutionController . forCommand ( this ) ; } private UnfilteredPartitionIterator withMetricsRecording ( UnfilteredPartitionIterator iter , final TableMetrics metric , final long startTimeNanos ) { class MetricRecording extends Transformation < UnfilteredRowIterator > { private final int failureThreshold = DatabaseDescriptor . getTombstoneFailureThreshold ( ) ; private final int warningThreshold = DatabaseDescriptor . getTombstoneWarnThreshold ( ) ; private final boolean respectTombstoneThresholds = ! Schema . isSystemKeyspace ( ReadCommand . this . metadata ( ) . ksName ) ; private int liveRows = <int> ; private int tombstones = <int> ; private DecoratedKey currentKey ; @Override public UnfilteredRowIterator applyToPartition ( UnfilteredRowIterator iter ) { currentKey = iter . partitionKey ( ) ; return Transformation . apply ( iter , this ) ; } @Override public Row applyToStatic ( Row row ) { return applyToRow ( row ) ; } @Override public Row applyToRow ( Row row ) { if ( row . hasLiveData ( ReadCommand . this . nowInSec ( ) ) ) + + liveRows ; for ( Cell cell : row . cells ( ) ) { if ( ! cell . isLive ( ReadCommand . this . nowInSec ( ) ) ) countTombstone ( row . clustering ( ) ) ; } return row ; } @Override public RangeTombstoneMarker applyToMarker ( RangeTombstoneMarker marker ) { countTombstone ( marker . clustering ( ) ) ; return marker ; } private void countTombstone ( ClusteringPrefix clustering ) { + + tombstones ; if ( tombstones > failureThreshold & & respectTombstoneThresholds ) { String query = ReadCommand . this . toCQLString ( ) ; Tracing . trace ( <str> , failureThreshold , query ) ; throw new TombstoneOverwhelmingException ( tombstones , query , ReadCommand . this . metadata ( ) , currentKey , clustering ) ; } } @Override public void onClose ( ) { recordLatency ( metric , System . nanoTime ( ) - startTimeNanos ) ; metric . tombstoneScannedHistogram . update ( tombstones ) ; metric . liveScannedHistogram . update ( liveRows ) ; boolean warnTombstones = tombstones > warningThreshold & & respectTombstoneThresholds ; if ( warnTombstones ) { String msg = String . format ( <str> , liveRows , tombstones , ReadCommand . this . toCQLString ( ) ) ; ClientWarn . warn ( msg ) ; logger . warn ( msg ) ; } Tracing . trace ( <str> , liveRows , tombstones , ( warnTombstones ? <str> : <str> ) ) ; } } ; return Transformation . apply ( iter , new MetricRecording ( ) ) ; } protected class CheckForAbort extends StoppingTransformation < BaseRowIterator < ? > > { protected BaseRowIterator < ? > applyToPartition ( BaseRowIterator partition ) { maybeAbort ( ) ; return partition ; } protected Row applyToRow ( Row row ) { maybeAbort ( ) ; return row ; } private void maybeAbort ( ) { if ( isAborted ( ) ) stop ( ) ; if ( TEST_ITERATION_DELAY_MILLIS > <int> ) maybeDelayForTesting ( ) ; } } protected UnfilteredPartitionIterator withStateTracking ( UnfilteredPartitionIterator iter ) { return Transformation . apply ( iter , new CheckForAbort ( ) ) ; } protected UnfilteredRowIterator withStateTracking ( UnfilteredRowIterator iter ) { return Transformation . apply ( iter , new CheckForAbort ( ) ) ; } private void maybeDelayForTesting ( ) { if ( ! metadata . ksName . startsWith ( <str> ) ) FBUtilities . sleepQuietly ( TEST_ITERATION_DELAY_MILLIS ) ; } public abstract MessageOut < ReadCommand > createMessage ( int version ) ; protected abstract void appendCQLWhereClause ( StringBuilder sb ) ; protected UnfilteredPartitionIterator withoutPurgeableTombstones ( UnfilteredPartitionIterator iterator , ColumnFamilyStore cfs ) { final boolean isForThrift = iterator . isForThrift ( ) ; class WithoutPurgeableTombstones extends PurgeFunction { public WithoutPurgeableTombstones ( ) { super ( isForThrift , cfs . gcBefore ( nowInSec ( ) ) , oldestUnrepairedTombstone ( ) , cfs . getCompactionStrategyManager ( ) . onlyPurgeRepairedTombstones ( ) ) ; } protected long getMaxPurgeableTimestamp ( ) { return Long . MAX_VALUE ; } } return Transformation . apply ( iterator , new WithoutPurgeableTombstones ( ) ) ; } public String toCQLString ( ) { StringBuilder sb = new StringBuilder ( ) ; sb . append ( <str> ) . append ( columnFilter ( ) ) ; sb . append ( <str> ) . append ( metadata ( ) . ksName ) . append ( <str> ) . append ( metadata . cfName ) ; appendCQLWhereClause ( sb ) ; if ( limits ( ) ! = DataLimits . NONE ) sb . append ( <str> ) . append ( limits ( ) ) ; return sb . toString ( ) ; } public String name ( ) { return toCQLString ( ) ; } private static class Serializer implements IVersionedSerializer < ReadCommand > { private static int digestFlag ( boolean isDigest ) { return isDigest ? <hex> : <int> ; } private static boolean isDigest ( int flags ) { return ( flags & <hex> ) ! = <int> ; } private static int thriftFlag ( boolean isForThrift ) { return isForThrift ? <hex> : <int> ; } private static boolean isForThrift ( int flags ) { return ( flags & <hex> ) ! = <int> ; } private static int indexFlag ( boolean hasIndex ) { return hasIndex ? <hex> : <int> ; } private static boolean hasIndex ( int flags ) { return ( flags & <hex> ) ! = <int> ; } public void serialize ( ReadCommand command , DataOutputPlus out , int version ) throws IOException { assert version > = MessagingService . VERSION_30 ; out . writeByte ( command . kind . ordinal ( ) ) ; out . writeByte ( digestFlag ( command . isDigestQuery ( ) ) | thriftFlag ( command . isForThrift ( ) ) | indexFlag ( command . index . isPresent ( ) ) ) ; if ( command . isDigestQuery ( ) ) out . writeUnsignedVInt ( command . digestVersion ( ) ) ; CFMetaData . serializer . serialize ( command . metadata ( ) , out , version ) ; out . writeInt ( command . nowInSec ( ) ) ; ColumnFilter . serializer . serialize ( command . columnFilter ( ) , out , version ) ; RowFilter . serializer . serialize ( command . rowFilter ( ) , out , version ) ; DataLimits . serializer . serialize ( command . limits ( ) , out , version ) ; if ( command . index . isPresent ( ) ) IndexMetadata . serializer . serialize ( command . index . get ( ) , out , version ) ; command . serializeSelection ( out , version ) ; } public ReadCommand deserialize ( DataInputPlus in , int version ) throws IOException { if ( version < MessagingService . VERSION_30 ) return legacyReadCommandSerializer . deserialize ( in , version ) ; Kind kind = Kind . values ( ) [ in . readByte ( ) ] ; int flags = in . readByte ( ) ; boolean isDigest = isDigest ( flags ) ; boolean isForThrift = isForThrift ( flags ) ; boolean hasIndex = hasIndex ( flags ) ; int digestVersion = isDigest ? ( int ) in . readUnsignedVInt ( ) : <int> ; CFMetaData metadata = CFMetaData . serializer . deserialize ( in , version ) ; int nowInSec = in . readInt ( ) ; ColumnFilter columnFilter = ColumnFilter . serializer . deserialize ( in , version , metadata ) ; RowFilter rowFilter = RowFilter . serializer . deserialize ( in , version , metadata ) ; DataLimits limits = DataLimits . serializer . deserialize ( in , version ) ; Optional < IndexMetadata > index = hasIndex ? deserializeIndexMetadata ( in , version , metadata ) : Optional . empty ( ) ; return kind . selectionDeserializer . deserialize ( in , version , isDigest , digestVersion , isForThrift , metadata , nowInSec , columnFilter , rowFilter , limits , index ) ; } private Optional < IndexMetadata > deserializeIndexMetadata ( DataInputPlus in , int version , CFMetaData cfm ) throws IOException { try { return Optional . of ( IndexMetadata . serializer . deserialize ( in , version , cfm ) ) ; } catch ( UnknownIndexException e ) { String message = String . format ( <str> + <str> + <str> + <str> , cfm . ksName , cfm . cfName , e . indexId . toString ( ) ) ; logger . info ( message ) ; return Optional . empty ( ) ; } } public long serializedSize ( ReadCommand command , int version ) { assert version > = MessagingService . VERSION_30 ; return <int> + ( command . isDigestQuery ( ) ? TypeSizes . sizeofUnsignedVInt ( command . digestVersion ( ) ) : <int> ) + CFMetaData . serializer . serializedSize ( command . metadata ( ) , version ) + TypeSizes . sizeof ( command . nowInSec ( ) ) + ColumnFilter . serializer . serializedSize ( command . columnFilter ( ) , version ) + RowFilter . serializer . serializedSize ( command . rowFilter ( ) , version ) + DataLimits . serializer . serializedSize ( command . limits ( ) , version ) + command . selectionSerializedSize ( version ) + command . indexSerializedSize ( version ) ; } } private static class RangeSliceSerializer implements IVersionedSerializer < ReadCommand > { public void serialize ( ReadCommand command , DataOutputPlus out , int version ) throws IOException { if ( version < MessagingService . VERSION_30 ) legacyRangeSliceCommandSerializer . serialize ( command , out , version ) ; else serializer . serialize ( command , out , version ) ; } public ReadCommand deserialize ( DataInputPlus in , int version ) throws IOException { return version < MessagingService . VERSION_30 ? legacyRangeSliceCommandSerializer . deserialize ( in , version ) : serializer . deserialize ( in , version ) ; } public long serializedSize ( ReadCommand command , int version ) { return version < MessagingService . VERSION_30 ? legacyRangeSliceCommandSerializer . serializedSize ( command , version ) : serializer . serializedSize ( command , version ) ; } } private enum LegacyType { GET_BY_NAMES ( ( byte ) <int> ) , GET_SLICES ( ( byte ) <int> ) ; public final byte serializedValue ; LegacyType ( byte b ) { this . serializedValue = b ; } public static LegacyType fromPartitionFilterKind ( ClusteringIndexFilter . Kind kind ) { return kind = = ClusteringIndexFilter . Kind . SLICE ? GET_SLICES : GET_BY_NAMES ; } public static LegacyType fromSerializedValue ( byte b ) { return b = = <int> ? GET_BY_NAMES : GET_SLICES ; } } private static class LegacyRangeSliceCommandSerializer implements IVersionedSerializer < ReadCommand > { public void serialize ( ReadCommand command , DataOutputPlus out , int version ) throws IOException { assert version < MessagingService . VERSION_30 ; PartitionRangeReadCommand rangeCommand = ( PartitionRangeReadCommand ) command ; assert ! rangeCommand . dataRange ( ) . isPaging ( ) ; rangeCommand = maybeConvertNamesToSlice ( rangeCommand ) ; CFMetaData metadata = rangeCommand . metadata ( ) ; out . writeUTF ( metadata . ksName ) ; out . writeUTF ( metadata . cfName ) ; out . writeLong ( rangeCommand . nowInSec ( ) * <int> ) ; if ( rangeCommand . isNamesQuery ( ) ) { out . writeByte ( <int> ) ; ClusteringIndexNamesFilter filter = ( ClusteringIndexNamesFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter ; LegacyReadCommandSerializer . serializeNamesFilter ( rangeCommand , filter , out ) ; } else { out . writeByte ( <int> ) ; ClusteringIndexSliceFilter filter = ( ClusteringIndexSliceFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter ; boolean makeStaticSlice = ! rangeCommand . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) & & ! filter . requestedSlices ( ) . selects ( Clustering . STATIC_CLUSTERING ) ; LegacyReadCommandSerializer . serializeSlices ( out , filter . requestedSlices ( ) , filter . isReversed ( ) , makeStaticSlice , metadata ) ; out . writeBoolean ( filter . isReversed ( ) ) ; DataLimits . Kind kind = rangeCommand . limits ( ) . kind ( ) ; boolean isDistinct = ( kind = = DataLimits . Kind . CQL_LIMIT | | kind = = DataLimits . Kind . CQL_PAGING_LIMIT ) & & rangeCommand . limits ( ) . perPartitionCount ( ) = = <int> ; if ( isDistinct ) out . writeInt ( <int> ) ; else out . writeInt ( LegacyReadCommandSerializer . updateLimitForQuery ( rangeCommand . limits ( ) . count ( ) , filter . requestedSlices ( ) ) ) ; int compositesToGroup ; boolean selectsStatics = ! rangeCommand . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) | | filter . requestedSlices ( ) . selects ( Clustering . STATIC_CLUSTERING ) ; if ( kind = = DataLimits . Kind . THRIFT_LIMIT ) compositesToGroup = - <int> ; else if ( isDistinct & & ! selectsStatics ) compositesToGroup = - <int> ; else compositesToGroup = metadata . isDense ( ) ? - <int> : metadata . clusteringColumns ( ) . size ( ) ; out . writeInt ( compositesToGroup ) ; } serializeRowFilter ( out , rangeCommand . rowFilter ( ) ) ; AbstractBounds . rowPositionSerializer . serialize ( rangeCommand . dataRange ( ) . keyRange ( ) , out , version ) ; out . writeInt ( rangeCommand . limits ( ) . count ( ) ) ; if ( rangeCommand . isForThrift ( ) | | rangeCommand . limits ( ) . perPartitionCount ( ) = = <int> ) out . writeBoolean ( false ) ; else out . writeBoolean ( true ) ; out . writeBoolean ( false ) ; } public ReadCommand deserialize ( DataInputPlus in , int version ) throws IOException { assert version < MessagingService . VERSION_30 ; String keyspace = in . readUTF ( ) ; String columnFamily = in . readUTF ( ) ; CFMetaData metadata = Schema . instance . getCFMetaData ( keyspace , columnFamily ) ; if ( metadata = = null ) { String message = String . format ( <str> , keyspace , columnFamily ) ; throw new UnknownColumnFamilyException ( message , null ) ; } int nowInSec = ( int ) ( in . readLong ( ) / <int> ) ; ClusteringIndexFilter filter ; ColumnFilter selection ; int compositesToGroup = <int> ; int perPartitionLimit = - <int> ; byte readType = in . readByte ( ) ; if ( readType = = <int> ) { Pair < ColumnFilter , ClusteringIndexNamesFilter > selectionAndFilter = LegacyReadCommandSerializer . deserializeNamesSelectionAndFilter ( in , metadata ) ; selection = selectionAndFilter . left ; filter = selectionAndFilter . right ; } else { Pair < ClusteringIndexSliceFilter , Boolean > p = LegacyReadCommandSerializer . deserializeSlicePartitionFilter ( in , metadata ) ; filter = p . left ; perPartitionLimit = in . readInt ( ) ; compositesToGroup = in . readInt ( ) ; selection = getColumnSelectionForSlice ( p . right , compositesToGroup , metadata ) ; } RowFilter rowFilter = deserializeRowFilter ( in , metadata ) ; AbstractBounds < PartitionPosition > keyRange = AbstractBounds . rowPositionSerializer . deserialize ( in , metadata . partitioner , version ) ; int maxResults = in . readInt ( ) ; in . readBoolean ( ) ; in . readBoolean ( ) ; boolean selectsStatics = ( ! selection . fetchedColumns ( ) . statics . isEmpty ( ) | | filter . selects ( Clustering . STATIC_CLUSTERING ) ) ; boolean isDistinct = compositesToGroup = = - <int> | | ( perPartitionLimit = = <int> & & selectsStatics ) ; DataLimits limits ; if ( isDistinct ) limits = DataLimits . distinctLimits ( maxResults ) ; else if ( compositesToGroup = = - <int> ) limits = DataLimits . thriftLimits ( maxResults , perPartitionLimit ) ; else limits = DataLimits . cqlLimits ( maxResults ) ; return new PartitionRangeReadCommand ( false , <int> , true , metadata , nowInSec , selection , rowFilter , limits , new DataRange ( keyRange , filter ) , Optional . empty ( ) ) ; } static void serializeRowFilter ( DataOutputPlus out , RowFilter rowFilter ) throws IOException { ArrayList < RowFilter . Expression > indexExpressions = Lists . newArrayList ( rowFilter . iterator ( ) ) ; out . writeInt ( indexExpressions . size ( ) ) ; for ( RowFilter . Expression expression : indexExpressions ) { ByteBufferUtil . writeWithShortLength ( expression . column ( ) . name . bytes , out ) ; expression . operator ( ) . writeTo ( out ) ; ByteBufferUtil . writeWithShortLength ( expression . getIndexValue ( ) , out ) ; } } static RowFilter deserializeRowFilter ( DataInputPlus in , CFMetaData metadata ) throws IOException { int numRowFilters = in . readInt ( ) ; if ( numRowFilters = = <int> ) return RowFilter . NONE ; RowFilter rowFilter = RowFilter . create ( numRowFilters ) ; for ( int i = <int> ; i < numRowFilters ; i + + ) { ByteBuffer columnName = ByteBufferUtil . readWithShortLength ( in ) ; ColumnDefinition column = metadata . getColumnDefinition ( columnName ) ; Operator op = Operator . readFrom ( in ) ; ByteBuffer indexValue = ByteBufferUtil . readWithShortLength ( in ) ; rowFilter . add ( column , op , indexValue ) ; } return rowFilter ; } static long serializedRowFilterSize ( RowFilter rowFilter ) { long size = TypeSizes . sizeof ( <int> ) ; for ( RowFilter . Expression expression : rowFilter ) { size + = ByteBufferUtil . serializedSizeWithShortLength ( expression . column ( ) . name . bytes ) ; size + = TypeSizes . sizeof ( <int> ) ; size + = ByteBufferUtil . serializedSizeWithShortLength ( expression . getIndexValue ( ) ) ; } return size ; } public long serializedSize ( ReadCommand command , int version ) { assert version < MessagingService . VERSION_30 ; assert command . kind = = Kind . PARTITION_RANGE ; PartitionRangeReadCommand rangeCommand = ( PartitionRangeReadCommand ) command ; rangeCommand = maybeConvertNamesToSlice ( rangeCommand ) ; CFMetaData metadata = rangeCommand . metadata ( ) ; long size = TypeSizes . sizeof ( metadata . ksName ) ; size + = TypeSizes . sizeof ( metadata . cfName ) ; size + = TypeSizes . sizeof ( ( long ) rangeCommand . nowInSec ( ) ) ; size + = <int> ; if ( rangeCommand . isNamesQuery ( ) ) { PartitionColumns columns = rangeCommand . columnFilter ( ) . fetchedColumns ( ) ; ClusteringIndexNamesFilter filter = ( ClusteringIndexNamesFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter ; size + = LegacyReadCommandSerializer . serializedNamesFilterSize ( filter , metadata , columns ) ; } else { ClusteringIndexSliceFilter filter = ( ClusteringIndexSliceFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter ; boolean makeStaticSlice = ! rangeCommand . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) & & ! filter . requestedSlices ( ) . selects ( Clustering . STATIC_CLUSTERING ) ; size + = LegacyReadCommandSerializer . serializedSlicesSize ( filter . requestedSlices ( ) , makeStaticSlice , metadata ) ; size + = TypeSizes . sizeof ( filter . isReversed ( ) ) ; size + = TypeSizes . sizeof ( rangeCommand . limits ( ) . perPartitionCount ( ) ) ; size + = TypeSizes . sizeof ( <int> ) ; } if ( rangeCommand . rowFilter ( ) . equals ( RowFilter . NONE ) ) { size + = TypeSizes . sizeof ( <int> ) ; } else { ArrayList < RowFilter . Expression > indexExpressions = Lists . newArrayList ( rangeCommand . rowFilter ( ) . iterator ( ) ) ; size + = TypeSizes . sizeof ( indexExpressions . size ( ) ) ; for ( RowFilter . Expression expression : indexExpressions ) { size + = ByteBufferUtil . serializedSizeWithShortLength ( expression . column ( ) . name . bytes ) ; size + = TypeSizes . sizeof ( expression . operator ( ) . ordinal ( ) ) ; size + = ByteBufferUtil . serializedSizeWithShortLength ( expression . getIndexValue ( ) ) ; } } size + = AbstractBounds . rowPositionSerializer . serializedSize ( rangeCommand . dataRange ( ) . keyRange ( ) , version ) ; size + = TypeSizes . sizeof ( rangeCommand . limits ( ) . count ( ) ) ; size + = TypeSizes . sizeof ( ! rangeCommand . isForThrift ( ) ) ; return size + TypeSizes . sizeof ( rangeCommand . dataRange ( ) . isPaging ( ) ) ; } static PartitionRangeReadCommand maybeConvertNamesToSlice ( PartitionRangeReadCommand command ) { if ( ! command . dataRange ( ) . isNamesQuery ( ) ) return command ; CFMetaData metadata = command . metadata ( ) ; if ( ! LegacyReadCommandSerializer . shouldConvertNamesToSlice ( metadata , command . columnFilter ( ) . fetchedColumns ( ) ) ) return command ; ClusteringIndexNamesFilter filter = ( ClusteringIndexNamesFilter ) command . dataRange ( ) . clusteringIndexFilter ; ClusteringIndexSliceFilter sliceFilter = LegacyReadCommandSerializer . convertNamesFilterToSliceFilter ( filter , metadata ) ; DataRange newRange = new DataRange ( command . dataRange ( ) . keyRange ( ) , sliceFilter ) ; return new PartitionRangeReadCommand ( command . isDigestQuery ( ) , command . digestVersion ( ) , command . isForThrift ( ) , metadata , command . nowInSec ( ) , command . columnFilter ( ) , command . rowFilter ( ) , command . limits ( ) , newRange , Optional . empty ( ) ) ; } static ColumnFilter getColumnSelectionForSlice ( boolean selectsStatics , int compositesToGroup , CFMetaData metadata ) { if ( compositesToGroup = = - <int> ) return ColumnFilter . all ( metadata ) ; PartitionColumns columns = selectsStatics ? metadata . partitionColumns ( ) : metadata . partitionColumns ( ) . withoutStatics ( ) ; return ColumnFilter . selectionBuilder ( ) . addAll ( columns ) . build ( ) ; } } private static class LegacyPagedRangeCommandSerializer implements IVersionedSerializer < ReadCommand > { public void serialize ( ReadCommand command , DataOutputPlus out , int version ) throws IOException { assert version < MessagingService . VERSION_30 ; PartitionRangeReadCommand rangeCommand = ( PartitionRangeReadCommand ) command ; assert rangeCommand . dataRange ( ) . isPaging ( ) ; CFMetaData metadata = rangeCommand . metadata ( ) ; out . writeUTF ( metadata . ksName ) ; out . writeUTF ( metadata . cfName ) ; out . writeLong ( rangeCommand . nowInSec ( ) * <int> ) ; AbstractBounds . rowPositionSerializer . serialize ( rangeCommand . dataRange ( ) . keyRange ( ) , out , version ) ; ClusteringIndexSliceFilter filter ; if ( rangeCommand . dataRange ( ) . clusteringIndexFilter . kind ( ) = = ClusteringIndexFilter . Kind . NAMES ) filter = LegacyReadCommandSerializer . convertNamesFilterToSliceFilter ( ( ClusteringIndexNamesFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter , metadata ) ; else filter = ( ClusteringIndexSliceFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter ; boolean makeStaticSlice = ! rangeCommand . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) & & ! filter . requestedSlices ( ) . selects ( Clustering . STATIC_CLUSTERING ) ; LegacyReadCommandSerializer . serializeSlices ( out , filter . requestedSlices ( ) , filter . isReversed ( ) , makeStaticSlice , metadata ) ; out . writeBoolean ( filter . isReversed ( ) ) ; DataLimits . Kind kind = rangeCommand . limits ( ) . kind ( ) ; boolean isDistinct = ( kind = = DataLimits . Kind . CQL_LIMIT | | kind = = DataLimits . Kind . CQL_PAGING_LIMIT ) & & rangeCommand . limits ( ) . perPartitionCount ( ) = = <int> ; if ( isDistinct ) out . writeInt ( <int> ) ; else out . writeInt ( LegacyReadCommandSerializer . updateLimitForQuery ( rangeCommand . limits ( ) . perPartitionCount ( ) , filter . requestedSlices ( ) ) ) ; boolean selectsStatics = ! rangeCommand . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) | | filter . requestedSlices ( ) . selects ( Clustering . STATIC_CLUSTERING ) ; int compositesToGroup ; if ( kind = = DataLimits . Kind . THRIFT_LIMIT ) compositesToGroup = - <int> ; else if ( isDistinct & & ! selectsStatics ) compositesToGroup = - <int> ; else compositesToGroup = metadata . isDense ( ) ? - <int> : metadata . clusteringColumns ( ) . size ( ) ; out . writeInt ( compositesToGroup ) ; DataRange . Paging pagingRange = ( DataRange . Paging ) rangeCommand . dataRange ( ) ; Clustering lastReturned = pagingRange . getLastReturned ( ) ; Slice . Bound newStart = Slice . Bound . exclusiveStartOf ( lastReturned ) ; Slice lastSlice = filter . requestedSlices ( ) . get ( filter . requestedSlices ( ) . size ( ) - <int> ) ; ByteBufferUtil . writeWithShortLength ( LegacyLayout . encodeBound ( metadata , newStart , true ) , out ) ; ByteBufferUtil . writeWithShortLength ( LegacyLayout . encodeClustering ( metadata , lastSlice . end ( ) . clustering ( ) ) , out ) ; LegacyRangeSliceCommandSerializer . serializeRowFilter ( out , rangeCommand . rowFilter ( ) ) ; int maxResults = rangeCommand . limits ( ) . count ( ) + ( metadata . isCompound ( ) ? <int> : <int> ) ; out . writeInt ( maxResults ) ; if ( rangeCommand . isForThrift ( ) | | rangeCommand . limits ( ) . perPartitionCount ( ) = = <int> ) out . writeBoolean ( false ) ; else out . writeBoolean ( true ) ; } public ReadCommand deserialize ( DataInputPlus in , int version ) throws IOException { assert version < MessagingService . VERSION_30 ; String keyspace = in . readUTF ( ) ; String columnFamily = in . readUTF ( ) ; CFMetaData metadata = Schema . instance . getCFMetaData ( keyspace , columnFamily ) ; if ( metadata = = null ) { String message = String . format ( <str> , keyspace , columnFamily ) ; throw new UnknownColumnFamilyException ( message , null ) ; } int nowInSec = ( int ) ( in . readLong ( ) / <int> ) ; AbstractBounds < PartitionPosition > keyRange = AbstractBounds . rowPositionSerializer . deserialize ( in , metadata . partitioner , version ) ; Pair < ClusteringIndexSliceFilter , Boolean > p = LegacyReadCommandSerializer . deserializeSlicePartitionFilter ( in , metadata ) ; ClusteringIndexSliceFilter filter = p . left ; boolean selectsStatics = p . right ; int perPartitionLimit = in . readInt ( ) ; int compositesToGroup = in . readInt ( ) ; LegacyLayout . LegacyBound startBound = LegacyLayout . decodeBound ( metadata , ByteBufferUtil . readWithShortLength ( in ) , true ) ; ByteBufferUtil . readWithShortLength ( in ) ; ColumnFilter selection = LegacyRangeSliceCommandSerializer . getColumnSelectionForSlice ( selectsStatics , compositesToGroup , metadata ) ; RowFilter rowFilter = LegacyRangeSliceCommandSerializer . deserializeRowFilter ( in , metadata ) ; int maxResults = in . readInt ( ) ; in . readBoolean ( ) ; boolean isDistinct = compositesToGroup = = - <int> | | ( perPartitionLimit = = <int> & & selectsStatics ) ; DataLimits limits ; if ( isDistinct ) limits = DataLimits . distinctLimits ( maxResults ) ; else limits = DataLimits . cqlLimits ( maxResults ) ; limits = limits . forPaging ( maxResults ) ; DataRange dataRange = new DataRange ( keyRange , filter ) ; Slices slices = filter . requestedSlices ( ) ; if ( ! isDistinct & & startBound ! = LegacyLayout . LegacyBound . BOTTOM & & ! startBound . bound . equals ( slices . get ( <int> ) . start ( ) ) ) { dataRange = dataRange . forPaging ( keyRange , metadata . comparator , startBound . getAsClustering ( metadata ) , false ) ; } return new PartitionRangeReadCommand ( false , <int> , true , metadata , nowInSec , selection , rowFilter , limits , dataRange , Optional . empty ( ) ) ; } public long serializedSize ( ReadCommand command , int version ) { assert version < MessagingService . VERSION_30 ; assert command . kind = = Kind . PARTITION_RANGE ; PartitionRangeReadCommand rangeCommand = ( PartitionRangeReadCommand ) command ; CFMetaData metadata = rangeCommand . metadata ( ) ; assert rangeCommand . dataRange ( ) . isPaging ( ) ; long size = TypeSizes . sizeof ( metadata . ksName ) ; size + = TypeSizes . sizeof ( metadata . cfName ) ; size + = TypeSizes . sizeof ( ( long ) rangeCommand . nowInSec ( ) ) ; size + = AbstractBounds . rowPositionSerializer . serializedSize ( rangeCommand . dataRange ( ) . keyRange ( ) , version ) ; ClusteringIndexSliceFilter filter ; if ( rangeCommand . dataRange ( ) . clusteringIndexFilter . kind ( ) = = ClusteringIndexFilter . Kind . NAMES ) filter = LegacyReadCommandSerializer . convertNamesFilterToSliceFilter ( ( ClusteringIndexNamesFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter , metadata ) ; else filter = ( ClusteringIndexSliceFilter ) rangeCommand . dataRange ( ) . clusteringIndexFilter ; boolean makeStaticSlice = ! rangeCommand . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) & & ! filter . requestedSlices ( ) . selects ( Clustering . STATIC_CLUSTERING ) ; size + = LegacyReadCommandSerializer . serializedSlicesSize ( filter . requestedSlices ( ) , makeStaticSlice , metadata ) ; size + = TypeSizes . sizeof ( filter . isReversed ( ) ) ; size + = TypeSizes . sizeof ( rangeCommand . limits ( ) . perPartitionCount ( ) ) ; size + = TypeSizes . sizeof ( <int> ) ; DataRange . Paging pagingRange = ( DataRange . Paging ) rangeCommand . dataRange ( ) ; Clustering lastReturned = pagingRange . getLastReturned ( ) ; Slice lastSlice = filter . requestedSlices ( ) . get ( filter . requestedSlices ( ) . size ( ) - <int> ) ; size + = ByteBufferUtil . serializedSizeWithShortLength ( LegacyLayout . encodeClustering ( metadata , lastReturned ) ) ; size + = ByteBufferUtil . serializedSizeWithShortLength ( LegacyLayout . encodeClustering ( metadata , lastSlice . end ( ) . clustering ( ) ) ) ; size + = LegacyRangeSliceCommandSerializer . serializedRowFilterSize ( rangeCommand . rowFilter ( ) ) ; size + = TypeSizes . sizeof ( rangeCommand . limits ( ) . count ( ) ) ; return size + TypeSizes . sizeof ( true ) ; } } static class LegacyReadCommandSerializer implements IVersionedSerializer < ReadCommand > { public void serialize ( ReadCommand command , DataOutputPlus out , int version ) throws IOException { assert version < MessagingService . VERSION_30 ; assert command . kind = = Kind . SINGLE_PARTITION ; SinglePartitionReadCommand singleReadCommand = ( SinglePartitionReadCommand ) command ; singleReadCommand = maybeConvertNamesToSlice ( singleReadCommand ) ; CFMetaData metadata = singleReadCommand . metadata ( ) ; out . writeByte ( LegacyType . fromPartitionFilterKind ( singleReadCommand . clusteringIndexFilter ( ) . kind ( ) ) . serializedValue ) ; out . writeBoolean ( singleReadCommand . isDigestQuery ( ) ) ; out . writeUTF ( metadata . ksName ) ; ByteBufferUtil . writeWithShortLength ( singleReadCommand . partitionKey ( ) . getKey ( ) , out ) ; out . writeUTF ( metadata . cfName ) ; out . writeLong ( singleReadCommand . nowInSec ( ) * <int> ) ; if ( singleReadCommand . clusteringIndexFilter ( ) . kind ( ) = = ClusteringIndexFilter . Kind . SLICE ) serializeSliceCommand ( singleReadCommand , out ) ; else serializeNamesCommand ( singleReadCommand , out ) ; } public ReadCommand deserialize ( DataInputPlus in , int version ) throws IOException { assert version < MessagingService . VERSION_30 ; LegacyType msgType = LegacyType . fromSerializedValue ( in . readByte ( ) ) ; boolean isDigest = in . readBoolean ( ) ; String keyspaceName = in . readUTF ( ) ; ByteBuffer key = ByteBufferUtil . readWithShortLength ( in ) ; String cfName = in . readUTF ( ) ; long nowInMillis = in . readLong ( ) ; int nowInSeconds = ( int ) ( nowInMillis / <int> ) ; CFMetaData metadata = Schema . instance . getCFMetaData ( keyspaceName , cfName ) ; DecoratedKey dk = metadata . partitioner . decorateKey ( key ) ; switch ( msgType ) { case GET_BY_NAMES : return deserializeNamesCommand ( in , isDigest , metadata , dk , nowInSeconds , version ) ; case GET_SLICES : return deserializeSliceCommand ( in , isDigest , metadata , dk , nowInSeconds , version ) ; default : throw new AssertionError ( ) ; } } public long serializedSize ( ReadCommand command , int version ) { assert version < MessagingService . VERSION_30 ; assert command . kind = = Kind . SINGLE_PARTITION ; SinglePartitionReadCommand singleReadCommand = ( SinglePartitionReadCommand ) command ; singleReadCommand = maybeConvertNamesToSlice ( singleReadCommand ) ; int keySize = singleReadCommand . partitionKey ( ) . getKey ( ) . remaining ( ) ; CFMetaData metadata = singleReadCommand . metadata ( ) ; long size = <int> ; size + = TypeSizes . sizeof ( command . isDigestQuery ( ) ) ; size + = TypeSizes . sizeof ( metadata . ksName ) ; size + = TypeSizes . sizeof ( ( short ) keySize ) + keySize ; size + = TypeSizes . sizeof ( ( long ) command . nowInSec ( ) ) ; if ( singleReadCommand . clusteringIndexFilter ( ) . kind ( ) = = ClusteringIndexFilter . Kind . SLICE ) return size + serializedSliceCommandSize ( singleReadCommand ) ; else return size + serializedNamesCommandSize ( singleReadCommand ) ; } private void serializeNamesCommand ( SinglePartitionReadCommand command , DataOutputPlus out ) throws IOException { serializeNamesFilter ( command , ( ClusteringIndexNamesFilter ) command . clusteringIndexFilter ( ) , out ) ; } private static void serializeNamesFilter ( ReadCommand command , ClusteringIndexNamesFilter filter , DataOutputPlus out ) throws IOException { PartitionColumns columns = command . columnFilter ( ) . fetchedColumns ( ) ; CFMetaData metadata = command . metadata ( ) ; SortedSet < Clustering > requestedRows = filter . requestedRows ( ) ; if ( requestedRows . isEmpty ( ) ) { out . writeInt ( columns . size ( ) ) ; for ( ColumnDefinition column : columns ) ByteBufferUtil . writeWithShortLength ( column . name . bytes , out ) ; } else { out . writeInt ( requestedRows . size ( ) * columns . size ( ) ) ; for ( Clustering clustering : requestedRows ) { for ( ColumnDefinition column : columns ) ByteBufferUtil . writeWithShortLength ( LegacyLayout . encodeCellName ( metadata , clustering , column . name . bytes , null ) , out ) ; } } if ( command . isForThrift ( ) | | ( command . limits ( ) . kind ( ) = = DataLimits . Kind . CQL_LIMIT & & command . limits ( ) . perPartitionCount ( ) = = <int> ) ) out . writeBoolean ( false ) ; else out . writeBoolean ( true ) ; } static long serializedNamesFilterSize ( ClusteringIndexNamesFilter filter , CFMetaData metadata , PartitionColumns fetchedColumns ) { SortedSet < Clustering > requestedRows = filter . requestedRows ( ) ; long size = <int> ; if ( requestedRows . isEmpty ( ) ) { size + = TypeSizes . sizeof ( fetchedColumns . size ( ) ) ; for ( ColumnDefinition column : fetchedColumns ) size + = ByteBufferUtil . serializedSizeWithShortLength ( column . name . bytes ) ; } else { size + = TypeSizes . sizeof ( requestedRows . size ( ) * fetchedColumns . size ( ) ) ; for ( Clustering clustering : requestedRows ) { for ( ColumnDefinition column : fetchedColumns ) size + = ByteBufferUtil . serializedSizeWithShortLength ( LegacyLayout . encodeCellName ( metadata , clustering , column . name . bytes , null ) ) ; } } return size + TypeSizes . sizeof ( true ) ; } private SinglePartitionReadCommand deserializeNamesCommand ( DataInputPlus in , boolean isDigest , CFMetaData metadata , DecoratedKey key , int nowInSeconds , int version ) throws IOException { Pair < ColumnFilter , ClusteringIndexNamesFilter > selectionAndFilter = deserializeNamesSelectionAndFilter ( in , metadata ) ; return new SinglePartitionReadCommand ( isDigest , version , true , metadata , nowInSeconds , selectionAndFilter . left , RowFilter . NONE , DataLimits . NONE , key , selectionAndFilter . right ) ; } static Pair < ColumnFilter , ClusteringIndexNamesFilter > deserializeNamesSelectionAndFilter ( DataInputPlus in , CFMetaData metadata ) throws IOException { int numCellNames = in . readInt ( ) ; NavigableSet < Clustering > clusterings = new TreeSet < > ( metadata . comparator ) ; ColumnFilter . Builder selectionBuilder = ColumnFilter . selectionBuilder ( ) ; for ( int i = <int> ; i < numCellNames ; i + + ) { ByteBuffer buffer = ByteBufferUtil . readWithShortLength ( in ) ; LegacyLayout . LegacyCellName cellName ; try { cellName = LegacyLayout . decodeCellName ( metadata , buffer ) ; } catch ( UnknownColumnException exc ) { throw new UnknownColumnFamilyException ( <str> + <str> + ByteBufferUtil . bytesToHex ( buffer ) , metadata . cfId ) ; } if ( ! cellName . clustering . equals ( Clustering . STATIC_CLUSTERING ) ) clusterings . add ( cellName . clustering ) ; selectionBuilder . add ( cellName . column ) ; } if ( metadata . isStaticCompactTable ( ) & & clusterings . isEmpty ( ) ) selectionBuilder . addAll ( metadata . partitionColumns ( ) ) ; in . readBoolean ( ) ; ClusteringIndexNamesFilter filter = new ClusteringIndexNamesFilter ( clusterings , false ) ; return Pair . create ( selectionBuilder . build ( ) , filter ) ; } private long serializedNamesCommandSize ( SinglePartitionReadCommand command ) { ClusteringIndexNamesFilter filter = ( ClusteringIndexNamesFilter ) command . clusteringIndexFilter ( ) ; PartitionColumns columns = command . columnFilter ( ) . fetchedColumns ( ) ; return serializedNamesFilterSize ( filter , command . metadata ( ) , columns ) ; } private void serializeSliceCommand ( SinglePartitionReadCommand command , DataOutputPlus out ) throws IOException { CFMetaData metadata = command . metadata ( ) ; ClusteringIndexSliceFilter filter = ( ClusteringIndexSliceFilter ) command . clusteringIndexFilter ( ) ; Slices slices = filter . requestedSlices ( ) ; boolean makeStaticSlice = ! command . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) & & ! slices . selects ( Clustering . STATIC_CLUSTERING ) ; serializeSlices ( out , slices , filter . isReversed ( ) , makeStaticSlice , metadata ) ; out . writeBoolean ( filter . isReversed ( ) ) ; boolean selectsStatics = ! command . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) | | slices . selects ( Clustering . STATIC_CLUSTERING ) ; DataLimits . Kind kind = command . limits ( ) . kind ( ) ; boolean isDistinct = ( kind = = DataLimits . Kind . CQL_LIMIT | | kind = = DataLimits . Kind . CQL_PAGING_LIMIT ) & & command . limits ( ) . perPartitionCount ( ) = = <int> ; if ( isDistinct ) out . writeInt ( <int> ) ; else out . writeInt ( updateLimitForQuery ( command . limits ( ) . count ( ) , filter . requestedSlices ( ) ) ) ; int compositesToGroup ; if ( kind = = DataLimits . Kind . THRIFT_LIMIT | | metadata . isDense ( ) ) compositesToGroup = - <int> ; else if ( isDistinct & & ! selectsStatics ) compositesToGroup = - <int> ; else compositesToGroup = metadata . clusteringColumns ( ) . size ( ) ; out . writeInt ( compositesToGroup ) ; } private SinglePartitionReadCommand deserializeSliceCommand ( DataInputPlus in , boolean isDigest , CFMetaData metadata , DecoratedKey key , int nowInSeconds , int version ) throws IOException { Pair < ClusteringIndexSliceFilter , Boolean > p = deserializeSlicePartitionFilter ( in , metadata ) ; ClusteringIndexSliceFilter filter = p . left ; boolean selectsStatics = p . right ; int count = in . readInt ( ) ; int compositesToGroup = in . readInt ( ) ; ColumnFilter columnFilter = LegacyRangeSliceCommandSerializer . getColumnSelectionForSlice ( selectsStatics , compositesToGroup , metadata ) ; boolean isDistinct = compositesToGroup = = - <int> | | ( count = = <int> & & selectsStatics ) ; DataLimits limits ; if ( compositesToGroup = = - <int> | | isDistinct ) limits = DataLimits . distinctLimits ( count ) ; else if ( compositesToGroup = = - <int> ) limits = DataLimits . thriftLimits ( <int> , count ) ; else limits = DataLimits . cqlLimits ( count ) ; return new SinglePartitionReadCommand ( isDigest , version , true , metadata , nowInSeconds , columnFilter , RowFilter . NONE , limits , key , filter ) ; } private long serializedSliceCommandSize ( SinglePartitionReadCommand command ) { CFMetaData metadata = command . metadata ( ) ; ClusteringIndexSliceFilter filter = ( ClusteringIndexSliceFilter ) command . clusteringIndexFilter ( ) ; Slices slices = filter . requestedSlices ( ) ; boolean makeStaticSlice = ! command . columnFilter ( ) . fetchedColumns ( ) . statics . isEmpty ( ) & & ! slices . selects ( Clustering . STATIC_CLUSTERING ) ; long size = serializedSlicesSize ( slices , makeStaticSlice , metadata ) ; size + = TypeSizes . sizeof ( command . clusteringIndexFilter ( ) . isReversed ( ) ) ; size + = TypeSizes . sizeof ( command . limits ( ) . count ( ) ) ; return size + TypeSizes . sizeof ( <int> ) ; } static void serializeSlices ( DataOutputPlus out , Slices slices , boolean isReversed , boolean makeStaticSlice , CFMetaData metadata ) throws IOException { out . writeInt ( slices . size ( ) + ( makeStaticSlice ? <int> : <int> ) ) ; if ( isReversed ) { for ( int i = slices . size ( ) - <int> ; i > = <int> ; i - - ) serializeSlice ( out , slices . get ( i ) , true , metadata ) ; if ( makeStaticSlice ) serializeStaticSlice ( out , true , metadata ) ; } else { if ( makeStaticSlice ) serializeStaticSlice ( out , false , metadata ) ; for ( Slice slice : slices ) serializeSlice ( out , slice , false , metadata ) ; } } static long serializedSlicesSize ( Slices slices , boolean makeStaticSlice , CFMetaData metadata ) { long size = TypeSizes . sizeof ( slices . size ( ) ) ; for ( Slice slice : slices ) { ByteBuffer sliceStart = LegacyLayout . encodeBound ( metadata , slice . start ( ) , true ) ; size + = ByteBufferUtil . serializedSizeWithShortLength ( sliceStart ) ; ByteBuffer sliceEnd = LegacyLayout . encodeBound ( metadata , slice . end ( ) , false ) ; size + = ByteBufferUtil . serializedSizeWithShortLength ( sliceEnd ) ; } if ( makeStaticSlice ) size + = serializedStaticSliceSize ( metadata ) ; return size ; } static long serializedStaticSliceSize ( CFMetaData metadata ) { ByteBuffer sliceStart = LegacyLayout . encodeBound ( metadata , Slice . Bound . BOTTOM , false ) ; long size = ByteBufferUtil . serializedSizeWithShortLength ( sliceStart ) ; size + = TypeSizes . sizeof ( ( short ) ( metadata . comparator . size ( ) * <int> + <int> ) ) ; size + = TypeSizes . sizeof ( ( short ) LegacyLayout . STATIC_PREFIX ) ; for ( int i = <int> ; i < metadata . comparator . size ( ) ; i + + ) { size + = ByteBufferUtil . serializedSizeWithShortLength ( ByteBufferUtil . EMPTY_BYTE_BUFFER ) ; size + = <int> ; } return size ; } private static void serializeSlice ( DataOutputPlus out , Slice slice , boolean isReversed , CFMetaData metadata ) throws IOException { ByteBuffer sliceStart = LegacyLayout . encodeBound ( metadata , isReversed ? slice . end ( ) : slice . start ( ) , ! isReversed ) ; ByteBufferUtil . writeWithShortLength ( sliceStart , out ) ; ByteBuffer sliceEnd = LegacyLayout . encodeBound ( metadata , isReversed ? slice . start ( ) : slice . end ( ) , isReversed ) ; ByteBufferUtil . writeWithShortLength ( sliceEnd , out ) ; } private static void serializeStaticSlice ( DataOutputPlus out , boolean isReversed , CFMetaData metadata ) throws IOException { if ( ! isReversed ) { ByteBuffer sliceStart = LegacyLayout . encodeBound ( metadata , Slice . Bound . BOTTOM , false ) ; ByteBufferUtil . writeWithShortLength ( sliceStart , out ) ; } out . writeShort ( <int> + metadata . comparator . size ( ) * <int> ) ; out . writeShort ( LegacyLayout . STATIC_PREFIX ) ; for ( int i = <int> ; i < metadata . comparator . size ( ) ; i + + ) { ByteBufferUtil . writeWithShortLength ( ByteBufferUtil . EMPTY_BYTE_BUFFER , out ) ; out . writeByte ( i = = metadata . comparator . size ( ) - <int> ? <int> : <int> ) ; } if ( isReversed ) { ByteBuffer sliceStart = LegacyLayout . encodeBound ( metadata , Slice . Bound . BOTTOM , false ) ; ByteBufferUtil . writeWithShortLength ( sliceStart , out ) ; } } static Pair < ClusteringIndexSliceFilter , Boolean > deserializeSlicePartitionFilter ( DataInputPlus in , CFMetaData metadata ) throws IOException { int numSlices = in . readInt ( ) ; ByteBuffer [ ] startBuffers = new ByteBuffer [ numSlices ] ; ByteBuffer [ ] finishBuffers = new ByteBuffer [ numSlices ] ; for ( int i = <int> ; i < numSlices ; i + + ) { startBuffers [ i ] = ByteBufferUtil . readWithShortLength ( in ) ; finishBuffers [ i ] = ByteBufferUtil . readWithShortLength ( in ) ; } boolean reversed = in . readBoolean ( ) ; if ( reversed ) { ByteBuffer [ ] tmp = finishBuffers ; finishBuffers = startBuffers ; startBuffers = tmp ; } boolean selectsStatics = false ; Slices . Builder slicesBuilder = new Slices . Builder ( metadata . comparator ) ; for ( int i = <int> ; i < numSlices ; i + + ) { LegacyLayout . LegacyBound start = LegacyLayout . decodeBound ( metadata , startBuffers [ i ] , true ) ; LegacyLayout . LegacyBound finish = LegacyLayout . decodeBound ( metadata , finishBuffers [ i ] , false ) ; if ( start . isStatic ) { start = LegacyLayout . LegacyBound . BOTTOM ; if ( start . bound . isInclusive ( ) ) selectsStatics = true ; } else if ( start = = LegacyLayout . LegacyBound . BOTTOM ) { selectsStatics = true ; } if ( finish . isStatic ) { assert finish . bound . isInclusive ( ) ; continue ; } slicesBuilder . add ( Slice . make ( start . bound , finish . bound ) ) ; } return Pair . create ( new ClusteringIndexSliceFilter ( slicesBuilder . build ( ) , reversed ) , selectsStatics ) ; } private static SinglePartitionReadCommand maybeConvertNamesToSlice ( SinglePartitionReadCommand command ) { if ( command . clusteringIndexFilter ( ) . kind ( ) ! = ClusteringIndexFilter . Kind . NAMES ) return command ; CFMetaData metadata = command . metadata ( ) ; if ( ! shouldConvertNamesToSlice ( metadata , command . columnFilter ( ) . fetchedColumns ( ) ) ) return command ; ClusteringIndexNamesFilter filter = ( ClusteringIndexNamesFilter ) command . clusteringIndexFilter ( ) ; ClusteringIndexSliceFilter sliceFilter = convertNamesFilterToSliceFilter ( filter , metadata ) ; return new SinglePartitionReadCommand ( command . isDigestQuery ( ) , command . digestVersion ( ) , command . isForThrift ( ) , metadata , command . nowInSec ( ) , command . columnFilter ( ) , command . rowFilter ( ) , command . limits ( ) , command . partitionKey ( ) , sliceFilter ) ; } static boolean shouldConvertNamesToSlice ( CFMetaData metadata , PartitionColumns columns ) { if ( ! metadata . isDense ( ) & & metadata . isCompound ( ) ) return true ; for ( ColumnDefinition column : columns ) { if ( column . type . isMultiCell ( ) ) return true ; } return false ; } private static ClusteringIndexSliceFilter convertNamesFilterToSliceFilter ( ClusteringIndexNamesFilter filter , CFMetaData metadata ) { SortedSet < Clustering > requestedRows = filter . requestedRows ( ) ; Slices slices ; if ( requestedRows . isEmpty ( ) ) { slices = Slices . NONE ; } else if ( requestedRows . size ( ) = = <int> & & requestedRows . first ( ) . size ( ) = = <int> ) { slices = Slices . ALL ; } else { Slices . Builder slicesBuilder = new Slices . Builder ( metadata . comparator ) ; for ( Clustering clustering : requestedRows ) slicesBuilder . add ( Slice . Bound . inclusiveStartOf ( clustering ) , Slice . Bound . inclusiveEndOf ( clustering ) ) ; slices = slicesBuilder . build ( ) ; } return new ClusteringIndexSliceFilter ( slices , filter . isReversed ( ) ) ; } static int updateLimitForQuery ( int limit , Slices slices ) { if ( ! slices . hasLowerBound ( ) & & ! slices . hasUpperBound ( ) ) return limit ; for ( Slice slice : slices ) { if ( limit = = Integer . MAX_VALUE ) return limit ; if ( ! slice . start ( ) . isInclusive ( ) ) limit + + ; if ( ! slice . end ( ) . isInclusive ( ) ) limit + + ; } return limit ; } } } 
