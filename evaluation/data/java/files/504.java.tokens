package org . elasticsearch . search . aggregations . bucket . terms ; import org . apache . lucene . index . LeafReaderContext ; import org . apache . lucene . util . BytesRef ; import org . apache . lucene . util . BytesRefBuilder ; import org . elasticsearch . common . lease . Releasables ; import org . elasticsearch . common . util . BytesRefHash ; import org . elasticsearch . index . fielddata . SortedBinaryDocValues ; import org . elasticsearch . search . aggregations . Aggregator ; import org . elasticsearch . search . aggregations . AggregatorFactories ; import org . elasticsearch . search . aggregations . LeafBucketCollectorBase ; import org . elasticsearch . search . aggregations . InternalAggregation ; import org . elasticsearch . search . aggregations . LeafBucketCollector ; import org . elasticsearch . search . aggregations . bucket . terms . support . BucketPriorityQueue ; import org . elasticsearch . search . aggregations . bucket . terms . support . IncludeExclude ; import org . elasticsearch . search . aggregations . pipeline . PipelineAggregator ; import org . elasticsearch . search . aggregations . support . AggregationContext ; import org . elasticsearch . search . aggregations . support . ValuesSource ; import java . io . IOException ; import java . util . Arrays ; import java . util . List ; import java . util . Map ; public class StringTermsAggregator extends AbstractStringTermsAggregator { private final ValuesSource valuesSource ; protected final BytesRefHash bucketOrds ; private final IncludeExclude . StringFilter includeExclude ; public StringTermsAggregator ( String name , AggregatorFactories factories , ValuesSource valuesSource , Terms . Order order , BucketCountThresholds bucketCountThresholds , IncludeExclude . StringFilter includeExclude , AggregationContext aggregationContext , Aggregator parent , SubAggCollectionMode collectionMode , boolean showTermDocCountError , List < PipelineAggregator > pipelineAggregators , Map < String , Object > metaData ) throws IOException { super ( name , factories , aggregationContext , parent , order , bucketCountThresholds , collectionMode , showTermDocCountError , pipelineAggregators , metaData ) ; this . valuesSource = valuesSource ; this . includeExclude = includeExclude ; bucketOrds = new BytesRefHash ( <int> , aggregationContext . bigArrays ( ) ) ; } @Override public boolean needsScores ( ) { return ( valuesSource ! = null & & valuesSource . needsScores ( ) ) | | super . needsScores ( ) ; } @Override public LeafBucketCollector getLeafCollector ( LeafReaderContext ctx , final LeafBucketCollector sub ) throws IOException { final SortedBinaryDocValues values = valuesSource . bytesValues ( ctx ) ; return new LeafBucketCollectorBase ( sub , values ) { final BytesRefBuilder previous = new BytesRefBuilder ( ) ; @Override public void collect ( int doc , long bucket ) throws IOException { assert bucket = = <int> ; values . setDocument ( doc ) ; final int valuesCount = values . count ( ) ; previous . clear ( ) ; for ( int i = <int> ; i < valuesCount ; + + i ) { final BytesRef bytes = values . valueAt ( i ) ; if ( includeExclude ! = null & & ! includeExclude . accept ( bytes ) ) { continue ; } if ( previous . get ( ) . equals ( bytes ) ) { continue ; } long bucketOrdinal = bucketOrds . add ( bytes ) ; if ( bucketOrdinal < <int> ) { bucketOrdinal = - <int> - bucketOrdinal ; collectExistingBucket ( sub , doc , bucketOrdinal ) ; } else { collectBucket ( sub , doc , bucketOrdinal ) ; } previous . copyBytes ( bytes ) ; } } } ; } @Override public InternalAggregation buildAggregation ( long owningBucketOrdinal ) throws IOException { assert owningBucketOrdinal = = <int> ; if ( bucketCountThresholds . getMinDocCount ( ) = = <int> & & ( order ! = InternalOrder . COUNT_DESC | | bucketOrds . size ( ) < bucketCountThresholds . getRequiredSize ( ) ) ) { for ( LeafReaderContext ctx : context . searchContext ( ) . searcher ( ) . getTopReaderContext ( ) . leaves ( ) ) { final SortedBinaryDocValues values = valuesSource . bytesValues ( ctx ) ; for ( int docId = <int> ; docId < ctx . reader ( ) . maxDoc ( ) ; + + docId ) { values . setDocument ( docId ) ; final int valueCount = values . count ( ) ; for ( int i = <int> ; i < valueCount ; + + i ) { final BytesRef term = values . valueAt ( i ) ; if ( includeExclude = = null | | includeExclude . accept ( term ) ) { bucketOrds . add ( term ) ; } } } } } final int size = ( int ) Math . min ( bucketOrds . size ( ) , bucketCountThresholds . getShardSize ( ) ) ; long otherDocCount = <int> ; BucketPriorityQueue ordered = new BucketPriorityQueue ( size , order . comparator ( this ) ) ; StringTerms . Bucket spare = null ; for ( int i = <int> ; i < bucketOrds . size ( ) ; i + + ) { if ( spare = = null ) { spare = new StringTerms . Bucket ( new BytesRef ( ) , <int> , null , showTermDocCountError , <int> ) ; } bucketOrds . get ( i , spare . termBytes ) ; spare . docCount = bucketDocCount ( i ) ; otherDocCount + = spare . docCount ; spare . bucketOrd = i ; if ( bucketCountThresholds . getShardMinDocCount ( ) < = spare . docCount ) { spare = ( StringTerms . Bucket ) ordered . insertWithOverflow ( spare ) ; } } final InternalTerms . Bucket [ ] list = new InternalTerms . Bucket [ ordered . size ( ) ] ; long survivingBucketOrds [ ] = new long [ ordered . size ( ) ] ; for ( int i = ordered . size ( ) - <int> ; i > = <int> ; - - i ) { final StringTerms . Bucket bucket = ( StringTerms . Bucket ) ordered . pop ( ) ; survivingBucketOrds [ i ] = bucket . bucketOrd ; list [ i ] = bucket ; otherDocCount - = bucket . docCount ; } runDeferredCollections ( survivingBucketOrds ) ; for ( int i = <int> ; i < list . length ; i + + ) { final StringTerms . Bucket bucket = ( StringTerms . Bucket ) list [ i ] ; bucket . termBytes = BytesRef . deepCopyOf ( bucket . termBytes ) ; bucket . aggregations = bucketAggregations ( bucket . bucketOrd ) ; bucket . docCountError = <int> ; } return new StringTerms ( name , order , bucketCountThresholds . getRequiredSize ( ) , bucketCountThresholds . getShardSize ( ) , bucketCountThresholds . getMinDocCount ( ) , Arrays . asList ( list ) , showTermDocCountError , <int> , otherDocCount , pipelineAggregators ( ) , metaData ( ) ) ; } @Override public void doClose ( ) { Releasables . close ( bucketOrds ) ; } } 
