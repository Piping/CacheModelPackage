package org . elasticsearch . index . analysis ; import org . apache . lucene . analysis . Tokenizer ; import org . apache . lucene . analysis . core . WhitespaceTokenizer ; import org . elasticsearch . common . settings . Settings ; import org . elasticsearch . test . ESTokenStreamTestCase ; import java . io . IOException ; import java . io . StringReader ; public class LimitTokenCountFilterFactoryTests extends ESTokenStreamTestCase { public void testDefault ( ) throws IOException { Settings settings = Settings . settingsBuilder ( ) . put ( <str> , <str> ) . put ( <str> , createTempDir ( ) . toString ( ) ) . build ( ) ; AnalysisService analysisService = AnalysisTestsHelper . createAnalysisServiceFromSettings ( settings ) ; { TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } { TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } } public void testSettings ( ) throws IOException { { Settings settings = Settings . settingsBuilder ( ) . put ( <str> , <str> ) . put ( <str> , <int> ) . put ( <str> , true ) . put ( <str> , createTempDir ( ) . toString ( ) ) . build ( ) ; AnalysisService analysisService = AnalysisTestsHelper . createAnalysisServiceFromSettings ( settings ) ; TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> , <str> , <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } { Settings settings = Settings . settingsBuilder ( ) . put ( <str> , <str> ) . put ( <str> , <int> ) . put ( <str> , false ) . put ( <str> , createTempDir ( ) . toString ( ) ) . build ( ) ; AnalysisService analysisService = AnalysisTestsHelper . createAnalysisServiceFromSettings ( settings ) ; TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> , <str> , <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } { Settings settings = Settings . settingsBuilder ( ) . put ( <str> , <str> ) . put ( <str> , <int> ) . put ( <str> , true ) . put ( <str> , createTempDir ( ) . toString ( ) ) . build ( ) ; AnalysisService analysisService = AnalysisTestsHelper . createAnalysisServiceFromSettings ( settings ) ; TokenFilterFactory tokenFilter = analysisService . tokenFilter ( <str> ) ; String source = <str> ; String [ ] expected = new String [ ] { <str> , <str> , <str> , <str> } ; Tokenizer tokenizer = new WhitespaceTokenizer ( ) ; tokenizer . setReader ( new StringReader ( source ) ) ; assertTokenStreamContents ( tokenFilter . create ( tokenizer ) , expected ) ; } } } 
